{"cells":[{"cell_type":"markdown","metadata":{"id":"zhC-_1xcQD1y"},"source":["Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"chB0ISYVwsiR"},"outputs":[],"source":["!pip install scikeras==0.12.0\n","!pip install missingpy==0.2.0\n","!pip install statsmodels\n","!pip uninstall -y cvxpy\n","!pip install cvxpy==1.5.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XU8Z_nYVQFdS"},"outputs":[],"source":["import os\n","import random\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import statsmodels.api as sm\n","\n","from scipy.stats import norm\n","\n","from sklearn import ensemble\n","from sklearn.linear_model import LinearRegression\n","\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout\n","#from keras.wrappers.scikit_learn import KerasClassifier\n","#from keras.wrappers.scikit_learn import KerasRegressor\n","from scikeras.wrappers import KerasClassifier, KerasRegressor\n","#import shap\n","\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","\n","import pickle\n","\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","\n","import sklearn.metrics\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn import tree\n","from sklearn.tree import export_text\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","\n","import collections\n","\n","import time\n","\n","import pickle\n","\n","\n","import random\n","\n","from pandas.compat import numpy\n","import scipy\n"]},{"cell_type":"markdown","metadata":{"id":"iZ1I0Hi5PQE6"},"source":["Data generation (dat.py)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_tTFuD7PYzO"},"outputs":[],"source":["\n","def gen_sim_data(n, s_type, y_set):\n","    \"\"\"generate simulated data\"\"\"\n","\n","\n","    if (y_set == \"toy\"):\n","\n","        y_fn = \"(X1>0.5)*(24 - 11*A - 19*(1-cos(3.1415*0.5*S)) + 21*A*(1-cos(3.1415*0.5*S))) + (X1<=0.5)*(11 + 14*A + 2*(1-cos(3.1415*0.5*S)) - 27*A*(1-cos(3.1415*0.5*S)))\"\n","\n","        X1 = np.random.normal(0.5,1,n)\n","        X2 = np.random.uniform(0,0, n)\n","        err = np.random.normal(loc=0.0, scale=1, size=n)\n","\n","        if s_type == \"cont\":\n","            s1 = np.random.beta(4,1,n//2)\n","            s0 = np.random.beta(1,4,n-n//2)\n","            S = np.concatenate([s0,s1])\n","            random.shuffle(S)\n","\n","        elif s_type == \"disc\":\n","            S = np.random.binomial(1, 0.5, n)\n","        A = np.random.binomial(1, 0.5, n)\n","        dat = np.concatenate((A[:,np.newaxis], S[:,np.newaxis], X1[:,np.newaxis], X2[:,np.newaxis]),axis=1) #, X3[:,np.newaxis], X4[:,np.newaxis]\n","        df = pd.DataFrame(data=dat, columns=['A','S','X1','X2'])\n","        df[\"Y\"] = df.eval(y_fn) + err\n","\n","\n","\n","    if (y_set == \"complex\"):\n","        y_fn = \"(0.5 + 1*A + exp(S) - 2.5*A*S) * (1+X1 -X2 +X3**2 +exp(X4)) + (1 + 2*A + 0.2*exp(S) - 3.5*A*S) * (1+5*X1 -2*X2 +3*X3 +2*exp(X4))\"\n","        X1 = np.random.uniform(0,1, n)\n","        X2 = np.random.uniform(0,1, n)\n","        X3 = np.random.uniform(0,1, n)\n","        X4 = np.random.uniform(0,1, n)\n","        X5 = np.random.uniform(0,1, n)\n","        X6 = np.random.uniform(0,1, n)\n","        err = np.random.normal(loc=0.0, scale=1, size=n)\n","\n","        pr_s1 = 1 / (1 + np.exp( 2.5 - 0.8*(X1+X2+X3+X4+X5+X6) )) # expit(-2.5 + 0.8()) # norm\n","\n","        if s_type == \"cont\":\n","            s1 = np.random.beta(4,1,n//2)\n","            s0 = np.random.beta(1,4,n-n//2)\n","            S = np.concatenate([s0,s1])\n","        else:\n","            S = np.random.binomial(1, pr_s1, n)\n","\n","        dat = np.concatenate((S[:,np.newaxis], X1[:,np.newaxis], X2[:,np.newaxis], X3[:,np.newaxis], X4[:,np.newaxis], X5[:,np.newaxis], X6[:,np.newaxis]),axis=1)\n","        df = pd.DataFrame(data=dat, columns=['S','X1','X2','X3','X4','X5','X6'])\n","\n","\n","        a_fn = \"1 / (1 + exp( 0.6*S - 0.6*X1 + 0.6*X2 - 0.6*X3 + 0.6*X4 - 0.6*X5 + 0.6*X6 ))\"\n","        prop_sc = df.eval(a_fn)\n","        df['A'] = np.random.binomial(1, prop_sc, n)\n","\n","        df[\"Y\"] = df.eval(y_fn) + err\n","\n","\n","    # convert to float\n","    cols=[i for i in df.columns]\n","    for col in cols:\n","        df[col]=pd.to_numeric(df[col])\n","\n","    df[\"A\"] = df[\"A\"].astype(int)\n","\n","    if (s_type == \"disc\"):\n","        df[\"S\"] = df[\"S\"].astype(int)\n","\n","    return(df, y_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4c4nNnza8H36"},"outputs":[],"source":["\n","def gen_sim_data_RWDRCT(df,n):\n","    Spprob=np.exp(-5+3*df['X1']-2*df['S'])/(1+np.exp(-5+3*df['X1']-2*df['S']))\n","    Sp=np.random.binomial(1,Spprob,n)\n","    df[\"Spprob\"]=Spprob\n","    df[\"Sp\"]=Sp\n","    return df\n","\n","N=100000\n","\n","\n","def transfer_weight_obj(alpha):\n","    m=df_RWD.shape[0]\n","    df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[[\"X1\",\"X2\",\"S\"]]),axis=1)\n","    df_RWD_1X=np.concatenate((np.ones((df_RWD.shape[0],1)),df_RWD[[\"X1\",\"X2\",\"S\"]]),axis=1)\n","    return -(1/N)*np.sum(df_RCT_1X@alpha)+(1/m)*np.sum(np.log(1+np.exp(df_RWD_1X@alpha)))\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"J3ZhUlZ0QaD6"},"source":["Randomization & Normalization & Split train test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5c0Ifh17QhrX"},"outputs":[],"source":["\n","def set_random(seed_value):\n","    os.environ['PYTHONHASHSEED']=str(seed_value)\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    tf.random.set_seed(seed_value)\n","\n","    return\n","\n","\n","def normalize(df_tr, df_te, names_to_norm):\n","    df = df_tr.copy()\n","    df_test = df_te.copy()\n","\n","    # normalize X & S\n","    scaler = StandardScaler()\n","    df[names_to_norm] = scaler.fit_transform(df[names_to_norm])\n","    df_test[names_to_norm] = scaler.transform(df_test[names_to_norm])\n","\n","    return(df.copy(), df_test.copy())\n","\n","\n","def get_train_test(df_all, s_type, seed_value):\n","    # train-test split by A\n","    df0_ori = df_all[df_all[\"A\"]==0].reset_index(drop=True).copy()\n","    df1_ori = df_all[df_all[\"A\"]==1].reset_index(drop=True).copy()\n","    df0_train, df0_test = train_test_split(df0_ori, test_size=0.2, random_state=seed_value)\n","    df1_train, df1_test = train_test_split(df1_ori, test_size=0.2, random_state=seed_value)\n","\n","    df_ori = pd.concat([df0_train, df1_train]).reset_index(drop=True).copy()\n","    df_test_ori = pd.concat([df0_test, df1_test]).reset_index(drop=True).copy()\n","\n","\n","    print(\"df_ori.shape\", df_ori.shape)\n","    if (s_type == \"disc\"):\n","        print(\"train Pr(S=0)/Pr(S=1):\", round(sum(df_ori[\"S\"]==0) / sum(df_ori[\"S\"]==1),3))\n","    print(\"train Pr(A=0)/Pr(A=1):\", round(sum(df_ori[\"A\"]==0) / sum(df_ori[\"A\"]==1),3))\n","\n","    print(\"df_test_ori.shape\", df_test_ori.shape)\n","    if (s_type == \"disc\"):\n","        print(\"test Pr(S=0)/Pr(S=1):\", round(sum(df_test_ori[\"S\"]==0) / sum(df_test_ori[\"S\"]==1),3))\n","    print(\"test Pr(A=0)/Pr(A=1):\", round(sum(df_test_ori[\"A\"]==0) / sum(df_test_ori[\"A\"]==1),3))\n","\n","    return(df_ori.copy(), df_test_ori.copy())\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FjEac6PMQPWq"},"source":["Get PS fit & Keras & Hyper tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lb1p31sVOn-k"},"outputs":[],"source":["\n","def get_ps_fit(df_train, use_covars, transfer_weights):\n","    df_tr = df_train.copy()\n","    regr = RandomForestClassifier(random_state=1234).fit(df_tr[use_covars], df_tr[\"A\"],sample_weight=transfer_weights)\n","    return(regr)\n","\n","def keras_wrap(x_train, train_labels, train_wts, x_test, loss_fn, act_out,\n","               layer=2, node=1024, dropout=0.2, n_epoch=100, bsize=64, act=\"relu\",\n","               opt=\"Adam\", val_split=0.2, is_early_stop=True, verb=0):\n","\n","    if is_early_stop:\n","        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n","        callback = [early_stop]\n","    else:\n","        callback = None\n","\n","    # set input_dim for the number of features\n","    if len(x_train.shape) == 1:\n","        input_dim = 1\n","    else:\n","        input_dim = x_train.shape[1]\n","\n","    # model\n","    model = Sequential()\n","    for i in range(layer):\n","        if i==0:\n","            model.add(Dense(node, input_dim=input_dim, activation=act)) # Hidden 1\n","            model.add(Dropout(dropout))\n","        else:\n","           model.add(Dense(node, activation=act)) # Hidden 2\n","           model.add(Dropout(dropout))\n","\n","    model.add(Dense(1, activation=act_out)) # Output\n","\n","    model.compile(loss=loss_fn, optimizer=opt, weighted_metrics='acc')\n","    model.fit(x_train, train_labels,\n","              sample_weight=train_wts,\n","              epochs=n_epoch, batch_size=bsize,\n","              validation_split=val_split, callbacks=callback, verbose=verb)\n","\n","    # predict\n","    pred_test = model.predict(x_test).flatten()\n","    pred_train = model.predict(x_train).flatten()\n","    return pred_test, pred_train, model\n","\n","\n","def hyper_tuning(x_train, train_labels, train_wts, loss_fn, act_out,\n","                 param_grid, n_cv=5, n_jobs=1):\n","    \"\"\"\n","    layers = [2,3]\n","    nodes=[100,300,512]\n","    dropout=[0.2] #,0.4\n","    activation = [\"sigmoid\",\"relu\"]\n","    optimizer = [\"adam\"] #,\"nadam\"\n","    bsize = [32,64] #,128\n","    n_epochs = [50,100] #,200\n","    bst_params = hyper_tuning(x_train, train_labels, train_wts, loss_fn, act_out,\n","                              param_grid, n_cv=5, n_jobs=1)\n","    \"\"\"\n","    # set input_dim for the number of features\n","    if len(x_train.shape) == 1:\n","        input_dim = 1\n","    else:\n","        input_dim = x_train.shape[1]\n","\n","    def create_model(layers,nodes,activation,optimizer,dropout):\n","        model = Sequential()\n","        for i in range(layers):\n","            if i==0:\n","                model.add(Dense(nodes, input_dim=input_dim))\n","                model.add(Activation(activation))\n","                model.add(Dropout(dropout))\n","            else:\n","                model.add(Dense(nodes, activation=activation))\n","                model.add(Activation(activation))\n","                model.add(Dropout(dropout))\n","\n","        model.add(Dense(units=1, activation=act_out))\n","\n","        model.compile(optimizer=optimizer, loss=loss_fn, metrics=['acc'],weighted_metrics=['acc'])\n","        return model\n","\n","    if act_out == \"sigmoid\": #for classification\n","        model = KerasClassifier(build_fn=create_model, verbose=0)\n","    else: #None #for regression including quantile\n","        model = KerasRegressor(build_fn=create_model, verbose=0)\n","\n","    assert param_grid is not None\n","    layers = param_grid['layers']\n","    nodes = param_grid['nodes']\n","    dropout = param_grid['dropouts']\n","    activation = param_grid['acts']\n","    optimizer = param_grid['opts']\n","    bsizes = param_grid['bsizes']\n","    n_epochs = param_grid['n_epochs']\n","\n","    param_grid = dict(layers=layers, nodes=nodes, activation=activation, optimizer=optimizer,\n","                      dropout=dropout, batch_size=bsizes, epochs=n_epochs)\n","    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=n_cv, n_jobs=n_jobs)\n","\n","    grid_result = grid.fit(x_train, train_labels, sample_weight=train_wts)\n","\n","    print(\"hyperparams tuning:\", grid_result.best_score_,grid_result.best_params_)\n","\n","    bst_params = grid_result.best_params_\n","    layer = bst_params['layers']\n","    node = bst_params['nodes']\n","    dropout = bst_params['dropout']\n","    n_epoch = bst_params['epochs']\n","    bsize = bst_params['batch_size']\n","    act = bst_params['activation']\n","    opt = bst_params['optimizer']\n","\n","    return layer, node, dropout, n_epoch, bsize, act, opt\n"]},{"cell_type":"markdown","metadata":{"id":"usuZEQZNRAXx"},"source":["Fit expectation & Fit quantiles"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74Utl239QqZz"},"outputs":[],"source":["\n","\n","\n","def fit_expectation(obj_name, df, use_covars, df_pred, is_class, is_tune, param_grid, transfer_weights, df_val=None):\n","    \"\"\"E(Y|X=x,A=a) or E(Y|X=x,S=s,A=a)\n","    (model separated by A)\n","    \"\"\"\n","\n","    df_tr = df.copy()\n","    df_te = df_pred.copy()\n","    if df_val is not None:\n","        df_va = df_val.copy()\n","\n","    # fit on df0/df1\n","    if is_class:\n","        loss_fn = 'binary_crossentropy'\n","        act_out = 'sigmoid'\n","    else:\n","        loss_fn = \"mean_squared_error\"\n","        act_out = None\n","\n","    if is_tune:\n","        layer, node, dropout, n_epoch, bsize, act, opt = \\\n","                hyper_tuning(df_tr[use_covars], df_tr[\"Y\"], transfer_weights, loss_fn, act_out,\n","                    param_grid, n_cv=5, n_jobs=1)\n","        Yhat, _, regr = keras_wrap(df_tr[use_covars], df_tr[\"Y\"], transfer_weights,\n","                            df_te[use_covars], loss_fn, act_out,\n","                            layer, node, dropout, n_epoch, bsize, act, opt,\n","                            val_split=None, is_early_stop=False, verb=0)\n","    else:\n","        Yhat, _, regr = keras_wrap(df_tr[use_covars], df_tr[\"Y\"], transfer_weights,\n","                            df_te[use_covars], loss_fn, act_out,\n","                            layer=2, node=1024, dropout=0.2, n_epoch=100, bsize=64, act=\"relu\", opt=\"Adam\",\n","                            val_split=0.2, is_early_stop=True, verb=0)\n","\n","    if df_val is not None:\n","        if not is_class: #NRMSE\n","            y_tr = regr.predict(df_tr[use_covars])\n","            rms = mean_squared_error(df_tr['Y'], y_tr, squared=False)\n","            met = rms / (np.max(df_tr['Y']) - np.min(df_tr['Y']))\n","            print(obj_name, \"evaluate on train: NRMSE\", met)\n","\n","            y_va = regr.predict(df_va[use_covars])\n","            rms = mean_squared_error(df_va['Y'], y_va, squared=False)\n","            met = rms / (np.max(df_va['Y']) - np.min(df_va['Y']))\n","            print(obj_name, \"evaluate on test : NRMSE\", met)\n","        elif is_class: #AUC\n","            y_tr = regr.predict(df_tr[use_covars])\n","            met = roc_auc_score(df_tr['Y'], y_tr)\n","            print(obj_name, \"evaluate on train: AUC\", met)\n","\n","            y_va = regr.predict(df_va[use_covars])\n","            met = roc_auc_score(df_va['Y'], y_va)\n","            print(obj_name, \"evaluate on test : AUC\", met)\n","\n","    return(Yhat, regr)\n","\n","\n","def fit_quantile(x_train, train_labels, x_test, q, is_tune, param_grid, transfer_weights):\n","    \"\"\"\n","    quantile regression: yhat | X,A\n","    \"\"\"\n","\n","    def tilted_loss(q, y, f):\n","        \"\"\"quantile loss for Keras\"\"\"\n","\n","        e = (y - f)\n","        return keras.backend.mean(keras.backend.maximum(q * e, (q - 1) * e), axis=-1)\n","\n","    loss_fn = lambda y, f: tilted_loss(q, y, f)\n","    act_out = None\n","\n","    if is_tune:\n","        layer, node, dropout, n_epoch, bsize, act, opt = \\\n","                    hyper_tuning(x_train, train_labels, transfer_weights, loss_fn, act_out,\n","                        param_grid, n_cv=5, n_jobs=1)\n","        pred_test, _, model = keras_wrap(x_train, train_labels, transfer_weights,\n","                        x_test, loss_fn, act_out,\n","                        layer, node, dropout, n_epoch, bsize, act, opt,\n","                        val_split=None, is_early_stop=False, verb=0)\n","    else:\n","        pred_test, _, model = keras_wrap(x_train, train_labels, transfer_weights, x_test, loss_fn, act_out,\n","                layer=2, node=1024, dropout=0.2, n_epoch=100, bsize=64, act=\"relu\",\n","                opt=\"Adam\", val_split=0.2, is_early_stop=True, verb=0)\n","\n","    return model, pred_test\n","\n","\n","\n","def fit_infinite(df, X_names, XS_names, fit):\n","    df_use = df.copy()\n","\n","    S_names = list( set(XS_names) - set(X_names) )\n","\n","    if len(S_names) == 1:\n","        s_min = int(np.min(df_use[\"S\"]))\n","        s_max = int(np.max(df_use[\"S\"]))\n","        s_grid = range(s_min, s_max+1)\n","    else:\n","        s_lst = []\n","        for s_var in S_names:\n","            s_min = int(np.min(df_use[s_var]))\n","            s_max = int(np.max(df_use[s_var]))\n","            s_grid = range(s_min, s_max+1)\n","            s_lst.append(np.array(s_grid))\n","        s_grid = s_lst.copy()\n","        print(\"s_grid:\", s_grid)\n","\n","    X_df = df_use[X_names].copy()\n","    idx = range(X_df.shape[0])\n","    X_df[\"idx\"] = idx\n","\n","    if len(S_names) == 1:\n","        idx_s = np.array(np.meshgrid(idx, s_grid)).reshape(2, len(idx)*len(s_grid)).T\n","        idx_s = pd.DataFrame(idx_s, columns = ['idx','S'])\n","    else:\n","        idx_s_lst = [np.array(idx)] + s_grid\n","        idx_s = np.array(np.meshgrid(*np.array(idx_s_lst, dtype=object))).reshape(1+len(S_names), -1).T\n","        idx_s = pd.DataFrame(idx_s, columns = ['idx']+S_names)\n","\n","    aug_df = X_df.merge(idx_s, on='idx', how='left').copy()\n","\n","    # predict Y\n","    aug_df[\"pred\"] = fit.predict(aug_df[XS_names])#????\n","\n","    smry = aug_df.groupby(by=\"idx\", as_index=False).agg({\"pred\":[\"min\"]})\n","\n","    return(smry.loc[:,(\"pred\",\"min\")].values.copy())\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"imCv008FRiDa"},"source":["Get label & weight for classification\n","\n","Binary classification with sample weights for decision rule by Keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVLeDFf3RR4X"},"outputs":[],"source":["\n","def get_label_wt(g1, g0):\n","    \"\"\"get label & weight for classification\"\"\"\n","    c1 = g1\n","    c2 = g0\n","    label = np.sign(c1 - c2)\n","    label = np.where(label==1, 1, 0)\n","\n","    weight = np.abs(c1 - c2)\n","\n","\n","    return(label, weight)\n","\n","\n","\n","\n","def class_pred(df_train, df_test, use_covars, label, weight, s_type, is_tune, param_grid,transfer_weights):\n","    \"\"\"binary classification with sample weights for decision rule by Keras\n","    \"\"\"\n","\n","    df_tr = df_train.copy()\n","    df_te = df_test.copy()\n","\n","    loss_fn = 'binary_crossentropy'\n","    act_out = 'sigmoid'\n","\n","    # classification (train & pred)\n","    if is_tune:\n","        layer, node, dropout, n_epoch, bsize, act, opt = \\\n","                    hyper_tuning(df_tr[use_covars], label, weight*transfer_weights, loss_fn, act_out,\n","                        param_grid, n_cv=5, n_jobs=1)\n","        prob_test, prob_train, model = keras_wrap(df_tr[use_covars], label, weight*transfer_weights,\n","                        df_te[use_covars], loss_fn, act_out,\n","                        layer, node, dropout, n_epoch, bsize, act, opt,\n","                        val_split=None, is_early_stop=False, verb=0)\n","    else:\n","        prob_test, prob_train, model = keras_wrap(df_tr[use_covars], label, weight*transfer_weights,\n","                        df_te[use_covars], loss_fn, act_out,\n","                        layer=2, node=1024, dropout=0.2, n_epoch=100, bsize=64, act=\"relu\",\n","                        opt=\"Adam\", val_split=0.2, is_early_stop=True, verb=0)\n","\n","\n","    pred_test = np.where(prob_test>0.5, \"yes\", \"no\")\n","    pred_train = np.where(prob_train>0.5, \"yes\", \"no\")\n","\n","    # print table\n","    print(\"pred_train table:\", collections.Counter(pred_train.ravel()))\n","    print(\"pred_test table:\", collections.Counter(pred_test.ravel()))\n","    if s_type == \"disc\": #disc S\n","        df_tr[\"pred_train\"] = pred_train\n","        df_te[\"pred_test\"] = pred_test\n","        print(pd.crosstab(df_tr[\"S\"], df_tr[\"pred_train\"], normalize='index'))\n","        print(pd.crosstab(df_te[\"S\"], df_te[\"pred_test\"], normalize='index'))\n","    print(\"=\"*30)\n","\n","    return(pred_test, pred_train, model)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PrgwLxPZVHJc"},"source":["get_A_names & add A_pred to df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDpxsVHKR6nz"},"outputs":[],"source":["\n","def get_A_names(is_base, is_exp, is_expOracle, is_qua, is_inf):\n","    A_names = list()\n","    A_names.append(\"A_obs\")\n","    if is_base:\n","        A_names.append(\"A_base\")\n","    if is_exp:\n","        A_names.append(\"A_exp\")\n","    if is_expOracle:\n","        A_names.append(\"A_expOracle\")\n","    if is_qua or is_inf:\n","        A_names.append(\"A_qua\")\n","    print(\"A_names:\",A_names)\n","\n","    return A_names\n","\n","\n","def add_Apred_Midx(df, A_names, A_base, A_exp, A_expOracle, A_qua):\n","    df_use = df.copy()\n","\n","    # add A_pred to df\n","    df_use[\"A_obs\"] = np.where(df_use[\"A\"]==1, \"yes\",\"no\")\n","    if \"A_base\" in A_names:\n","        df_use[\"A_base\"] = A_base\n","    if \"A_exp\" in A_names:\n","        df_use[\"A_exp\"] = A_exp\n","    if \"A_expOracle\" in A_names:\n","        df_use[\"A_expOracle\"] = A_expOracle\n","    if \"A_qua\" in A_names:\n","        df_use[\"A_qua\"] = A_qua\n","\n","    return df_use.copy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3AOdF0eFsXC"},"outputs":[],"source":["\n","def get_A_names(is_base, is_exp, is_MI, is_expOracle, is_qua, is_inf):\n","    A_names = list()\n","    A_names.append(\"A_obs\")\n","    if is_base:\n","        A_names.append(\"A_base\")\n","    if is_exp:\n","        A_names.append(\"A_exp\")\n","    if is_MI:\n","        A_names.append(\"A_MI\")\n","    if is_expOracle:\n","        A_names.append(\"A_expOracle\")\n","    if is_qua or is_inf:\n","        A_names.append(\"A_qua\")\n","    print(\"A_names:\",A_names)\n","\n","    return A_names\n","\n","\n","def add_Apred_Midx(df, A_names, A_base, A_exp, A_MI, A_expOracle, A_qua):\n","    df_use = df.copy()\n","\n","    # add A_pred to df\n","    df_use[\"A_obs\"] = np.where(df_use[\"A\"]==1, \"yes\",\"no\")\n","    if \"A_base\" in A_names:\n","        df_use[\"A_base\"] = A_base\n","    if \"A_exp\" in A_names:\n","        df_use[\"A_exp\"] = A_exp\n","    if \"A_MI\" in A_names:\n","        df_use[\"A_MI\"] = A_MI\n","    if \"A_expOracle\" in A_names:\n","        df_use[\"A_expOracle\"] = A_expOracle\n","    if \"A_qua\" in A_names:\n","        df_use[\"A_qua\"] = A_qua\n","\n","    return df_use.copy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQAbXEnXUVC8"},"outputs":[],"source":["def optimal_AS_inf(df, proj, is_sim, s_type, X_names, XS_names, y_fn, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1, df_pop):\n","    \"\"\"\n","    Used for real data with disc S | simulation with disc/cont S\n","\n","    get df with minority index\n","        input:  df_train for E(Y|X,S,A) and df_test for Yhat -> optimal A and S\n","        output: df_test with optimal A and S\n","    \"\"\"\n","    df_use = df.copy()\n","    df_sgrid = df_pop.copy()\n","\n","    # S grid from empirical test data\n","    a_grid = range(0, 2)\n","    if s_type == \"disc\":\n","        s_grid = range(0, 2)\n","        assert len(s_grid) == 2\n","    elif s_type == \"cont\":\n","\n","        s_grid = df_sgrid[\"S\"].sort_values().values.copy()\n","\n","        assert len(s_grid) > 2\n","    else:\n","        assert(1 == 0)\n","    print(\"len(s_grid):\", len(s_grid))\n","\n","    # augment X with A&S grid\n","    a_s_grid = np.array(np.meshgrid(a_grid, s_grid)).reshape(2, len(a_grid)*len(s_grid)).T\n","    a_s_grid = pd.DataFrame(a_s_grid, columns = ['A','S'])\n","    idx_as = range(a_s_grid.shape[0])\n","    a_s_grid[\"idx_as\"] = idx_as\n","\n","    X_df = df_use[X_names].copy()\n","    idx_i = range(X_df.shape[0])\n","    X_df[\"idx_i\"] = idx_i\n","\n","    idxs = np.array(np.meshgrid(idx_as, idx_i)).reshape(2, len(idx_as)*len(idx_i)).T\n","    idxs = pd.DataFrame(idxs, columns = ['idx_as','idx_i'])\n","\n","    aug_df = X_df.merge(idxs, on='idx_i', how='left').copy()\n","    aug_df = aug_df.merge(a_s_grid, on='idx_as', how='left').copy()\n","\n","    # aug_df: each X with all combinations of (S,A)\n","    aug_df = aug_df[['idx_i','A','S']+X_names].copy()\n","    assert(aug_df.shape[0] == len(a_grid)*len(s_grid)*df_use.shape[0])\n","    aug_df_a0 = aug_df[aug_df[\"A\"]==0].reset_index(drop=True).copy()\n","    aug_df_a1 = aug_df[aug_df[\"A\"]==1].reset_index(drop=True).copy()\n","\n","    # get E[Y|X,S,A=d(x)]\n","    if is_sim: # from true distn (simulation)\n","        aug_df_a0[\"EY\"] = aug_df_a0.eval(y_fn)\n","        aug_df_a1[\"EY\"] = aug_df_a1.eval(y_fn)\n","    else: # from fitted model (real data)\n","        aug_df_a0[\"EY\"] = fit_Y_XS_A0.predict(aug_df_a0[XS_names])\n","        aug_df_a1[\"EY\"] = fit_Y_XS_A1.predict(aug_df_a1[XS_names])\n","\n","    # find quantile/inf wrt S\n","    def quan(x):\n","        return x.quantile(qua_use)\n","\n","    if s_type == \"disc\":\n","        agg_fn1 = {\"EY\":[\"min\"]}\n","        agg_fn2 = {\"EY_min\":\"max\"}\n","        agg_fn3 = {\"EY_min\":\"min\"}\n","    else:\n","        agg_fn1 = {\"EY\":[quan]}\n","        agg_fn2 = {\"EY_quan\":\"max\"}\n","        agg_fn3 = {\"EY_quan\":\"min\"}\n","\n","    # min E(Y|X,S,A) separate for A=0 and A=1\n","    min_a0 = aug_df_a0.groupby(by=\"idx_i\", as_index=False).agg(agg_fn1)\n","    min_a0.columns = [\"idx_i\"] + ['_'.join(col) for col in min_a0.columns[1:]]\n","    aug_df_a0_min = aug_df_a0.merge(min_a0, on='idx_i').copy()\n","    min_a1 = aug_df_a1.groupby(by=\"idx_i\", as_index=False).agg(agg_fn1)\n","    min_a1.columns = [\"idx_i\"] + ['_'.join(col) for col in min_a1.columns[1:]]\n","    aug_df_a1_min = aug_df_a1.merge(min_a1, on='idx_i').copy()\n","    aug_df_cat = pd.concat([aug_df_a0_min, aug_df_a1_min]).reset_index(drop=True).copy()\n","\n","    # find S*: min-min E(Y|X,S,A)\n","    min_df = aug_df_cat.groupby(by=\"idx_i\", as_index=False).agg(agg_fn3).copy()\n","    min_min_df = aug_df_cat.merge(min_df, on='idx_i',suffixes=('', '_min'))\n","\n","    # get S_opt (S*) vulnerable groups\n","    if s_type == \"disc\":\n","        msk1 = min_min_df['EY_min'] == min_min_df['EY_min_min']\n","        msk2 = min_min_df['EY'] <= min_min_df['EY_min_min']\n","    elif s_type == \"cont\":\n","        msk1 = min_min_df['EY_quan'] == min_min_df['EY_quan_min']\n","        msk2 = min_min_df['EY'] <= min_min_df['EY_quan_min']\n","    df_filter = min_min_df[msk1 & msk2].reset_index(drop=True).copy() # may contains more rows than df_use\n","    assert( df_filter.shape[0] >= df_use.shape[0] )\n","\n","    X_df = df_use[X_names].copy()\n","    X_df[\"idx_i\"] = idx_i\n","    df_S_opt = X_df.merge(df_filter, on=['idx_i']+X_names, how=\"left\").copy()\n","    df_S_opt.rename(columns={\"S\":\"S_opt\"}, inplace=True)\n","    # df_S_opt should be used to plot tree (find pattern of X->S)\n","\n","    # link S* to observed data\n","    XS_df = df_use[X_names+['S']].copy()\n","    XS_df[\"idx_i\"] = idx_i\n","    merge_df = XS_df.merge(df_S_opt, left_on=['idx_i','S']+X_names, right_on=['idx_i','S_opt']+X_names, how=\"left\").copy()\n","    merge_df = merge_df.drop_duplicates(subset=['idx_i']+X_names, keep='first').reset_index(drop=True).copy()\n","\n","    S_opt = merge_df['S_opt'].copy() # if not NaN, this subject is vulnerable\n","    assert( S_opt.shape[0] == X_df.shape[0] )\n","\n","    # minor_index\n","    merge_df['M_opt'] = np.where(merge_df['S']==merge_df['S_opt'], 1, 0)\n","    print(\"M_opt 1:minor 0:rest\\n\", merge_df['M_opt'].value_counts(dropna=False))\n","    M_opt = merge_df['M_opt'].copy()\n","    assert( M_opt.shape[0] == X_df.shape[0] )\n","\n","    return S_opt, M_opt, df_S_opt # valid_df should be used to plot tree (find pattern of X->S)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6ymAGvCarsQ"},"outputs":[],"source":["def optimal_AS_inf_train(df, proj, is_sim, s_type, X_names, XS_names, y_fn, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1, df_pop):\n","    \"\"\"\n","    Used for real data with disc S | simulation with disc/cont S\n","\n","    get df with minority index\n","        input:  df_train for E(Y|X,S,A) and df_test for Yhat -> optimal A and S\n","        output: df_test with optimal A and S\n","    \"\"\"\n","    df_use = df.copy()\n","    df_sgrid = df_pop.copy()\n","\n","\n","    # S grid from empirical test data\n","    a_grid = range(0, 2)\n","    if s_type == \"disc\":\n","        s_grid = range(0, 2)\n","        assert len(s_grid) == 2\n","    elif s_type == \"cont\":\n","\n","        s_grid = df_sgrid[\"S\"].sort_values().values.copy()\n","\n","        assert len(s_grid) > 2\n","    else:\n","        assert(1 == 0)\n","    print(\"len(s_grid):\", len(s_grid))\n","\n","    # augment X with A&S grid\n","    a_s_grid = np.array(np.meshgrid(a_grid, s_grid)).reshape(2, len(a_grid)*len(s_grid)).T\n","    a_s_grid = pd.DataFrame(a_s_grid, columns = ['A','S'])\n","    idx_as = range(a_s_grid.shape[0])\n","    a_s_grid[\"idx_as\"] = idx_as\n","\n","    X_df = df_use[X_names].copy()\n","    idx_i = range(X_df.shape[0])\n","    X_df[\"idx_i\"] = idx_i\n","\n","    idxs = np.array(np.meshgrid(idx_as, idx_i)).reshape(2, len(idx_as)*len(idx_i)).T\n","    idxs = pd.DataFrame(idxs, columns = ['idx_as','idx_i'])\n","\n","    aug_df = X_df.merge(idxs, on='idx_i', how='left').copy()\n","    aug_df = aug_df.merge(a_s_grid, on='idx_as', how='left').copy()\n","\n","    # aug_df: each X with all combinations of (S,A)\n","    aug_df = aug_df[['idx_i','A','S']+X_names].copy()\n","    assert(aug_df.shape[0] == len(a_grid)*len(s_grid)*df_use.shape[0])\n","    aug_df_a0 = aug_df[aug_df[\"A\"]==0].reset_index(drop=True).copy()\n","    aug_df_a1 = aug_df[aug_df[\"A\"]==1].reset_index(drop=True).copy()\n","\n","    # get E[Y|X,S,A=d(x)]\n","    if is_sim: # from true distn (simulation)\n","        aug_df_a0[\"EY\"] = aug_df_a0.eval(y_fn)\n","        aug_df_a1[\"EY\"] = aug_df_a1.eval(y_fn)\n","    else: # from fitted model (real data)\n","        aug_df_a0[\"EY\"] = fit_Y_XS_A0.predict(aug_df_a0[XS_names])\n","        aug_df_a1[\"EY\"] = fit_Y_XS_A1.predict(aug_df_a1[XS_names])\n","\n","    # find quantile/inf wrt S\n","    def quan(x,A):\n","        return x.quantile(qua_use)\n","\n","    if s_type == \"disc\":\n","        agg_fn1 = {\"EY\":[\"min\"]}\n","        agg_fn2 = {\"EY_min\":\"max\"}\n","        agg_fn3 = {\"EY_min\":\"min\"}\n","    else:\n","        agg_fn1 = {\"EY\":[quan]}\n","        agg_fn2 = {\"EY_quan\":\"max\"}\n","        agg_fn3 = {\"EY_quan\":\"min\"}\n","\n","    # min E(Y|X,S,A) separate for A=0 and A=1\n","    min_a0 = aug_df_a0.groupby(by=\"idx_i\", as_index=False).agg(agg_fn1)\n","    min_a0.columns = [\"idx_i\"] + ['_'.join(col) for col in min_a0.columns[1:]]\n","    aug_df_a0_min = aug_df_a0.merge(min_a0, on='idx_i').copy()\n","    min_a1 = aug_df_a1.groupby(by=\"idx_i\", as_index=False).agg(agg_fn1)\n","    min_a1.columns = [\"idx_i\"] + ['_'.join(col) for col in min_a1.columns[1:]]\n","    aug_df_a1_min = aug_df_a1.merge(min_a1, on='idx_i').copy()\n","    aug_df_cat = pd.concat([aug_df_a0_min, aug_df_a1_min]).reset_index(drop=True).copy()\n","\n","    # find S*: min-min E(Y|X,S,A)\n","    min_df = aug_df_cat.groupby(by=\"idx_i\", as_index=False).agg(agg_fn3).copy()\n","    min_min_df = aug_df_cat.merge(min_df, on='idx_i',suffixes=('', '_min'))\n","\n","    # get S_opt (S*) vulnerable groups\n","    if s_type == \"disc\":\n","        msk1 = min_min_df['EY_min'] == min_min_df['EY_min_min']\n","        msk2 = min_min_df['EY'] <= min_min_df['EY_min_min']\n","    elif s_type == \"cont\":\n","        msk1 = min_min_df['EY_quan'] == min_min_df['EY_quan_min']\n","        msk2 = min_min_df['EY'] <= min_min_df['EY_quan_min']\n","    df_filter = min_min_df[msk1 & msk2].reset_index(drop=True).copy() # may contains more rows than df_use\n","    assert( df_filter.shape[0] >= df_use.shape[0] )\n","\n","    X_df = df_use[X_names].copy()\n","    X_df[\"idx_i\"] = idx_i\n","    df_S_opt = X_df.merge(df_filter, on=['idx_i']+X_names, how=\"left\").copy()\n","    df_S_opt.rename(columns={\"S\":\"S_opt\"}, inplace=True)\n","    # df_S_opt should be used to plot tree (find pattern of X->S)\n","\n","    # link S* to observed data\n","    XS_df = df_use[X_names+['S']].copy()\n","    XS_df[\"idx_i\"] = idx_i\n","    merge_df = XS_df.merge(df_S_opt, left_on=['idx_i','S']+X_names, right_on=['idx_i','S_opt']+X_names, how=\"left\").copy()\n","    merge_df = merge_df.drop_duplicates(subset=['idx_i']+X_names, keep='first').reset_index(drop=True).copy()\n","\n","    S_opt = merge_df['S_opt'].copy() # if not NaN, this subject is vulnerable\n","    assert( S_opt.shape[0] == X_df.shape[0] )\n","\n","    # minor_index\n","    merge_df['M_opt'] = np.where(merge_df['S']==merge_df['S_opt'], 1, 0)\n","    print(\"M_opt 1:minor 0:rest\\n\", merge_df['M_opt'].value_counts(dropna=False))\n","    M_opt = merge_df['M_opt'].copy()\n","    assert( M_opt.shape[0] == X_df.shape[0] )\n","\n","    return S_opt, M_opt, df_S_opt # valid_df should be used to plot tree (find pattern of X->S)\n"]},{"cell_type":"markdown","metadata":{"id":"_orUDAirWBOn"},"source":["rule of vulnerable group (X -> S)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6il4xINWCmr"},"outputs":[],"source":["\n","def minor_rule(df, use_covars, s_type, S_name):\n","    \"\"\"use decision tree to print out rule of vulnerable group (X -> S)\n","        only apply when S is discrete\n","    \"\"\"\n","    df_use = df.copy()\n","\n","    df_use[\"S_minor\"] = np.where(np.isnan(df_use[S_name]), -999,df_use[S_name])\n","    print('df_use[\"S_minor\"]', df_use[\"S_minor\"].value_counts(dropna=False))\n","\n","    if s_type == \"disc\":\n","        clf = DecisionTreeClassifier(max_depth=3, random_state=4211)\n","    else:\n","        clf = DecisionTreeRegressor(max_depth=3, random_state=4211)\n","\n","    clf.fit(df_use[use_covars], df_use[\"S_minor\"])\n","\n","    # export the decision rules\n","    tree_rules = export_text(clf, feature_names = list(use_covars))\n","    print(S_name)\n","    print(tree_rules)\n","\n","    clf.predict(df_use[use_covars])\n","\n","    return\n"]},{"cell_type":"markdown","metadata":{"id":"hSK3inxVWE6N"},"source":["Expect reward & pred Y & value function for minor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTSnpQsDWryC"},"outputs":[],"source":["\n","\n","def expect_reward(df, A_name, minor_name):\n","    df_use = df.copy()\n","\n","    minor_df = df_use[df_use[minor_name]==1].reset_index(drop=True).copy()\n","    rest_df = df_use[df_use[minor_name]==0].reset_index(drop=True).copy()\n","\n","    numer = df_use[\"Y\"] * (df_use[\"A_obs\"] == df_use[A_name]) / df_use[\"ps\"]\n","    denom = (df_use[\"A_obs\"] == df_use[A_name]) / df_use[\"ps\"]\n","    reward_all = np.sum(numer) / np.sum(denom)\n","\n","    if not minor_df.empty:\n","        numer = minor_df[\"Y\"] * (minor_df[\"A_obs\"] == minor_df[A_name]) / minor_df[\"ps\"]\n","        denom = (minor_df[\"A_obs\"] == minor_df[A_name]) / minor_df[\"ps\"]\n","        reward_minor = np.sum(numer) / np.sum(denom)\n","    else:\n","        reward_minor = np.nan\n","\n","    if not rest_df.empty:\n","        numer = rest_df[\"Y\"] * (rest_df[\"A_obs\"] == rest_df[A_name]) / rest_df[\"ps\"]\n","        denom = (rest_df[\"A_obs\"] == rest_df[A_name]) / rest_df[\"ps\"]\n","        reward_rest = np.sum(numer) / np.sum(denom)\n","    else:\n","        reward_rest = np.nan\n","\n","    print(A_name, \"reward  \",\n","          \"all:\",round(reward_all,3),\n","          \"minor:\",round(reward_minor,3),\n","          \"rest:\",round(reward_rest,3))\n","\n","    return reward_all, reward_minor, reward_rest\n","\n","\n","def get_pred_Y(df, A_name, XS_names, y_fn, model_sxa0, model_sxa1):\n","    \"\"\"generate Y based on predicted A (treatment assignment)\n","    model: Y|X,S,A=a\n","    should use unnormalized X & S for simulation & normalized for real data!\n","    \"\"\"\n","    df_use = df[XS_names].copy()\n","\n","    df_use[\"A\"] = np.where(df[A_name]==\"yes\", 1,0)\n","    if y_fn is not None:\n","        Y_pred = df_use.eval(y_fn)\n","    else:\n","        Y_pred0 = model_sxa0.predict(df_use[XS_names]).flatten()\n","        Y_pred1 = model_sxa1.predict(df_use[XS_names]).flatten()\n","        Y_pred = np.where(df_use[\"A\"]==0, Y_pred0, Y_pred1)\n","    assert len(Y_pred.shape)==1\n","\n","    return Y_pred\n","\n","\n","def get_value_fn(df, A_name, XS_names, minor_name, y_fn, model_sxa0, model_sxa1):\n","    \"\"\"get value function for subgroup of interest\"\"\"\n","    df_use = df.copy()\n","\n","    df_use[\"Y_pred\"] = get_pred_Y(df_use, A_name, XS_names, y_fn, model_sxa0, model_sxa1)\n","\n","    minor_df = df_use[df_use[minor_name]==1].reset_index(drop=True).copy()\n","    rest_df = df_use[df_use[minor_name]==0].reset_index(drop=True).copy()\n","\n","    value_all = np.mean(df_use[\"Y_pred\"].values)\n","\n","    if not minor_df.empty:\n","        value_minor = np.mean(minor_df[\"Y_pred\"].values)\n","        minorPct=minor_df.shape[0]/df_use.shape[0]\n","    else:\n","        value_minor = np.nan\n","        minorPct=np.nan\n","\n","    if not rest_df.empty:\n","        value_rest = np.mean(rest_df[\"Y_pred\"].values)\n","    else:\n","        value_rest = np.nan\n","\n","    print(A_name, \"value   \",\n","          \"all:\",round(value_all,3),\n","          \"minor:\",round(value_minor,3),\n","          \"rest:\",round(value_rest,3),\n","          \"minorPct:\",round(minorPct,3))\n","\n","    return value_all, value_minor, value_rest, minorPct"]},{"cell_type":"markdown","metadata":{"id":"UBOZasYuW-67"},"source":["Objective Quantile & Inf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blaBlL_hXEF6"},"outputs":[],"source":["\n","def get_obj_qua(df, X_names, s_type, A_name, minor_name, fit_qua0, fit_qua1):\n","    \"\"\"\n","    Used for real data with cont S (no need S grid)\n","\n","    df includes X,A_test (A_test is from prediction)\n","    objective: E[Gs{E[Y|X,S,A=d(x)]}]\n","    -> Quantile_regress Y|X,A -> E[.]\n","    -> take avg over all subjects\n","    \"\"\"\n","    df_use = df.copy()\n","    del df_use['A']\n","    del df_use['Y']\n","\n","    if s_type != \"cont\":\n","        assert(1 == 0)\n","\n","    Y_pred0 = fit_qua0.predict(df_use[X_names]).flatten()\n","    Y_pred1 = fit_qua1.predict(df_use[X_names]).flatten()\n","\n","    df_use[\"A\"] = np.where(df_use[A_name]==\"yes\", 1,0) #contains X & A\n","    df_use[\"Y\"] = np.where(df_use[\"A\"]==0, Y_pred0, Y_pred1)\n","\n","    minor_df = df_use[df_use[minor_name]==1].reset_index(drop=True).copy()\n","    rest_df = df_use[df_use[minor_name]==0].reset_index(drop=True).copy()\n","\n","    obj_all = np.mean(df_use[\"Y\"].values)\n","\n","    if not minor_df.empty:\n","        obj_minor = np.mean(minor_df[\"Y\"].values)\n","    else:\n","        obj_minor = np.nan\n","\n","    if not rest_df.empty:\n","        obj_rest = np.mean(rest_df[\"Y\"].values)\n","    else:\n","        obj_rest = np.nan\n","\n","    print(A_name, \"obj     \",\n","          \"all:\",round(obj_all,3),\n","          \"minor:\",round(obj_minor,3),\n","          \"rest:\",round(obj_rest,3))\n","\n","    return obj_all, obj_minor, obj_rest\n","\n","\n","def get_obj_inf(df, X_names, XS_names, A_name, minor_name, y_fn, s_type, is_class, qua_use, model_sxa0, model_sxa1):\n","    \"\"\"\n","    Used for real data with disc S | simulation with disc/cont S\n","    df includes X,A_test (A_test is from prediction)\n","    objective: E[Gs{E[Y|X,S,A=d(x)]}]\n","    -> get E[Y|X,S,A=d(x)] from true distn (simulation) or fitted model (real data)\n","    -> given X & A, generate S from empirical test data\n","    -> find quan/inf wrt S\n","    -> take avg over all subjects\n","    \"\"\"\n","    df_use = df.copy()\n","    df_sgrid = df.copy()\n","\n","\n","    # S grid from empirical test data\n","    if s_type == \"disc\": # real/simulation with disc S\n","        S_names = list( set(XS_names) - set(X_names) )\n","        if len(S_names) == 1:\n","            s_min = int(np.min(df_use[\"S\"]))\n","            s_max = int(np.max(df_use[\"S\"]))\n","            s_grid = range(s_min, s_max+1)\n","            if y_fn is None:\n","                s_grid = [s_grid]\n","        else:\n","            s_lst = []\n","            for s_var in S_names:\n","                s_min = int(np.min(df_use[s_var]))\n","                s_max = int(np.max(df_use[s_var]))\n","                s_grid = range(s_min, s_max+1)\n","                s_lst.append(np.array(s_grid))\n","            s_grid = s_lst.copy()\n","    elif s_type == \"cont\" and y_fn is not None: # simulation with cont S\n","        s_grid = df[\"S\"].values.copy()\n","        assert len(s_grid) > 2\n","    else:\n","        assert(1 == 0)\n","\n","\n","    # S grid from empirical test data\n","    a_grid = range(0, 2)\n","    if s_type == \"disc\":\n","        s_grid = range(0, 2)\n","        assert len(s_grid) == 2\n","    elif s_type == \"cont\":\n","\n","        s_grid = df_sgrid[\"S\"].sort_values().values.copy()\n","\n","        assert len(s_grid) > 2\n","\n","\n","\n","    # augment X&A with S grid\n","    df_XA = df_use[[minor_name]+X_names].copy() #contains X\n","    df_XA[\"A\"] = np.where(df_use[A_name]==\"yes\", 1,0) #contains X & A\n","    idx = range(df_XA.shape[0])\n","    df_XA[\"idx\"] = idx\n","\n","    if y_fn is None:\n","        idx_s_lst = [np.array(idx)] + s_grid\n","        idx_s = np.array(np.meshgrid(*np.array(idx_s_lst, dtype=object))).reshape(1+len(S_names), -1).T\n","        idx_s = pd.DataFrame(idx_s, columns = ['idx']+S_names)\n","    else: # if remove, may occur error in .eval(y_fn) # this works as long as there is no multi-S in simulation\n","        idx_s = np.array(np.meshgrid(idx, s_grid)).reshape(2, len(idx)*len(s_grid)).T\n","        idx_s = pd.DataFrame(idx_s, columns = ['idx','S'])\n","\n","    aug_df = df_XA.merge(idx_s, on='idx', how='left').reset_index(drop=True).copy()\n","\n","    # get E[Y|X,S,A=d(x)]\n","    if y_fn is not None: # from true distn (simulation)\n","        aug_df[\"Y\"] = aug_df.eval(y_fn)\n","    else: # from fitted model (real data)\n","        Y_pred0 = model_sxa0.predict(aug_df[XS_names]).flatten()\n","        Y_pred1 = model_sxa1.predict(aug_df[XS_names]).flatten()\n","        aug_df[\"Y\"] = np.where(aug_df[\"A\"]==0, Y_pred0, Y_pred1)\n","\n","    # find quantile/inf wrt S\n","    def quan(x):\n","        return x.quantile(qua_use)\n","\n","    minor_df = aug_df[aug_df[minor_name]==1].reset_index(drop=True).copy()\n","    rest_df = aug_df[aug_df[minor_name]==0].reset_index(drop=True).copy()\n","\n","    # Gs{E[Y|X,S,A=d(x)]}\n","    if (s_type == \"cont\"):\n","        agg_fn = {\"Y\": quan}\n","    elif (s_type == \"disc\"):\n","        agg_fn = {\"Y\": \"min\"}\n","\n","    smry_all = aug_df.groupby(by=\"idx\", as_index=False).agg(agg_fn)\n","    smry_minor = minor_df.groupby(by=\"idx\", as_index=False).agg(agg_fn)\n","    smry_rest = rest_df.groupby(by=\"idx\", as_index=False).agg(agg_fn)\n","\n","    # obj: E{G(.)}\n","    obj_all = np.mean(smry_all.loc[:,\"Y\"].values)\n","\n","    if not minor_df.empty:\n","        obj_minor = np.mean(smry_minor.loc[:,\"Y\"].values)\n","    else:\n","        obj_minor = np.nan\n","\n","    if not rest_df.empty:\n","        obj_rest = np.mean(smry_rest.loc[:,\"Y\"].values)\n","    else:\n","        obj_rest = np.nan\n","\n","    print(A_name, \"obj     \",\n","          \"all:\",round(obj_all,3),\n","          \"minor:\",round(obj_minor,3),\n","          \"rest:\",round(obj_rest,3))\n","\n","    return obj_all, obj_minor, obj_rest\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ba0Vxgv2XFTd"},"source":["dict mean & std & prop_treat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SD1gieW-XPyk"},"outputs":[],"source":["\n","def dict_mean(dict_list):\n","    mean_list = list()\n","    mean_dict = dict()\n","    for A_name in dict_list[0].keys():\n","        mean_dict[A_name] = dict()\n","        for metric in dict_list[0][A_name].keys():\n","            mean_dict[A_name][metric] = np.nansum([d[A_name][metric] for d in dict_list]) / len(dict_list)\n","            mean_list.append(mean_dict[A_name][metric])\n","\n","    mean_list = np.array(mean_list)\n","    mean_list = np.reshape(mean_list, (len(dict_list[0].keys()), -1)).T\n","    mean_df = pd.DataFrame(data=mean_list, columns=dict_list[0].keys())\n","    mean_df.index = list(dict_list[0][list(dict_list[0].keys())[0]].keys())\n","\n","    return mean_dict, mean_df\n","\n","\n","def dict_std(dict_list):\n","    \"\"\"\n","    Replicate this sampling test data procedure 100 times,\n","    estimate values of each ITR for each replicate,\n","    and get the averaged estimated values (standard error)\n","    \"\"\"\n","    mean_list = list()\n","    mean_dict = dict()\n","    for A_name in dict_list[0].keys():\n","        mean_dict[A_name] = dict()\n","        for metric in dict_list[0][A_name].keys():\n","            mean = np.nansum([d[A_name][metric] for d in dict_list]) / len(dict_list)\n","            variance = np.nansum([(d[A_name][metric] - mean)**2 for d in dict_list]) / len(dict_list)\n","            mean_dict[A_name][metric] = variance**0.5 / (len(dict_list))**0.5  #SE\n","            mean_list.append(mean_dict[A_name][metric])\n","\n","    mean_list = np.array(mean_list)\n","    mean_list = np.reshape(mean_list, (len(dict_list[0].keys()), -1)).T\n","    mean_df = pd.DataFrame(data=mean_list, columns=dict_list[0].keys())\n","    mean_df.index = list(dict_list[0][list(dict_list[0].keys())[0]].keys())\n","\n","    return mean_dict, mean_df\n","\n","\n","def prop_treat(df, A_name, minor_name):\n","    df_use = df.reset_index(drop=True).copy()\n","\n","    minor_df = df_use[df_use[minor_name]==1].reset_index(drop=True).copy()\n","    rest_df = df_use[df_use[minor_name]==0].reset_index(drop=True).copy()\n","\n","    prop_all = df_use[df_use[A_name]==\"yes\"].shape[0] / df_use.shape[0]\n","\n","    if not minor_df.empty:\n","        prop_minor = minor_df[minor_df[A_name]==\"yes\"].shape[0] / minor_df.shape[0]\n","    else:\n","        prop_minor = np.nan\n","\n","    if not rest_df.empty:\n","        prop_rest = rest_df[rest_df[A_name]==\"yes\"].shape[0] / rest_df.shape[0]\n","    else:\n","        prop_rest = np.nan\n","\n","    print(A_name, \"pr_treat\",\n","          \"all:\",round(prop_all,3),\n","          \"minor:\",round(prop_minor,3),\n","          \"rest:\",round(prop_rest,3))\n","\n","    return prop_all, prop_minor, prop_rest\n"]},{"cell_type":"markdown","metadata":{"id":"G1Es-vwcXaKB"},"source":["Evaluation Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21bdLO7oXcCP"},"outputs":[],"source":["\n","def eval_metrics(minor_name, res_df, A_names, X_names, XS_names, y_fn, s_type, is_class,\n","                    qua_use, fit_Y_XS_A0, fit_Y_XS_A1, fit_qua0, fit_qua1, res_df2):\n","    print(minor_name)\n","    print(\"=\"*30)\n","    res = dict()\n","    for A_name in A_names:\n","        res[A_name] = dict()\n","        # metrics\n","        prop_all, prop_minor, prop_rest = prop_treat(res_df, A_name, minor_name)\n","        reward_all, reward_minor, reward_rest = expect_reward(res_df, A_name, minor_name)\n","        value_all, value_minor, value_rest, minorPct = get_value_fn(res_df, A_name, XS_names, minor_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","        if s_type == \"disc\" or y_fn is not None: # real with disc S | sim with disc/cont S\n","            obj_all, obj_minor, obj_rest = get_obj_inf(res_df, X_names, XS_names, A_name, minor_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","        elif s_type == \"cont\": # real with cont S\n","            obj_all, obj_minor, obj_rest = get_obj_inf(res_df, X_names, s_type, A_name, minor_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","        else:\n","            assert(1 == 0)\n","\n","        minor_df_train = res_df2[res_df2[minor_name]==1].reset_index(drop=True).copy()\n","        if not minor_df_train.empty:\n","            minorPct_train=minor_df_train.shape[0]/res_df2.shape[0]\n","        else:\n","            minorPct_train=np.nan\n","\n","        # save\n","        res[A_name][\"prop_all\"] = prop_all;     res[A_name][\"prop_minor\"] = prop_minor;     res[A_name][\"prop_rest\"] = prop_rest\n","        res[A_name][\"reward_all\"] = reward_all; res[A_name][\"reward_minor\"] = reward_minor; res[A_name][\"reward_rest\"] = reward_rest\n","        res[A_name][\"value_all\"] = value_all;   res[A_name][\"value_minor\"] = value_minor;   res[A_name][\"value_rest\"] = value_rest\n","        res[A_name][\"minorPct_test\"] = minorPct; res[A_name][\"minorPct_train\"] = minorPct_train\n","        res[A_name][\"obj_all\"] = obj_all;       res[A_name][\"obj_minor\"] = obj_minor;       res[A_name][\"obj_rest\"] = obj_rest\n","        print(\"=\"*30)\n","    print(\"=\"*50)\n","    return res\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlC14ongEEsb"},"outputs":[],"source":["def gen_sim_data_RWDRCT(df,n):\n","\n","    Spprob=np.exp(-5+3*df['X1']-2*df['S'])/(1+np.exp(-5+3*df['X1']-2*df['S']))\n","    Sp=np.random.binomial(1,Spprob,n)\n","    df[\"Spprob\"]=Spprob\n","    df[\"Sp\"]=Sp\n","    return df\n","\n","N=20000\n","df,_ = gen_sim_data(N,\"cont\",\"complex\")\n","df= gen_sim_data_RWDRCT(df,N)\n","df_RCT=df[df[\"Sp\"]==1]\n","df_RWD=df[np.random.binomial(1,0.02,df.shape[0])==1]\n","\n","\n","def transfer_weight_obj(alpha):\n","    m=df_RWD.shape[0]\n","    df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[[\"X1\",\"X2\",\"S\"]]),axis=1)\n","    df_RWD_1X=np.concatenate((np.ones((df_RWD.shape[0],1)),df_RWD[[\"X1\",\"X2\",\"S\"]]),axis=1)\n","    return -(1/N)*np.sum(df_RCT_1X@alpha)+(1/m)*np.sum(np.log(1+np.exp(df_RWD_1X@alpha)))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6sv0gVG08gaw"},"source":["New Sim Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdZbWyStxdtu"},"outputs":[],"source":["\n","def gen_sim_data_RWDsplit(df,n):\n","    probMCAR=np.exp(0.5-1*df['X1']+0*df['X2'])/(1+np.exp(0.5-1*df['X1']+0*df['X2']))\n","    pMCAR=np.random.binomial(1,probMCAR,n)\n","    df[\"pMCAR\"]=pMCAR\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dy3n8YTY0yST"},"outputs":[],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import LogisticRegression\n","\n","from collections import Counter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMo-i5qPggGr"},"outputs":[],"source":["import cvxpy as cp\n","import numpy as np\n","\n","def entr(w):\n","    # Define the entropy function\n","    return cp.sum(cp.entr(w))\n","\n","def entr_balance(tol, x_in_rct, x_in_rwe):\n","    # the optimization problem for the nonparametric weighting method\n","    n = x_in_rct.shape[0]\n","    w = cp.Variable(n)\n","    p = x_in_rct.shape[1]-1\n","    objective = cp.Maximize(cp.sum(cp.entr(w)))\n","    constraints = [w >= 0, cp.sum(w) == 1]\n","    for k in range(p):\n","        constraints += [cp.abs(cp.sum(w*x_in_rct[:,k+1]) - np.mean(x_in_rwe[:,k+1])) <= tol*np.std(x_in_rwe[:,k+1])*np.sqrt(n/(n-1))]\n","        constraints += [cp.abs(cp.sum(w*(x_in_rct[:,k+1]**2)) - np.mean(x_in_rwe[:,k+1]**2)) <= tol*np.std(x_in_rwe[:,k+1]**2)*np.sqrt(n/(n-1))]\n","\n","    prob = cp.Problem(objective, constraints)\n","    w_tilde = np.zeros(n)\n","    try:\n","        result = prob.solve(solver=cp.ECOS)\n","        w_tilde = w.value\n","        if np.isnan(w_tilde[0]):\n","            w_tilde = np.zeros(n)\n","    except cp.error.SolverError as e:\n","        # print(\"error\")\n","        pass\n","    return w_tilde\n","\n","def entr_balance_wmid(tol, x_in_rct, x_in_rwe,wmid):\n","    # the optimization problem for the nonparametric weighting method\n","    n = x_in_rct.shape[0]\n","    wmid0 =  x_in_rwe.shape[0] * wmid\n","    w = cp.Variable(n)\n","    p = x_in_rct.shape[1]-1\n","    objective = cp.Maximize(cp.sum(cp.entr(w)))\n","    constraints = [w >= 0, cp.sum(w) == 1]\n","    for k in range(p):\n","        constraints += [cp.abs(cp.sum(w*x_in_rct[:,k+1]) - np.mean(wmid0*x_in_rwe[:,k+1])) <= tol*np.std(wmid0*x_in_rwe[:,k+1])*np.sqrt(n/(n-1))]\n","        constraints += [cp.abs(cp.sum(w*(x_in_rct[:,k+1]**2)) - np.mean(wmid0*(x_in_rwe[:,k+1]**2))) <= tol*np.std(wmid0*(x_in_rwe[:,k+1]**2))*np.sqrt(n/(n-1))]\n","\n","\n","    prob = cp.Problem(objective, constraints)\n","    w_tilde = np.zeros(n)\n","    try:\n","        result = prob.solve(solver=cp.ECOS)\n","        w_tilde = w.value\n","        if np.isnan(w_tilde[0]):\n","            w_tilde = np.zeros(n)\n","    except cp.error.SolverError as e:\n","        pass\n","    return w_tilde\n","\n","\n","def weight_tuned_entr(tols, x_in_rct, x_in_rwe, prop=0.5, smps=10):\n","    # Evaluate covariate balance with bootstrap samples to choose tuning parameters:\n","    # the degree of approximate balance sigma_k\n","    n = x_in_rct.shape[0]\n","    sds = np.apply_along_axis(np.std, 0, x_in_rwe)\n","    means = np.apply_along_axis(np.mean, 0, x_in_rwe)\n","    sds2 = np.apply_along_axis(np.std, 0, x_in_rwe**2)\n","    moment2 = np.apply_along_axis(np.mean, 0, x_in_rwe**2)\n","    cov_diff_bars = []\n","    w_tildes = np.full((n, len(tols)), np.nan)\n","    for i in range(len(tols)):\n","        tol = tols[i]\n","        w_tilde = entr_balance(tol, x_in_rct, x_in_rwe)\n","        w_tilde[w_tilde < 0] = 0  # the optimization results sometimes return negative weights, so clip them to 0\n","        if np.sum(w_tilde) == 0:\n","            cov_diff_bar = 1e+10\n","        else:\n","            w_tildes[:, i] = w_tilde\n","            cov_diffs = []\n","            for s in range(smps):\n","                boot_ind = np.random.choice(n, size=int(round(prop * n)), replace=True)\n","                x_in_rct_boot = x_in_rct[boot_ind, :]\n","                w_tilde_boot = w_tilde[boot_ind]\n","                cov_diff = (np.sum(x_in_rct_boot * w_tilde_boot.reshape((-1, 1)), axis=0) / np.sum(w_tilde_boot) - means)[1:] / sds[1:]\n","                cov_diff_moment2 = (np.sum(x_in_rct_boot**2 * w_tilde_boot.reshape((-1, 1)), axis=0) / np.sum(w_tilde_boot) - moment2)[1:] / sds2[1:]\n","\n","                cov_diffs.append(np.sum(np.concatenate((cov_diff, cov_diff_moment2))**2))  # L2 measure\n","            cov_diff_bar = np.mean(cov_diffs)\n","        cov_diff_bars.append(cov_diff_bar)\n","    tuned_tol = tols[np.argmin(cov_diff_bars)]\n","    if min(cov_diff_bars) < 1e+10:\n","        return w_tildes[:, np.argmin(cov_diff_bars)]\n","    else:\n","        raise ValueError('************ optimization not feasible *****************')\n","\n","def weight_tuned_entr_wmid(tols, x_in_rct, x_in_rwe, wmid, prop=0.5, smps=10):\n","    # Evaluate covariate balance with bootstrap samples to choose tuning parameters:\n","    # the degree of approximate balance sigma_k\n","    n = x_in_rct.shape[0]\n","    wmid0 =  x_in_rwe.shape[0] * wmid\n","    sds = np.apply_along_axis(np.std, 0, wmid0[:,np.newaxis]*x_in_rwe)\n","\n","    means = np.apply_along_axis(np.mean, 0, wmid0[:,np.newaxis]*x_in_rwe)\n","    sds2 = np.apply_along_axis(np.std, 0, wmid0[:,np.newaxis]*(x_in_rwe**2))\n","\n","    moment2 = np.apply_along_axis(np.mean, 0, wmid0[:,np.newaxis]*(x_in_rwe**2))\n","    cov_diff_bars = []\n","    w_tildes = np.full((n, len(tols)), np.nan)\n","    for i in range(len(tols)):\n","        tol = tols[i]\n","        w_tilde = entr_balance_wmid(tol, x_in_rct, x_in_rwe,wmid)\n","\n","        w_tilde[w_tilde < 0] = 0  # the optimization results sometimes return negative weights, so clip them to 0\n","        if np.sum(w_tilde) == 0:\n","            cov_diff_bar = 1e+10\n","        else:\n","            w_tildes[:, i] = w_tilde\n","            cov_diffs = []\n","            for s in range(smps):\n","\n","                boot_ind = np.random.choice(n, size=int(round(prop * n)), replace=True)\n","                x_in_rct_boot = x_in_rct[boot_ind, :]\n","                w_tilde_boot = w_tilde[boot_ind]\n","                cov_diff = (np.sum(x_in_rct_boot * w_tilde_boot.reshape((-1, 1)), axis=0) / np.sum(w_tilde_boot) - means)[1:] / sds[1:]\n","                cov_diff_moment2 = (np.sum(x_in_rct_boot**2 * w_tilde_boot.reshape((-1, 1)), axis=0) / np.sum(w_tilde_boot) - moment2)[1:] / sds2[1:]\n","\n","                cov_diffs.append(np.sum(np.concatenate((cov_diff, cov_diff_moment2))**2))  # L2 measure\n","            cov_diff_bar = np.mean(cov_diffs)\n","        cov_diff_bars.append(cov_diff_bar)\n","    tuned_tol = tols[np.argmin(cov_diff_bars)]\n","    if min(cov_diff_bars) < 1e+10:\n","        return w_tildes[:, np.argmin(cov_diff_bars)]\n","    else:\n","        raise ValueError('************ optimization not feasible *****************')\n","\n","\n","def learn_weights(y_in_rct, x_in_rct, a_in_rct, x_in_rwe, w_method, misspecify, N):\n","    # w_method: 1, 2, 3, 4\n","    n = x_in_rct.shape[0]\n","    p = x_in_rct.shape[1] - 1\n","    if w_method == 4:\n","        # nonparametric weighting\n","        w_tilde = weight_tuned_entr([0.000, 0.001, 0.002, 0.005, 0.010, 0.020, 0.050, 0.100, 0.200, 0.500, 1.000], x_in_rct, x_in_rwe)\n","    else:\n","        raise ValueError(\"Wrong input for weighting method\")\n","    return w_tilde\n","def learn_weights_wmid(y_in_rct, x_in_rct, a_in_rct, x_in_rwe, w_method, misspecify, N,wmid):\n","    # w_method: 1, 2, 3, 4\n","    n = x_in_rct.shape[0]\n","    p = x_in_rct.shape[1] - 1\n","    if w_method == 4:\n","        # nonparametric weighting\n","        w_tilde = weight_tuned_entr_wmid([0.000, 0.001, 0.002, 0.005, 0.010, 0.020, 0.050, 0.100, 0.200, 0.500, 1.000], x_in_rct, x_in_rwe,wmid)\n","    else:\n","        raise ValueError(\"Wrong input for weighting method\")\n","    return w_tilde\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YmKkJG48i-j"},"outputs":[],"source":["\n","def gen_sim_data_RWDRCT(df,n):\n","    Spprob=np.exp(-2+0*df['X1']-4*df['S'])/(1+np.exp(-2+0*df['X1']-4*df['S']))\n","    Sp=np.random.binomial(1,Spprob,n)\n","    df[\"Spprob\"]=Spprob\n","    df[\"Sp\"]=Sp\n","    return df\n","\n","\n","def sim_loop(proj, i_sim, df_all, s_type, X_names, XS_names, names_to_norm, qua_use,\n","             is_sim, y_fn, is_class, is_qua, is_inf, is_rct,\n","             is_tune, param_grid, mypath, is_save, is_ones):\n","\n","    print(\"*\"*15,proj,\"i_sim =\",i_sim,\"begin\",\"*\"*15)\n","    seed = i_sim + 12345\n","    set_random(seed)\n","\n","    ########################################################################################################################\n","    ###################################################### Preprocess ######################################################\n","    ########################################################################################################################\n","    df = df_all\n","    df = gen_sim_data_RWDRCT(df,df.shape[0])\n","\n","\n","    df_RCT=df[df[\"Sp\"]==1]\n","    np.random.seed(0)\n","\n","\n","    np.random.seed(10000)\n","\n","\n","    df_RWD_0=df[np.random.binomial(1,0.4,df.shape[0])==1]\n","\n","\n","\n","    df_RWD_tosplit = gen_sim_data_RWDsplit(df_RWD_0,df_RWD_0.shape[0])\n","\n","    df_RWD_wtcalc = df_RWD_tosplit\n","\n","    df_RWD = df_RWD_tosplit[df_RWD_tosplit[\"pMCAR\"]==1]\n","    print(\"*******NR1\",np.sum(df_RWD_tosplit[\"pMCAR\"]==1))\n","    print(\"*******NR0\",np.sum(df_RWD_tosplit[\"pMCAR\"]==0))\n","\n","    df_pop=df_RWD.copy()\n","    wt_type=\"all\"\n","    if is_ones==\"complete\":\n","        wt_type=\"complete\"\n","        df_RWD_wtcalc = df_RWD_tosplit[df_RWD_tosplit[\"pMCAR\"]==0]\n","    if is_ones==\"impute\" or is_ones==\"np_impute_old\":\n","        wt_type=\"impute\"\n","        df_RWD_tosplit_impute = df_RWD_tosplit.copy()\n","        df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1]=np.nan\n","\n","        rf_classifier = RandomForestRegressor(n_estimators=100, random_state=42)\n","        rf_classifier.fit(df_RWD_tosplit_impute[X_names][df_RWD_tosplit[\"pMCAR\"]==0], df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==0])\n","        df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1] = rf_classifier.predict(df_RWD_tosplit_impute[X_names][df_RWD_tosplit[\"pMCAR\"]==1])\n","\n","        impute_accuracy = mean_squared_error(df_RWD_tosplit[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1], df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1])\n","\n","        print(f'ImputeAccuracy: {impute_accuracy}')\n","        print(\"*****\")\n","\n","        df_RWD_wtcalc = df_RWD_tosplit_impute.copy()\n","    if is_ones==\"complete\" or is_ones==\"impute\":\n","        df_RWD_forweights=pd.concat([df_RWD_wtcalc.copy(),df_RCT.copy()]).reset_index(drop=True).copy()\n","        print(\"df_RWD_forweights number of rows\",df_RWD_forweights.shape[0])\n","\n","\n","        def transfer_weight_obj(alpha):\n","            m=df_RWD_forweights.shape[0]\n","            n=df_RCT.shape[0]\n","\n","\n","            df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","            df_RWD_1X=np.concatenate((np.ones((df_RWD_forweights.shape[0],1)),df_RWD_forweights[XS_names]),axis=1)\n","\n","            return -(1/m+n)*np.sum(df_RCT_1X@alpha)+(1/m+n)*np.sum(np.log(1+np.exp(df_RWD_1X@alpha)))\n","\n","\n","\n","        alpha_sol=scipy.optimize.minimize(transfer_weight_obj, x0=np.asarray([0,0,0]), method='Nelder-Mead', tol=1e-6)\n","        alpha_sol=alpha_sol.x\n","\n","        df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","        transfer_weights=np.exp(-df_RCT_1X@alpha_sol)-1\n","\n","\n","        alpha_true=np.array([-2,-4])\n","        print(df_RCT.shape[0],df_RWD.shape[0])\n","        print(alpha_true)\n","        print(np.around(alpha_sol,4))\n","\n","    if is_ones==\"impute_only_no\":\n","        wt_type=\"impute_only\"\n","        df_RWD_tosplit_impute = df_RWD_tosplit.copy()\n","        df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1]=np.nan\n","\n","        rf_classifier = RandomForestRegressor(n_estimators=100, random_state=42)\n","        rf_classifier.fit(df_RWD_tosplit_impute[X_names][df_RWD_tosplit[\"pMCAR\"]==0], df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==0])\n","        df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1] = rf_classifier.predict(df_RWD_tosplit_impute[X_names][df_RWD_tosplit[\"pMCAR\"]==1])\n","\n","        impute_accuracy = mean_squared_error(df_RWD_tosplit[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1], df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1])\n","\n","        print(f'ImputeAccuracy: {impute_accuracy}')\n","        print(\"*****\")\n","\n","\n","        df_RWD_wtcalc = df_RWD_tosplit_impute[df_RWD_tosplit[\"pMCAR\"]==1].copy()\n","    if is_ones==\"impute\":\n","        df_RWD[\"impute_M\"]=df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1]\n","\n","    if is_ones==\"impute_only\":\n","        wt_type=\"impute_only\"\n","        df_RWD_impar = df_RWD_tosplit[XS_names].copy()  # Select the XS_names columns from RWD\n","        df_RWD_impar['Source'] = 0  # Add 'Source' column indicating RWD data\n","        df_RCT_impar = df_RCT[XS_names].copy()  # Select the XS_names columns from RCT\n","        df_RCT_impar['Source'] = 1  # Add 'Source' column indicating RCT data\n","        df_impar = pd.concat([df_RWD_impar, df_RCT_impar], axis=0).reset_index(drop=True)\n","        rf_classifier1 = RandomForestClassifier(random_state=42)\n","        rf_classifier1.fit(df_impar[X_names], df_impar['Source'])\n","        prob_RCT1 = np.clip(rf_classifier1.predict_proba(df_RCT[X_names]), 1e-5, 1-1e-5)\n","        log_regressor1 = LogisticRegression(random_state=42)\n","        log_regressor1.fit(df_impar[X_names], df_impar['Source'])\n","        prob_RCT1 = np.clip(log_regressor1.predict_proba(df_RCT[X_names]), 1e-5, 1-1e-5)\n","\n","        df_RWD_impar_com = df_RWD_tosplit.loc[df_RWD_tosplit[\"pMCAR\"] == 1, XS_names].copy()\n","        df_RWD_impar_com['Source'] = 0  # Add 'Source' column indicating RWD data\n","        df_impar_com = pd.concat([df_RWD_impar_com, df_RCT_impar], axis=0).reset_index(drop=True)\n","        rf_classifier2 = RandomForestClassifier(random_state=42)\n","        rf_classifier2.fit(df_impar_com[X_names], df_impar_com['Source'])\n","        prob_RCT2 = np.clip(rf_classifier2.predict_proba(df_RCT[X_names]), 1e-5, 1-1e-5)\n","        log_regressor2 = LogisticRegression(random_state=42)\n","        log_regressor2.fit(df_impar_com[X_names], df_impar_com['Source'])\n","        prob_RCT2 = np.clip(log_regressor2.predict_proba(df_RCT[X_names]), 1e-5, 1-1e-5)\n","\n","        rf_classifier3 = RandomForestClassifier(random_state=42)\n","        rf_classifier3.fit(df_impar_com[XS_names], df_impar_com['Source'])\n","        prob_RCT3 = np.clip(rf_classifier3.predict_proba(df_RCT[XS_names]), 1e-5, 1-1e-5)\n","        log_regressor3 = LogisticRegression(random_state=42)\n","        log_regressor3.fit(df_impar_com[XS_names], df_impar_com['Source'])\n","        prob_RCT3 = np.clip(log_regressor3.predict_proba(df_RCT[XS_names]), 1e-5, 1-1e-5)\n","\n","\n","\n","\n","        transfer_weights = (prob_RCT1[:, 0]*prob_RCT2[:, 1]*prob_RCT3[:, 0])/(prob_RCT1[:, 1]*prob_RCT2[:, 0]*prob_RCT3[:, 1])\n","\n","    if is_ones==\"np\":\n","        df_RWD_wtcalc = df_RWD_tosplit[df_RWD_tosplit[\"pMCAR\"]==0]\n","        df_RWD_forweights_2=df_RWD_wtcalc.copy().reset_index(drop=True)\n","        df_RWD_1X_2=np.concatenate((np.ones((df_RWD_forweights_2.shape[0],1)),df_RWD_forweights_2[XS_names]),axis=1)\n","        df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","        transfer_weights=learn_weights(y_in_rct=0, x_in_rct=df_RCT_1X, a_in_rct=0, x_in_rwe=df_RWD_1X_2, w_method=4, misspecify=0, N=0)\n","        wt_type=\"np\"\n","    if is_ones==\"np_impute_old\":\n","        wt_type=\"np_impute_old\"\n","        df_RWD_forweights_2=df_RWD_wtcalc.copy().reset_index(drop=True)\n","        df_RWD_1X_2=np.concatenate((np.ones((df_RWD_forweights_2.shape[0],1)),df_RWD_forweights_2[XS_names]),axis=1)\n","        df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","        transfer_weights=learn_weights(y_in_rct=0, x_in_rct=df_RCT_1X, a_in_rct=0, x_in_rwe=df_RWD_1X_2, w_method=4, misspecify=0, N=0)\n","    if is_ones==\"np_impute\":\n","\n","        df_RWD_forweights_2=df_RWD_tosplit[df_RWD_tosplit[\"pMCAR\"]==0].copy().reset_index(drop=True)\n","        df_RWD_1X_2=np.concatenate((np.ones((df_RWD_forweights_2.shape[0],1)),df_RWD_forweights_2[XS_names]),axis=1)\n","        df_RWD_1X_2_Xonly=np.concatenate((np.ones((df_RWD_forweights_2.shape[0],1)),df_RWD_forweights_2[X_names]),axis=1)\n","        df_RWD_forweights_2_all=df_RWD_tosplit.copy().reset_index(drop=True)\n","        df_RWD_1X_2_all=np.concatenate((np.ones((df_RWD_forweights_2_all.shape[0],1)),df_RWD_forweights_2_all[X_names]),axis=1)\n","        transfer_weights_wmid=learn_weights(y_in_rct=0, x_in_rct=df_RWD_1X_2_Xonly, a_in_rct=0, x_in_rwe=df_RWD_1X_2_all, w_method=4, misspecify=0, N=0)\n","        df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","        transfer_weights=learn_weights_wmid(y_in_rct=0, x_in_rct=df_RCT_1X, a_in_rct=0, x_in_rwe=df_RWD_1X_2, w_method=4, misspecify=0, N=0,wmid=transfer_weights_wmid)\n","        wt_type=\"np_impute\"\n","\n","\n","    if is_ones==\"ones\":\n","        transfer_weights=np.ones(df_RCT.shape[0])\n","        wt_type=\"ones\"\n","\n","    print(np.around(1/transfer_weights[0:10],4))\n","    print(np.around(np.asarray(df_RCT[\"Spprob\"][0:10]),4))\n","\n","    set_random(seed)\n","\n","    df_RCT[\"TW\"]=transfer_weights\n","    print(np.around(np.asarray(df_RCT[\"TW\"][0:10]),4))\n","    df_tr_ori = df_RCT.reset_index(drop=True).copy()\n","    df_te_ori = df_RWD.reset_index(drop=True).copy()\n","    transfer_weights=df_tr_ori[\"TW\"]\n","\n","\n","    print(\"df_tr_ori.head(3)\\n\", df_tr_ori.head(3))\n","\n","\n","    df_RCT_TW_st=df_RCT[\"TW\"]/np.sum(df_RCT[\"TW\"])\n","    print(\"df_RCT_TW_st\",df_RCT_TW_st[:10])\n","    true_IPSW_st=(1/df_RCT[\"Spprob\"])/np.sum(1/df_RCT[\"Spprob\"])\n","\n","\n","    cor_wt=np.corrcoef(df_RCT_TW_st,true_IPSW_st)[0,1]\n","    if s_type == \"disc\":\n","        cor_wt=mean_squared_error(df_RCT_TW_st,true_IPSW_st)\n","\n","\n","    print(\"*********cor_wt:\",cor_wt)\n","\n","    if i_sim<=10:\n","\n","        plt.scatter(true_IPSW_st, df_RCT_TW_st)\n","        plt.xlabel(\"true IPSW\")\n","        plt.ylabel(\"estimated_weight_\"+wt_type)\n","        plt.show()\n","\n","\n","    # normalize\n","    if not is_sim: # only normalize for real (sim already in [0,1])\n","        df, df_test = normalize(df_tr_ori, df_te_ori, names_to_norm)\n","    else:\n","        df      = df_tr_ori.copy()\n","        df_test = df_te_ori.copy()\n","    print(\"df.head(3)\\n\", df.head(3))\n","\n","    # PS based on train\n","    if is_rct:\n","        pr_a1 = sum(df[\"A\"]==1) / df.shape[0]\n","        fit_ps = None\n","        df_test[\"ps\"] = pr_a1\n","        df[\"ps\"] = pr_a1\n","        print(\"pr_a1\", pr_a1)\n","    else:\n","        pr_a1 = None\n","\n","        fit_ps = get_ps_fit(df, XS_names, transfer_weights=df[\"TW\"]) # propensity score A|XS\n","        df_test[\"ps\"] = fit_ps.predict_proba(df_test[XS_names])[:,1]\n","        df[\"ps\"] = fit_ps.predict_proba(df[XS_names])[:,1]\n","\n","    df_test[\"ps\"] = np.where(df_test[\"A\"]==1, df_test[\"ps\"], 1-df_test[\"ps\"])\n","    df[\"ps\"] = np.where(df[\"A\"]==1, df[\"ps\"], 1-df[\"ps\"])\n","\n","    # PS trim using percentile cutpoints (only for observational study)\n","    # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3069059/\n","    if not is_rct:\n","        print(\"for observational study, trimming PS using percentile cutpoints\")\n","\n","        ps_q90 = df[\"ps\"].quantile(0.9)\n","        ps_q10 = df[\"ps\"].quantile(0.1)\n","        print(\"train: before ps min:\", np.min(df[\"ps\"]), \"ps max:\", np.max(df[\"ps\"]))\n","        df[\"ps\"] = np.where(df[\"ps\"] > ps_q90, ps_q90, df[\"ps\"])\n","        df[\"ps\"] = np.where(df[\"ps\"] < ps_q10, ps_q10, df[\"ps\"])\n","        print(\"train: after  ps min:\", np.min(df[\"ps\"]), \"ps max:\", np.max(df[\"ps\"]))\n","\n","        ps_q90 = df_test[\"ps\"].quantile(0.9)\n","        ps_q10 = df_test[\"ps\"].quantile(0.1)\n","        print(\"test: before ps min:\", np.min(df_test[\"ps\"]), \"ps max:\", np.max(df_test[\"ps\"]))\n","        df_test[\"ps\"] = np.where(df_test[\"ps\"] > ps_q90, ps_q90, df_test[\"ps\"])\n","        df_test[\"ps\"] = np.where(df_test[\"ps\"] < ps_q10, ps_q10, df_test[\"ps\"])\n","        print(\"test: after  ps min:\", np.min(df_test[\"ps\"]), \"ps max:\", np.max(df_test[\"ps\"]))\n","\n","    ########################################################################################################################\n","    #################################################### Begin Analysis ####################################################\n","    ########################################################################################################################\n","\n","    print(\"begin analysis\")\n","\n","    df0 = df[df[\"A\"]==0].reset_index(drop=True).copy()\n","    df1 = df[df[\"A\"]==1].reset_index(drop=True).copy()\n","\n","    df0_te = df_test[df_test[\"A\"]==0].reset_index(drop=True).copy()\n","    df1_te = df_test[df_test[\"A\"]==1].reset_index(drop=True).copy()\n","\n","    # Baseline E(Y|X,A) (no S)\n","    is_base = True\n","    obj_name = \"_base_\"\n","    g_base0, _ = fit_expectation(obj_name, df0, X_names, df, is_class, is_tune, param_grid, df0[\"TW\"], df0_te)\n","    g_base1, _ = fit_expectation(obj_name, df1, X_names, df, is_class, is_tune, param_grid, df1[\"TW\"], df1_te)\n","\n","    y_base, w_base = get_label_wt(g_base1, g_base0)\n","    A_base, A_base_tr, model_base = class_pred(df, df_test, X_names, y_base, w_base, s_type, is_tune, param_grid, df[\"TW\"])\n","\n","    # EXP E(Y|X,S,A) # XS train, X predict\n","    is_exp = True\n","    obj_name = \"_exp_\"\n","    g_exp0, _ = fit_expectation(obj_name, df0, XS_names, df, is_class, is_tune, param_grid, df0[\"TW\"], df0_te)\n","    g_exp1, _ = fit_expectation(obj_name, df1, XS_names, df, is_class, is_tune, param_grid, df1[\"TW\"], df1_te)\n","\n","    y_exp, w_exp = get_label_wt(g_exp1, g_exp0)\n","    A_exp, A_exp_tr, model_exp = class_pred(df, df_test, X_names, y_exp, w_exp, s_type, is_tune, param_grid, df[\"TW\"])\n","\n","\n","    is_MI = True\n","    obj_name = \"_MI_\"\n","    # Assuming A_expOracle_MI is generated in a for loop, we collect all generated arrays\n","    all_A_expOracle_MI = []  # list to store each A_expOracle_MI\n","\n","    # Example loop (replace with your actual loop where A_expOracle_MI is generated)\n","    for reprep in range(n_MI):  # num_loops is the number of iterations\n","\n","\n","        df_RWD_tosplit_impute_MI = df_RWD_tosplit.copy()\n","        df_RWD_tosplit_impute_MI[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1]=np.nan\n","\n","        rf_classifier = RandomForestRegressor(n_estimators=100, random_state=42+reprep*100)\n","        rf_classifier.fit(df_RWD_tosplit_impute_MI[X_names][df_RWD_tosplit[\"pMCAR\"]==0], df_RWD_tosplit_impute_MI[\"S\"][df_RWD_tosplit[\"pMCAR\"]==0])\n","        df_RWD_tosplit_impute_MI[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1] = rf_classifier.predict(df_RWD_tosplit_impute_MI[X_names][df_RWD_tosplit[\"pMCAR\"]==1])\n","        df_RWD_MI = df_RWD_tosplit_impute_MI.copy()\n","        df_RWD_MI = df_RWD_MI[df_RWD_tosplit[\"pMCAR\"]==1]\n","        df_test_MI = df_RWD_MI.reset_index(drop=True).copy()\n","\n","        A_expOracle_MI, _, _ = class_pred(df, df_test_MI, XS_names, y_exp, w_exp, s_type, is_tune, param_grid, df[\"TW\"])\n","        all_A_expOracle_MI.append(A_expOracle_MI)\n","\n","    # Convert list of arrays into a 2D NumPy array\n","    all_A_expOracle_MI = np.array(all_A_expOracle_MI)  # Shape: (num_loops, array_length)\n","\n","    # Now, find the majority vote for each position\n","    majority_vote = []\n","\n","    for col in range(all_A_expOracle_MI.shape[1]):  # Iterate over each column (position)\n","        votes = all_A_expOracle_MI[:, col]  # Get the votes for the current position\n","        most_common_vote = Counter(votes).most_common(1)[0][0]  # Get the majority vote\n","        majority_vote.append(most_common_vote)\n","\n","    # Convert the result to a NumPy array (if needed)\n","    final_majority_vote = np.array(majority_vote)\n","    A_MI = final_majority_vote\n","\n","\n","    is_expOracle = True\n","    obj_name = \"_expOracle_\"\n","\n","    A_expOracle, A_expOracle_tr, model_expOracle = class_pred(df, df_test, XS_names, y_exp, w_exp, s_type, is_tune, param_grid, df[\"TW\"])\n","\n","    # QUA/INF E(Y|X,S,A)\n","    if (is_qua or is_inf):\n","        # QUA/INF E(Y|X,S,A)\n","        set_random(seed)\n","        df0[\"Yhat\"], fit_Y_XS_A0 = fit_expectation(\"E(Y|X,S,A=0)\", df0, XS_names, df0, is_class, is_tune, param_grid, df0[\"TW\"], df0_te)\n","        df1[\"Yhat\"], fit_Y_XS_A1 = fit_expectation(\"E(Y|X,S,A=1)\", df1, XS_names, df1, is_class, is_tune, param_grid, df1[\"TW\"], df1_te)\n","\n","        fit_qua0, fit_qua1 = None, None\n","        if (s_type == \"cont\"): # quantile: Yhat|X,A (no S)\n","            set_random(seed)\n","            fit_qua0, g_qua0 = fit_quantile(df0[X_names], df0[\"Yhat\"], df[X_names], qua_use, is_tune, param_grid, df0[\"TW\"])\n","            fit_qua1, g_qua1 = fit_quantile(df1[X_names], df1[\"Yhat\"], df[X_names], qua_use, is_tune, param_grid, df1[\"TW\"])\n","\n","        if (s_type == \"disc\"): # infinite: try all s and find s that helps achieve a minimal\n","            g_qua0 = fit_infinite(df, X_names, XS_names, fit_Y_XS_A0)\n","            g_qua1 = fit_infinite(df, X_names, XS_names, fit_Y_XS_A1)\n","\n","        y_qua, w_qua = get_label_wt(g_qua1, g_qua0)\n","        set_random(seed)\n","        A_qua, A_qua_tr, model_qua = class_pred(df, df_test, X_names, y_qua, w_qua, s_type, is_tune, param_grid, df[\"TW\"])\n","\n","\n","\n","    ########################################################################################################################\n","    ################################################### Evaluate Metrics ###################################################\n","    ########################################################################################################################\n","\n","    A_names = get_A_names(is_base, is_exp, is_MI, is_expOracle, is_qua, is_inf)\n","    res_df_te = add_Apred_Midx(df_test, A_names, A_base, A_exp, A_MI, A_expOracle, A_qua)\n","\n","    A_names.append(\"A_trueOracle\")\n","    res_df_te_trueOracle=res_df_te.copy()\n","    res_df_te_trueOracle_A1=res_df_te_trueOracle.copy()\n","    res_df_te_trueOracle_A1[\"A1\"]=\"yes\"\n","    res_df_te_trueOracle_A1[\"Y_A1\"]=get_pred_Y(res_df_te_trueOracle_A1, \"A1\", XS_names, y_fn,None,None)\n","    res_df_te_trueOracle_A0=res_df_te_trueOracle.copy()\n","    res_df_te_trueOracle_A0[\"A0\"]=\"no\"\n","    res_df_te_trueOracle_A0[\"Y_A0\"]=get_pred_Y(res_df_te_trueOracle_A0, \"A0\", XS_names, y_fn,None,None)\n","    res_df_te[\"A_trueOracle\"]=np.where(res_df_te_trueOracle_A1[\"Y_A1\"]>=res_df_te_trueOracle_A0[\"Y_A0\"],\"yes\",\"no\")\n","\n","\n","\n","    # get optimal A,S given X (QUA/INF optimal)\n","    minor_name = 'M_opt'\n","\n","\n","    res_df=df.copy()\n","    res_df['S_opt'], res_df[minor_name], df_S_opt_train = \\\n","                optimal_AS_inf(df, proj, is_sim, s_type, X_names, XS_names, y_fn, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1, df_pop)\n","\n","    res_df_te['S_opt'], res_df_te[minor_name], df_S_opt_te = \\\n","                optimal_AS_inf(df_test, proj, is_sim, s_type, X_names, XS_names, y_fn, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1, df_pop)\n","    if not df_S_opt_te.empty:\n","        minor_rule(df_S_opt_te, X_names, s_type, \"S_opt\")\n","\n","    # metrics\n","    res_te = eval_metrics(minor_name, res_df_te, A_names, X_names, XS_names, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1, fit_qua0, fit_qua1,res_df)\n","\n","    # save\n","    if is_save and i_sim==0:\n","        # save pred A\n","        res_df_te.to_csv(mypath+proj+\"_\"+s_type+\"_res_df_te_\"+str(i_sim)+\".csv\", index=False)\n","\n","\n","    if is_ones==\"complete\" and i_sim==0:\n","        res_df.to_csv(\"drive/MyDrive/data/cont_MAR_df_train_20240128\"+str(is_ones)+str(i_sim)+\".csv\", index=False)\n","\n","    if is_ones==\"impute_only\" and i_sim==0:\n","        res_df_te.to_csv(\"drive/MyDrive/data/cont_MAR_df_test_20240921\"+str(is_ones)+str(i_sim)+\".csv\", index=False)\n","        res_df.to_csv(\"drive/MyDrive/data/cont_MAR_df_train_20240921\"+str(is_ones)+str(i_sim)+\".csv\", index=False)\n","        df_RWD_tosplit[df_RWD_tosplit[\"pMCAR\"]==0].to_csv(\"drive/MyDrive/data/cont_MAR_df_rwd2_20240921\"+str(is_ones)+str(i_sim)+\".csv\", index=False)\n","\n","\n","\n","\n","\n","    print(\"*\"*15,proj,\"i_sim=\",i_sim,\"finish\",\"*\"*15)\n","\n","    return res_te, cor_wt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1T71bigSVk7P"},"outputs":[],"source":["\n","\n","def dict_list_to_df(dict_list):\n","    dim1_temp=len(dict_list[0].keys())\n","    colname_temp=dict_list[0].keys()\n","    index_temp=[*dict_list[0][list(dict_list[0].keys())[0]].keys()]\n","\n","    for idx_dict_cell in range(len(dict_list)):\n","        print(idx_dict_cell)\n","        dict_cell=list()\n","        for A_name in dict_list[0].keys():\n","            for metric in dict_list[0][A_name].keys():\n","                dict_cell.append(dict_list[idx_dict_cell][A_name][metric])\n","        dict_cell = np.array(dict_cell)\n","        dict_cell = np.reshape(dict_cell, (dim1_temp, -1)).T\n","        dict_cell_df = pd.DataFrame(data=dict_cell, columns=colname_temp)\n","        dict_cell_df[\"metrics\"] = index_temp\n","        dict_cell_df[\"loop\"]=idx_dict_cell\n","        if idx_dict_cell==0:\n","            dict_df=dict_cell_df\n","        else:\n","            dict_df=pd.concat([dict_df,dict_cell_df])\n","\n","    return dict_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQIn0CBM3mk3"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4z34xNHsYl15"},"outputs":[],"source":["\n","# settings\n","n_MI=5\n","proj    = \"simulation\"\n","\n","y_set   = \"toy\"\n","s_type  = \"cont\"\n","\n","\n","qua_use = 0.2\n","\n","n_train = 10000 #train-test: 8-2\n","n_sim   = 100\n","\n","n_train = 15000 #train-test: 8-2\n","n_sim   = 100\n","is_tune = False\n","is_save = False\n","mypath  = \"\"\n","\n","if (proj == \"simulation\"):\n","    df_all_ori, y_fn = gen_sim_data(n_train, s_type, y_set)\n","    is_sim = True\n","else:\n","    is_sim = False\n","    y_fn = None\n","    y_set = None\n","\n","if proj in [\"simulation\"]:\n","    is_class = False\n","\n","if (s_type == \"disc\"):\n","    is_qua = False\n","    is_inf = True\n","else: #cont\n","    is_qua = True\n","    is_inf = False\n","\n","if y_set in ['toy','noise']:\n","    is_rct = True\n","elif y_set in ['complex','positivity','unconfound']:\n","    is_rct = False\n","else:\n","    assert(1==0)\n","\n","if (is_tune):\n","    param_grid = dict(layers=[1,2,3],\n","                      nodes=[256,512,1024],\n","                      dropouts=[0.1,0.2,0.3],\n","                      acts=[\"sigmoid\",\"relu\",\"tanh\"],\n","                      opts=[\"adam\",\"nadam\",\"adadelta\"],\n","                      bsizes=[32,64,128],\n","                      n_epochs=[50,100,200]\n","                     )\n","else:\n","    param_grid = None\n","\n","if not os.path.exists('logs'):\n","    os.makedirs('logs')\n","\n","# names of all X\n","df_all = df_all_ori.copy()\n","if is_sim:\n","    X_names = list(df_all.columns[np.flatnonzero(np.char.startswith(list(df_all.columns), 'X'))])\n","    S_names = [\"S\"]\n","else:\n","    X_names = list( set(df_all.columns) - set([\"A\",\"Y\",\"S\"]) )\n","    S_names = [\"S\"]\n","\n","\n","X_names=[\"X1\"]\n","XS_names = S_names + X_names\n","\n","if not is_sim:\n","    names_to_norm = list(df_all[XS_names].nunique()[df_all[XS_names].nunique() > 2].index)\n","else:\n","    names_to_norm = []\n","print(\"X_names:\", X_names)\n","print(\"S_names:\", S_names)\n","print(\"cont vars (need to normalize):\", names_to_norm)\n","print(\"disc vars (no need to normalize):\", list(set(XS_names) - set(names_to_norm)))\n","\n","# begin running\n","i_sim = 0\n","dict_list_est = list()\n","\n","\n","dict_list_est_ones = list()\n","dict_list_est_all = list()\n","dict_list_est_impute = list()\n","dict_list_est_impute_only = list()\n","\n","cor_list_est = list()\n","cor_list_est_ones = list()\n","cor_list_est_all = list()\n","cor_list_est_impute = list()\n","cor_list_est_impute_only = list()\n","\n","\n","dict_list_est_ones2 = list()\n","dict_list_est_impute2 = list()\n","dict_list_est_impute_only2 = list()\n","cor_list_est_ones2 = list()\n","cor_list_est_impute2 = list()\n","cor_list_est_impute_only2 = list()\n","\n","start0 = time.time()\n","start_time = time.strftime(\"%H:%M:%S\", time.localtime())\n","\n","for i_sim in range(50,n_sim,1):\n","\n","    start = time.time()\n","\n","    if is_sim:\n","        set_random(i_sim+12345)\n","        df_all_ori, y_fn = gen_sim_data(n_train, s_type, y_set)\n","        df_all = df_all_ori.copy()\n","\n","\n","\n","    res_est, cor_wt = sim_loop(proj, i_sim, df_all, s_type, X_names, XS_names, names_to_norm, qua_use,\n","          is_sim, y_fn, is_class, is_qua, is_inf, is_rct,\n","          is_tune, param_grid, mypath, is_save, is_ones=\"np\")\n","\n","    dict_list_est.append(res_est)\n","    cor_list_est.append(cor_wt)\n","\n","    res_est_ones, cor_wt_ones = sim_loop(proj, i_sim, df_all, s_type, X_names, XS_names, names_to_norm, qua_use,\n","             is_sim, y_fn, is_class, is_qua, is_inf, is_rct,\n","             is_tune, param_grid, mypath, is_save, is_ones=\"complete\")\n","\n","    dict_list_est_ones.append(res_est_ones)\n","    cor_list_est_ones.append(cor_wt_ones)\n","\n","\n","    res_est_impute, cor_wt_impute = sim_loop(proj, i_sim, df_all, s_type, X_names, XS_names, names_to_norm, qua_use,\n","             is_sim, y_fn, is_class, is_qua, is_inf, is_rct,\n","             is_tune, param_grid, mypath, is_save, is_ones=\"impute\")\n","\n","    dict_list_est_impute.append(res_est_impute)\n","    cor_list_est_impute.append(cor_wt_impute)\n","\n","\n","    res_est_impute_only, cor_wt_impute_only = sim_loop(proj, i_sim, df_all, s_type, X_names, XS_names, names_to_norm, qua_use,\n","             is_sim, y_fn, is_class, is_qua, is_inf, is_rct,\n","             is_tune, param_grid, mypath, is_save, is_ones=\"ones\")\n","\n","\n","    dict_list_est_impute_only.append(res_est_impute_only)\n","    cor_list_est_impute_only.append(cor_wt_impute_only)\n","\n","    res_est_impute2, cor_wt_impute2 = sim_loop(proj, i_sim, df_all, s_type, X_names, XS_names, names_to_norm, qua_use,\n","             is_sim, y_fn, is_class, is_qua, is_inf, is_rct,\n","             is_tune, param_grid, mypath, is_save, is_ones=\"np_impute\")\n","\n","    dict_list_est_impute2.append(res_est_impute2)\n","    cor_list_est_impute2.append(cor_wt_impute2)\n","\n","\n","    res_est_ones2, cor_wt_ones2 = sim_loop(proj, i_sim, df_all, s_type, X_names, XS_names, names_to_norm, qua_use,\n","             is_sim, y_fn, is_class, is_qua, is_inf, is_rct,\n","             is_tune, param_grid, mypath, is_save, is_ones=\"np_impute_old\")\n","\n","    dict_list_est_ones2.append(res_est_ones2)\n","    cor_list_est_ones2.append(cor_wt_ones2)\n","\n","    res_est_impute_only2, cor_wt_impute_only2 = sim_loop(proj, i_sim, df_all, s_type, X_names, XS_names, names_to_norm, qua_use,\n","             is_sim, y_fn, is_class, is_qua, is_inf, is_rct,\n","             is_tune, param_grid, mypath, is_save, is_ones=\"impute_only\")\n","\n","\n","    dict_list_est_impute_only2.append(res_est_impute_only2)\n","    cor_list_est_impute_only2.append(cor_wt_impute_only2)\n","\n","\n","\n","\n","    if i_sim % 1 == 0:\n","        _, tmp_df = dict_mean(dict_list_est)\n","        _, tmp_std_df = dict_std(dict_list_est)\n","        _, tmp_df_ones = dict_mean(dict_list_est_ones)\n","        _, tmp_std_df_ones = dict_std(dict_list_est_ones)\n","\n","        _, tmp_df_impute = dict_mean(dict_list_est_impute)\n","        _, tmp_std_df_impute = dict_std(dict_list_est_impute)\n","\n","        _, tmp_df_impute_only = dict_mean(dict_list_est_impute_only)\n","        _, tmp_std_df_impute_only = dict_std(dict_list_est_impute_only)\n","\n","        _, tmp_df_ones2 = dict_mean(dict_list_est_ones2)\n","        _, tmp_std_df_ones2 = dict_std(dict_list_est_ones2)\n","        _, tmp_df_impute2 = dict_mean(dict_list_est_impute2)\n","        _, tmp_std_df_impute2 = dict_std(dict_list_est_impute2)\n","        _, tmp_df_impute_only2 = dict_mean(dict_list_est_impute_only2)\n","        _, tmp_std_df_impute_only2 = dict_std(dict_list_est_impute_only2)\n","\n","\n","        mean_cor_impute=np.mean(cor_list_est_impute)\n","        mean_cor_impute_only=np.mean(cor_list_est_impute_only)\n","        mean_cor=np.mean(cor_list_est)\n","        mean_cor_ones=np.mean(cor_list_est_ones)\n","\n","        mean_cor_impute2=np.mean(cor_list_est_impute2)\n","        mean_cor_impute_only2=np.mean(cor_list_est_impute_only2)\n","        mean_cor_ones2=np.mean(cor_list_est_ones2)\n","        print(i_sim, \"M_test cor_ones\\n\", mean_cor_ones)\n","        print(i_sim, \"M_test mean_ones\\n\", tmp_df_ones.round(3))\n","        print(i_sim, \"M_test std_ones\\n\",  tmp_std_df_ones.round(3))\n","\n","        print(i_sim, \"M_test cor_complete\\n\", mean_cor)\n","        print(i_sim, \"M_test mean_complete\\n\", tmp_df.round(3))\n","        print(i_sim, \"M_test std_complete\\n\",  tmp_std_df.round(3))\n","\n","\n","        print(i_sim, \"M_test cor_impute\\n\", mean_cor_impute)\n","        print(i_sim, \"M_test mean_impute\\n\", tmp_df_impute.round(3))\n","        print(i_sim, \"M_test std_impute\\n\",  tmp_std_df_impute.round(3))\n","\n","\n","        print(i_sim, \"M_test cor_impute_only\\n\", mean_cor_impute_only)\n","        print(i_sim, \"M_test mean_impute_only\\n\", tmp_df_impute_only.round(3))\n","        print(i_sim, \"M_test std_impute_only\\n\",  tmp_std_df_impute_only.round(3))\n","\n","\n","\n","        if (i_sim+1) % 5 == 0:\n","\n","            dict_df = dict_list_to_df(dict_list_est)\n","            dict_df_ones = dict_list_to_df(dict_list_est_ones)\n","            dict_df_impute = dict_list_to_df(dict_list_est_impute)\n","            dict_df_impute_only = dict_list_to_df(dict_list_est_impute_only)\n","            dict_df_ones2 = dict_list_to_df(dict_list_est_ones2)\n","            dict_df_impute2 = dict_list_to_df(dict_list_est_impute2)\n","            dict_df_impute_only2 = dict_list_to_df(dict_list_est_impute_only2)\n","            dict_df_ones.to_csv(\"drive/MyDrive/data/result_cont_MAR_20241210_complete_par\"+str(i_sim)+\".csv\")\n","            dict_df_ones2.to_csv(\"drive/MyDrive/data/result_cont_MAR_20241210_np_impute_old\"+str(i_sim)+\".csv\")\n","            dict_df.to_csv(\"drive/MyDrive/data/result_cont_MAR_20241210_complete_np\"+str(i_sim)+\".csv\")\n","            dict_df_impute.to_csv(\"drive/MyDrive/data/result_cont_MAR_20241210_old_impute_par\"+str(i_sim)+\".csv\")\n","            dict_df_impute_only.to_csv(\"drive/MyDrive/data/result_cont_MAR_20241210_ones\"+str(i_sim)+\".csv\")\n","            dict_df_impute2.to_csv(\"drive/MyDrive/data/result_cont_MAR_20241210_impute_np\"+str(i_sim)+\".csv\")\n","            dict_df_impute_only2.to_csv(\"drive/MyDrive/data/result_cont_MAR_20241210_impute_par\"+str(i_sim)+\".csv\")\n","\n","        print(\"*\"*60)\n","\n","    end = time.time()\n","    print(\"iter\", i_sim, \"takes\", (end - start)//60, \"mins\")\n","    print(\"start time\", start_time)\n","    print(\"current process takes\", (end - start0)//60, \"mins\")\n","    print(\"*\"*80)\n","\n","\n","\n","\n","\n","\n","end0 = time.time()\n","print(\"whole process takes\", (end0 - start0)//60, \"mins\")\n","print(\"*\"*80)\n","\n"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}