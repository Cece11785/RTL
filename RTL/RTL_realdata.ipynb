{"cells":[{"cell_type":"markdown","metadata":{"id":"zhC-_1xcQD1y"},"source":["Imports"]},{"cell_type":"code","source":["!pip uninstall xgboost\n","!pip uninstall missingpy\n","!pip install xgboost==2.0.0\n","!pip install scikeras==0.12.0\n","!pip install missingpy==0.2.0\n","!pip install statsmodels"],"metadata":{"id":"DiCqMBv90nmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XU8Z_nYVQFdS"},"outputs":[],"source":["import os\n","import random\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import statsmodels.api as sm\n","\n","from scipy.stats import norm\n","\n","\n","from pandas.compat import numpy\n","import scipy\n","\n","from sklearn import ensemble\n","from sklearn.linear_model import LinearRegression\n","\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout\n","\n","\n","from scikeras.wrappers import KerasClassifier, KerasRegressor\n","\n","\n","\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","\n","import pickle\n","\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","\n","import sklearn.metrics\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn import tree\n","from sklearn.tree import export_text\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","\n","import collections\n","\n","import time\n","\n","import xgboost as xgb\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.metrics import roc_auc_score\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from xgboost import XGBClassifier\n","from sklearn.metrics import roc_auc_score\n","from xgboost import XGBRegressor\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer"]},{"cell_type":"code","source":["import os\n","import random\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import statsmodels.api as sm\n","\n","from scipy.stats import norm\n","\n","from sklearn import ensemble\n","from sklearn.linear_model import LinearRegression\n","\n","\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout\n","\n","\n","from scikeras.wrappers import KerasClassifier, KerasRegressor\n","\n","\n","\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","\n","import pickle\n","\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","\n","import sklearn.metrics\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn import tree\n","from sklearn.tree import export_text\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","\n","import collections\n","\n","import time\n","\n","import pickle\n","\n","\n","\n"],"metadata":{"id":"6b3p2hLlrZ_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dU9AWacJ-5CN"},"outputs":[],"source":["\n","dscc=pd.read_csv('drive/MyDrive/data/dscc_sub_20240220.csv')\n","rwd=pd.read_csv('drive/MyDrive/data/rwd_sub_pao2_albumin_20240403.csv')\n","\n","\n","dscc['pao2']=np.log(dscc['pao2']+1)\n","rwd['pao2']=np.log(rwd['pao2']+1)\n","\n","rwd[\"w\"]=0\n","rwd[\"y\"]=0\n","\n","dscc[\"y\"]=1-dscc[\"y\"]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0j9crbQSwsYV"},"outputs":[],"source":["\n","import sys\n","\n","class output:\n","    def __init__(self, A_pred, is_vul, obj_all, val_all, obj_vul, val_vul, model_rise):\n","        self.A_pred = A_pred\n","        self.is_vul = is_vul\n","        self.obj_all = obj_all\n","        self.val_all = val_all\n","        self.obj_vul = obj_vul\n","        self.val_vul = val_vul\n","        self.model = model_rise\n","\n","\n","class out_model:\n","    def __init__(self, seed, df, df_test, X_names, y_qua, w_qua, s_type, is_tune, param_grid, transfer_weights):\n","        self.seed = seed\n","        self.df = df\n","        self.df_test = df_test\n","        self.X_names = X_names\n","        self.y_qua = y_qua\n","        self.w_qua = w_qua\n","        self.s_type = s_type\n","        self.is_tune = is_tune\n","        self.param_grid = param_grid\n","\n","        self.transfer_weights = transfer_weights\n"]},{"cell_type":"code","source":["\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","n_MI=5"],"metadata":{"id":"gnDE0Mphr-0V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import cvxpy as cp\n","import numpy as np\n","\n","def entr(w):\n","\n","    return cp.sum(cp.entr(w))\n","\n","def entr_balance(tol, x_in_rct, x_in_rwe):\n","    # the optimization problem for the nonparametric weighting method\n","    n = x_in_rct.shape[0]\n","    w = cp.Variable(n)\n","    p = x_in_rct.shape[1]-1\n","    objective = cp.Maximize(cp.sum(cp.entr(w)))\n","    constraints = [w >= 0, cp.sum(w) == 1]\n","    for k in range(p):\n","        constraints += [cp.abs(cp.sum(w*x_in_rct[:,k+1]) - np.mean(x_in_rwe[:,k+1])) <= tol*np.std(x_in_rwe[:,k+1])*np.sqrt(n/(n-1))]\n","        constraints += [cp.abs(cp.sum(w*(x_in_rct[:,k+1]**2)) - np.mean(x_in_rwe[:,k+1]**2)) <= tol*np.std(x_in_rwe[:,k+1]**2)*np.sqrt(n/(n-1))]\n","\n","    prob = cp.Problem(objective, constraints)\n","    w_tilde = np.zeros(n)\n","    try:\n","        result = prob.solve(solver=cp.ECOS)\n","        w_tilde = w.value\n","        if w_tilde is None:\n","            w_tilde = np.zeros(n)\n","        else:\n","            if np.isnan(w_tilde[0]):\n","                w_tilde = np.zeros(n)\n","\n","    except cp.error.SolverError as e:\n","\n","        pass\n","    return w_tilde\n","\n","def entr_balance_wmid(tol, x_in_rct, x_in_rwe,wmid):\n","    # the optimization problem for the nonparametric weighting method\n","    n = x_in_rct.shape[0]\n","    wmid0 =  x_in_rwe.shape[0] * wmid\n","    w = cp.Variable(n)\n","    p = x_in_rct.shape[1]-1\n","    objective = cp.Maximize(cp.sum(cp.entr(w)))\n","    constraints = [w >= 0, cp.sum(w) == 1]\n","    for k in range(p):\n","        constraints += [cp.abs(cp.sum(w*x_in_rct[:,k+1]) - np.mean(wmid0*x_in_rwe[:,k+1])) <= tol*np.std(wmid0*x_in_rwe[:,k+1])*np.sqrt(n/(n-1))]\n","        constraints += [cp.abs(cp.sum(w*(x_in_rct[:,k+1]**2)) - np.mean(wmid0*(x_in_rwe[:,k+1]**2))) <= tol*np.std(wmid0*(x_in_rwe[:,k+1]**2))*np.sqrt(n/(n-1))]\n","\n","    prob = cp.Problem(objective, constraints)\n","    w_tilde = np.zeros(n)\n","    try:\n","        result = prob.solve(solver=cp.ECOS)\n","        w_tilde = w.value\n","        if w_tilde is None:\n","            w_tilde = np.zeros(n)\n","        else:\n","            if np.isnan(w_tilde[0]):\n","                w_tilde = np.zeros(n)\n","\n","    except cp.error.SolverError as e:\n","\n","        pass\n","    return w_tilde\n","\n","\n","def weight_tuned_entr(tols, x_in_rct, x_in_rwe, prop=0.5, smps=10):\n","    # Evaluate covariate balance with bootstrap samples to choose tuning parameters:\n","    # the degree of approximate balance sigma_k\n","    n = x_in_rct.shape[0]\n","    sds = np.apply_along_axis(np.std, 0, x_in_rwe)\n","    means = np.apply_along_axis(np.mean, 0, x_in_rwe)\n","    sds2 = np.apply_along_axis(np.std, 0, x_in_rwe**2)\n","    moment2 = np.apply_along_axis(np.mean, 0, x_in_rwe**2)\n","    cov_diff_bars = []\n","    w_tildes = np.full((n, len(tols)), np.nan)\n","    for i in range(len(tols)):\n","        tol = tols[i]\n","        w_tilde = entr_balance(tol, x_in_rct, x_in_rwe)\n","        w_tilde[w_tilde < 0] = 0  # the optimization results sometimes return negative weights, so clip them to 0\n","        if np.sum(w_tilde) == 0:\n","            cov_diff_bar = 1e+10\n","        else:\n","            w_tildes[:, i] = w_tilde\n","            cov_diffs = []\n","            for s in range(smps):\n","                boot_ind = np.random.choice(n, size=int(round(prop * n)), replace=True)\n","                x_in_rct_boot = x_in_rct[boot_ind, :]\n","                w_tilde_boot = w_tilde[boot_ind]\n","                cov_diff = (np.sum(x_in_rct_boot * w_tilde_boot.reshape((-1, 1)), axis=0) / np.sum(w_tilde_boot) - means)[1:] / sds[1:]\n","                cov_diff_moment2 = (np.sum(x_in_rct_boot**2 * w_tilde_boot.reshape((-1, 1)), axis=0) / np.sum(w_tilde_boot) - moment2)[1:] / sds2[1:]\n","\n","                cov_diffs.append(np.sum(np.concatenate((cov_diff, cov_diff_moment2))**2))  # L2 measure\n","            cov_diff_bar = np.mean(cov_diffs)\n","        cov_diff_bars.append(cov_diff_bar)\n","    tuned_tol = tols[np.argmin(cov_diff_bars)]\n","    if min(cov_diff_bars) < 1e+10:\n","        return w_tildes[:, np.argmin(cov_diff_bars)]\n","    else:\n","        raise ValueError('************ optimization not feasible *****************')\n","\n","def weight_tuned_entr_wmid(tols, x_in_rct, x_in_rwe, wmid, prop=0.5, smps=10):\n","    # Evaluate covariate balance with bootstrap samples to choose tuning parameters:\n","    # the degree of approximate balance sigma_k\n","    n = x_in_rct.shape[0]\n","    wmid0 =  x_in_rwe.shape[0] * wmid\n","    sds = np.apply_along_axis(np.std, 0, wmid0[:,np.newaxis]*x_in_rwe)\n","\n","    means = np.apply_along_axis(np.mean, 0, wmid0[:,np.newaxis]*x_in_rwe)\n","    sds2 = np.apply_along_axis(np.std, 0, wmid0[:,np.newaxis]*(x_in_rwe**2))\n","\n","    moment2 = np.apply_along_axis(np.mean, 0, wmid0[:,np.newaxis]*(x_in_rwe**2))\n","    cov_diff_bars = []\n","    w_tildes = np.full((n, len(tols)), np.nan)\n","    for i in range(len(tols)):\n","        tol = tols[i]\n","        w_tilde = entr_balance_wmid(tol, x_in_rct, x_in_rwe,wmid)\n","\n","        w_tilde[w_tilde < 0] = 0  # the optimization results sometimes return negative weights, so clip them to 0\n","        if np.sum(w_tilde) == 0:\n","            cov_diff_bar = 1e+10\n","        else:\n","            w_tildes[:, i] = w_tilde\n","            cov_diffs = []\n","            for s in range(smps):\n","\n","                boot_ind = np.random.choice(n, size=int(round(prop * n)), replace=True)\n","                x_in_rct_boot = x_in_rct[boot_ind, :]\n","                w_tilde_boot = w_tilde[boot_ind]\n","                cov_diff = (np.sum(x_in_rct_boot * w_tilde_boot.reshape((-1, 1)), axis=0) / np.sum(w_tilde_boot) - means)[1:] / sds[1:]\n","                cov_diff_moment2 = (np.sum(x_in_rct_boot**2 * w_tilde_boot.reshape((-1, 1)), axis=0) / np.sum(w_tilde_boot) - moment2)[1:] / sds2[1:]\n","\n","                cov_diffs.append(np.sum(np.concatenate((cov_diff, cov_diff_moment2))**2))  # L2 measure\n","            cov_diff_bar = np.mean(cov_diffs)\n","        cov_diff_bars.append(cov_diff_bar)\n","    tuned_tol = tols[np.argmin(cov_diff_bars)]\n","    if min(cov_diff_bars) < 1e+10:\n","        return w_tildes[:, np.argmin(cov_diff_bars)]\n","    else:\n","        raise ValueError('************ optimization not feasible *****************')\n","\n","\n","def learn_weights(y_in_rct, x_in_rct, a_in_rct, x_in_rwe, w_method, misspecify, N):\n","    # w_method: 1, 2, 3, 4\n","    n = x_in_rct.shape[0]\n","    p = x_in_rct.shape[1] - 1\n","    if w_method == 4:\n","        # nonparametric weighting\n","        w_tilde = weight_tuned_entr([0.000, 0.001, 0.002, 0.005, 0.010, 0.020, 0.050, 0.100, 0.200, 0.500, 1.000], x_in_rct, x_in_rwe)\n","    else:\n","        raise ValueError(\"Wrong input for weighting method\")\n","    return w_tilde\n","def learn_weights_wmid(y_in_rct, x_in_rct, a_in_rct, x_in_rwe, w_method, misspecify, N,wmid):\n","    # w_method: 1, 2, 3, 4\n","    n = x_in_rct.shape[0]\n","    p = x_in_rct.shape[1] - 1\n","    if w_method == 4:\n","        # nonparametric weighting\n","        w_tilde = weight_tuned_entr_wmid([0.000, 0.001, 0.002, 0.005, 0.010, 0.020, 0.050, 0.100, 0.200, 0.500, 1.000], x_in_rct, x_in_rwe,wmid)\n","    else:\n","        raise ValueError(\"Wrong input for weighting method\")\n","    return w_tilde\n","\n"],"metadata":{"id":"xl4ZpfObpQ3Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","X_names=[\"age\", \"rr\",\"temp\",\"hr\",\"lactate\", \"hgb\",\"plt\",\"map\", \"sbp\", \"male\",\"bili\",\"bun\", \"creat\",\"gluc\",\"o2_sat\",\"wbc\",\"sofa_total\",\"gcs\", \"mechvent\"]\n","S_names=[\"pao2\",\"albumin\"]\n","is_rct=False\n","is_class=True\n","s_type=\"cont\"\n","tw_type=None\n","qua_use=0.2\n","is_tune=False\n","param_grid=None\n","is_plot=False\n","method=None\n","seed=12345\n","\n","train_df=dscc\n","test_df=rwd\n","Y_name=\"y\"\n","A_name=\"w\"\n","\n","if (is_tune):\n","\n","    param_grid = dict(layers=[1],\n","                      nodes=[256],\n","                      dropouts=[0.1],\n","                      acts=[\"relu\"],\n","                      opts=[\"adam\"],\n","                      bsizes=[32],\n","                      n_epochs=[100]\n","                    )\n","\n","else:\n","    param_grid = None\n","\n","# sanity check\n","try:\n","    assert s_type in [\"cont\", \"disc\", \"multi-disc\"]\n","except:\n","    sys.exit('s_type should be in [\"cont\", \"disc\", \"multi-disc\"]')\n","\n","if s_type in [\"disc\", \"multi-disc\"]:\n","    qua_use = None\n","\n","if is_tune:\n","    try:\n","        assert param_grid is not None\n","    except:\n","        sys.exit('when is_tune=True, param_grid should not be None')\n","else:\n","    param_grid = None\n","\n","# default values\n","is_sim = False\n","proj, y_fn = None, None\n","\n","train_df=train_df.copy()\n","test_df=test_df.copy()\n","# reorganize data\n","train_df.rename(columns={Y_name:\"Y\"}, inplace=True)\n","test_df.rename(columns={Y_name:\"Y\"}, inplace=True)\n","\n","train_df.rename(columns={A_name:\"A\"}, inplace=True)\n","test_df.rename(columns={A_name:\"A\"}, inplace=True)\n","if set(train_df[\"A\"]) != set([0,1]): # randomly pick one as control/treatment\n","    assert set(train_df[\"A\"]) == set(test_df[\"A\"])\n","    a0 = list(set(train_df[\"A\"]))[0]\n","    train_df[\"A\"] = np.where(train_df[\"A\"]==a0, 0, 1)\n","    test_df[\"A\"] = np.where(test_df[\"A\"]==a0, 0, 1)\n","\n","XS_names = S_names + X_names\n","\n","# discrete var has <=5 values #TODO: may need to change in future\n","if s_type == \"cont\":\n","    names_to_norm = list(train_df[XS_names].nunique()[train_df[XS_names].nunique() > 5].index)\n","else: # not normalize S if disc/multi-disc\n","    names_to_norm = list(train_df[X_names].nunique()[train_df[X_names].nunique() > 5].index)\n","\n","set_random(seed)\n","\n","\n","############################################################################\n","################################ Preprocess ################################\n","############################################################################\n","\n","# normalize\n","df, df_test = normalize(train_df, test_df, names_to_norm)\n","\n","\n","df_RCT=df.copy()\n","\n","\n","\n","\n","df_RWD_tosplit=df_test.copy()\n","\n","df_RWD_wtcalc = df_RWD_tosplit[df_RWD_tosplit[\"two_status\"]==1]\n","df_RWD = df_RWD_tosplit[df_RWD_tosplit[\"two_status\"]==0]\n","\n","df_RWD_forweights=pd.concat([df_RWD_wtcalc.copy(),df_RCT.copy()]).reset_index(drop=True).copy()\n","\n","\n","print(\"df_RWD_forweights number of rows\",df_RWD_forweights.shape[0])\n","\n","df_pop=df_RWD.copy()\n","\n","\n","def transfer_weight_obj(alpha):\n","    m=df_RWD_forweights.shape[0]\n","    n=df_RCT.shape[0]\n","\n","    df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","    df_RWD_1X=np.concatenate((np.ones((df_RWD_forweights.shape[0],1)),df_RWD_forweights[XS_names]),axis=1)\n","\n","    return -(1/m+n)*np.sum(df_RCT_1X@alpha)+(1/m+n)*np.sum(np.log(1+np.exp(df_RWD_1X@alpha)))\n","\n","\n","\n","\n","alpha_sol=scipy.optimize.minimize(transfer_weight_obj, x0=np.asarray([0 for i in range(np.asarray(XS_names).shape[0]+1)]), method='Nelder-Mead', tol=1e-6)\n","alpha_sol=alpha_sol.x\n","\n","\n","\n","df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","\n","transfer_weights=np.exp(-df_RCT_1X@alpha_sol)\n","\n","\n","\n","\n","alpha_true=np.array([-2,-4])\n","\n","df_RCT[\"TW_test\"]=transfer_weights.copy()\n","df_RCT[\"TW\"]=transfer_weights\n","df_RCT[\"TW_ones\"]=np.ones(df_RCT.shape[0])\n","\n","\n","is_ones=\"impute\"\n","\n","df_RWD_tosplit[\"pMCAR\"] = (df_RWD_tosplit[\"two_status\"]==0)\n","if is_ones==\"complete\":\n","    wt_type=\"complete\"\n","    df_RWD_wtcalc = df_RWD_tosplit[df_RWD_tosplit[\"pMCAR\"]==0]\n","if is_ones==\"impute\" or is_ones==\"np_impute_old\":\n","    wt_type=\"impute\"\n","    df_RWD_wtcalc = df_RWD_tosplit.copy()\n","if is_ones==\"complete\" or is_ones==\"impute\":\n","    df_RWD_forweights=pd.concat([df_RWD_wtcalc.copy(),df_RCT.copy()]).reset_index(drop=True).copy()\n","    print(\"df_RWD_forweights number of rows\",df_RWD_forweights.shape[0])\n","\n","\n","    def transfer_weight_obj(alpha):\n","        m=df_RWD_forweights.shape[0]\n","        n=df_RCT.shape[0]\n","\n","\n","\n","        df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","        df_RWD_1X=np.concatenate((np.ones((df_RWD_forweights.shape[0],1)),df_RWD_forweights[XS_names]),axis=1)\n","\n","        return -(1/m+n)*np.sum(df_RCT_1X@alpha)+(1/m+n)*np.sum(np.log(1+np.exp(df_RWD_1X@alpha)))\n","\n","\n","\n","    alpha_sol=scipy.optimize.minimize(transfer_weight_obj, x0=np.asarray([0 for i in range(np.asarray(XS_names).shape[0]+1)]), method='Nelder-Mead', tol=1e-6)\n","    alpha_sol=alpha_sol.x\n","\n","\n","\n","\n","    df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","\n","    transfer_weights=np.exp(-df_RCT_1X@alpha_sol)\n","\n","    alpha_true=np.array([-2,-4])\n","\n","    print(df_RCT.shape[0],df_RWD.shape[0])\n","    print(alpha_true)\n","    print(np.around(alpha_sol,4))\n","\n","if is_ones==\"impute_only_no\":\n","    wt_type=\"impute_only\"\n","    df_RWD_tosplit_impute = df_RWD_tosplit.copy()\n","    df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1]=np.nan\n","\n","    rf_classifier = RandomForestRegressor(n_estimators=100, random_state=42)\n","    rf_classifier.fit(df_RWD_tosplit_impute[X_names][df_RWD_tosplit[\"pMCAR\"]==0], df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==0])\n","    df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1] = rf_classifier.predict(df_RWD_tosplit_impute[X_names][df_RWD_tosplit[\"pMCAR\"]==1])\n","\n","    impute_accuracy = mean_squared_error(df_RWD_tosplit[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1], df_RWD_tosplit_impute[\"S\"][df_RWD_tosplit[\"pMCAR\"]==1])\n","\n","    print(f'ImputeAccuracy: {impute_accuracy}')\n","    print(\"*****\")\n","\n","    df_RWD_wtcalc = df_RWD_tosplit_impute[df_RWD_tosplit[\"pMCAR\"]==1].copy()\n","\n","if is_ones==\"impute_only\":\n","    wt_type=\"impute_only\"\n","    df_RWD_impar = df_RWD_tosplit[XS_names].copy()  # Select the XS_names columns from RWD\n","    df_RWD_impar['Source'] = 0  # Add 'Source' column indicating RWD data\n","    df_RCT_impar = df_RCT[XS_names].copy()  # Select the XS_names columns from RCT\n","    df_RCT_impar['Source'] = 1  # Add 'Source' column indicating RCT data\n","    df_impar = pd.concat([df_RWD_impar, df_RCT_impar], axis=0).reset_index(drop=True)\n","    rf_classifier1 = RandomForestClassifier(random_state=42)\n","    rf_classifier1.fit(df_impar[X_names], df_impar['Source'])\n","    prob_RCT1 = np.clip(rf_classifier1.predict_proba(df_RCT[X_names]), 1e-5, 1-1e-5)\n","    log_regressor1 = LogisticRegression(random_state=42)\n","    log_regressor1.fit(df_impar[X_names], df_impar['Source'])\n","    prob_RCT1 = np.clip(log_regressor1.predict_proba(df_RCT[X_names]), 1e-5, 1-1e-5)\n","\n","    df_RWD_impar_com = df_RWD_tosplit.loc[df_RWD_tosplit[\"pMCAR\"] == 1, XS_names].copy()\n","    df_RWD_impar_com['Source'] = 0  # Add 'Source' column indicating RWD data\n","    df_impar_com = pd.concat([df_RWD_impar_com, df_RCT_impar], axis=0).reset_index(drop=True)\n","    rf_classifier2 = RandomForestClassifier(random_state=42)\n","    rf_classifier2.fit(df_impar_com[X_names], df_impar_com['Source'])\n","    prob_RCT2 = np.clip(rf_classifier2.predict_proba(df_RCT[X_names]), 1e-5, 1-1e-5)\n","    log_regressor2 = LogisticRegression(random_state=42)\n","    log_regressor2.fit(df_impar_com[X_names], df_impar_com['Source'])\n","    prob_RCT2 = np.clip(log_regressor2.predict_proba(df_RCT[X_names]), 1e-5, 1-1e-5)\n","\n","    rf_classifier3 = RandomForestClassifier(random_state=42)\n","    rf_classifier3.fit(df_impar_com[XS_names], df_impar_com['Source'])\n","    prob_RCT3 = np.clip(rf_classifier3.predict_proba(df_RCT[XS_names]), 1e-5, 1-1e-5)\n","    log_regressor3 = LogisticRegression(random_state=42)\n","    log_regressor3.fit(df_impar_com[XS_names], df_impar_com['Source'])\n","    prob_RCT3 = np.clip(log_regressor3.predict_proba(df_RCT[XS_names]), 1e-5, 1-1e-5)\n","\n","\n","\n","    transfer_weights = (prob_RCT1[:, 0]*prob_RCT2[:, 1]*prob_RCT3[:, 0])/(prob_RCT1[:, 1]*prob_RCT2[:, 0]*prob_RCT3[:, 1])\n","\n","if is_ones==\"np\":\n","    df_RWD_wtcalc = df_RWD_tosplit[df_RWD_tosplit[\"pMCAR\"]==0]\n","    df_RWD_forweights_2=df_RWD_wtcalc.copy().reset_index(drop=True)\n","    df_RWD_1X_2=np.concatenate((np.ones((df_RWD_forweights_2.shape[0],1)),df_RWD_forweights_2[XS_names]),axis=1)\n","    df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","    transfer_weights=learn_weights(y_in_rct=0, x_in_rct=df_RCT_1X, a_in_rct=0, x_in_rwe=df_RWD_1X_2, w_method=4, misspecify=0, N=0)\n","    wt_type=\"np\"\n","if is_ones==\"np_impute_old\":\n","    wt_type=\"np_impute_old\"\n","    df_RWD_forweights_2=df_RWD_wtcalc.copy().reset_index(drop=True)\n","    df_RWD_1X_2=np.concatenate((np.ones((df_RWD_forweights_2.shape[0],1)),df_RWD_forweights_2[XS_names]),axis=1)\n","    df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","    transfer_weights=learn_weights(y_in_rct=0, x_in_rct=df_RCT_1X, a_in_rct=0, x_in_rwe=df_RWD_1X_2, w_method=4, misspecify=0, N=0)\n","if is_ones==\"np_impute\":\n","\n","    df_RWD_forweights_2=df_RWD_tosplit[df_RWD_tosplit[\"pMCAR\"]==0].copy().reset_index(drop=True)\n","    df_RWD_1X_2=np.concatenate((np.ones((df_RWD_forweights_2.shape[0],1)),df_RWD_forweights_2[XS_names]),axis=1)\n","    df_RWD_1X_2_Xonly=np.concatenate((np.ones((df_RWD_forweights_2.shape[0],1)),df_RWD_forweights_2[X_names]),axis=1)\n","    df_RWD_forweights_2_all=df_RWD_tosplit.copy().reset_index(drop=True)\n","    df_RWD_1X_2_all=np.concatenate((np.ones((df_RWD_forweights_2_all.shape[0],1)),df_RWD_forweights_2_all[X_names]),axis=1)\n","    transfer_weights_wmid=learn_weights(y_in_rct=0, x_in_rct=df_RWD_1X_2_Xonly, a_in_rct=0, x_in_rwe=df_RWD_1X_2_all, w_method=4, misspecify=0, N=0)\n","    df_RCT_1X=np.concatenate((np.ones((df_RCT.shape[0],1)),df_RCT[XS_names]),axis=1)\n","    transfer_weights=learn_weights_wmid(y_in_rct=0, x_in_rct=df_RCT_1X, a_in_rct=0, x_in_rwe=df_RWD_1X_2, w_method=4, misspecify=0, N=0,wmid=transfer_weights_wmid)\n","    wt_type=\"np_impute\"\n","\n","\n","\n","df_RCT[\"TW_test\"]=transfer_weights.copy()\n","df_RCT[\"TW\"]=transfer_weights\n","df_RCT[\"TW_ones\"]=np.ones(df_RCT.shape[0])\n","\n","df_tr_ori = df_RCT.reset_index(drop=True).copy()\n","df_te_ori = df_RWD.reset_index(drop=True).copy()\n","\n","\n","df=df_RCT.reset_index(drop=True).copy()\n","df_test=df_RWD.reset_index(drop=True).copy()\n","\n","df_pop=df_RWD.reset_index(drop=True).copy()\n","\n","\n","# PS based on train\n","if is_rct:\n","    pr_a1 = sum(df[\"A\"]==1) / df.shape[0]\n","    fit_ps = None\n","    df_test[\"ps\"] = pr_a1\n","    df[\"ps\"] = pr_a1\n","    print(\"pr_a1\", pr_a1)\n","else:\n","    pr_a1 = None\n","    fit_ps = get_ps_fit(df, XS_names, transfer_weights=df[\"TW\"]) # propensity score A|XS\n","    df_test[\"ps\"] = fit_ps.predict_proba(df_test[XS_names])[:,1]\n","    df[\"ps\"] = fit_ps.predict_proba(df[XS_names])[:,1]\n","\n","df_test[\"ps\"] = np.where(df_test[\"A\"]==1, df_test[\"ps\"], 1-df_test[\"ps\"])\n","df[\"ps\"] = np.where(df[\"A\"]==1, df[\"ps\"], 1-df[\"ps\"])\n","\n","\n","if not is_rct:\n","    print(\"for observational study, trimming PS using percentile cutpoints\")\n","\n","    ps_q90 = df[\"ps\"].quantile(0.9)\n","    ps_q10 = df[\"ps\"].quantile(0.1)\n","    print(\"train: before ps min:\", np.min(df[\"ps\"]), \"ps max:\", np.max(df[\"ps\"]))\n","    df[\"ps\"] = np.where(df[\"ps\"] > ps_q90, ps_q90, df[\"ps\"])\n","    df[\"ps\"] = np.where(df[\"ps\"] < ps_q10, ps_q10, df[\"ps\"])\n","    print(\"train: after  ps min:\", np.min(df[\"ps\"]), \"ps max:\", np.max(df[\"ps\"]))\n","\n","    ps_q90 = df_test[\"ps\"].quantile(0.9)\n","    ps_q10 = df_test[\"ps\"].quantile(0.1)\n","    print(\"test: before ps min:\", np.min(df_test[\"ps\"]), \"ps max:\", np.max(df_test[\"ps\"]))\n","    df_test[\"ps\"] = np.where(df_test[\"ps\"] > ps_q90, ps_q90, df_test[\"ps\"])\n","    df_test[\"ps\"] = np.where(df_test[\"ps\"] < ps_q10, ps_q10, df_test[\"ps\"])\n","    print(\"test: after  ps min:\", np.min(df_test[\"ps\"]), \"ps max:\", np.max(df_test[\"ps\"]))\n","\n","\n","############################################################################\n","############################## Begin Analysis ##############################\n","############################################################################\n","\n","df0 = df[df[\"A\"]==0].reset_index(drop=True).copy()\n","df1 = df[df[\"A\"]==1].reset_index(drop=True).copy()\n","\n","df0_te = df_test[df_test[\"A\"]==0].reset_index(drop=True).copy()\n","df1_te = df_test[df_test[\"A\"]==1].reset_index(drop=True).copy()\n","\n","\n","if df1_te.shape[0]<1:\n","    df1_te=None\n","\n","df1_te=None\n","df0_te=None\n","\n","\n","\n","obj_name = \"_expOracle_\"\n","\n","\n","g_exp0, fit_Y_XS_A0 = fit_expectation(obj_name, df0, XS_names, df, is_class, is_tune, param_grid, df0[\"TW\"], df0_te)\n","g_exp1, fit_Y_XS_A1 = fit_expectation(obj_name, df1, XS_names, df, is_class, is_tune, param_grid, df1[\"TW\"], df1_te)\n","\n","g_exp0_ones, fit_Y_XS_A0_ones = fit_expectation(obj_name, df0, XS_names, df, is_class, is_tune, param_grid, df0[\"TW_ones\"], df0_te)\n","g_exp1_ones, fit_Y_XS_A1_ones = fit_expectation(obj_name, df1, XS_names, df, is_class, is_tune, param_grid, df1[\"TW_ones\"], df1_te)\n","\n","\n","y_exp, w_exp = get_label_wt(g_exp1, g_exp0)\n","A_te_expOracle, A_tr_expOracle, model_rise_expOracle = class_pred(df, df_test, XS_names, y_exp, w_exp, s_type, is_tune, param_grid, df[\"TW\"])\n","\n","y_exp_ones, w_exp_ones = get_label_wt(g_exp1_ones, g_exp0_ones)\n","A_te_expOracle_ones, A_tr_expOracle_ones, model_rise_expOracle_ones = class_pred(df, df_test, XS_names, y_exp_ones, w_exp_ones, s_type, is_tune, param_grid, df[\"TW_ones\"])\n","\n","all_A_expOracle_MI = []  # list to store each A_expOracle_MI\n","all_A_expOracle_MI_ones = []\n","\n","model_expOracle_MI = []  # list to store each A_expOracle_MI\n","model_expOracle_MI_ones = []\n","# Example loop (replace with your actual loop where A_expOracle_MI is generated)\n","for reprep in range(n_MI):  # num_loops is the number of iterations\n","\n","    df_RWD_tosplit_impute_MI = df_RWD_tosplit.copy()\n","    df_RWD_tosplit_impute_MI[S_names][df_RWD_tosplit[\"two_status\"]==0]=np.nan\n","\n","\n","    rf_classifierS1 = RandomForestRegressor(n_estimators=100, random_state=42+reprep*100)\n","    rf_classifierS1.fit(df_RWD_tosplit_impute_MI[X_names][df_RWD_tosplit[\"two_status\"]==1], df_RWD_tosplit_impute_MI[S_names[0]][df_RWD_tosplit[\"two_status\"]==1])\n","    df_RWD_tosplit_impute_MI[S_names[0]][df_RWD_tosplit[\"two_status\"]==0] = rf_classifierS1.predict(df_RWD_tosplit_impute_MI[X_names][df_RWD_tosplit[\"two_status\"]==0])\n","    rf_classifierS2 = RandomForestRegressor(n_estimators=100, random_state=42+reprep*100)\n","    rf_classifierS2.fit(df_RWD_tosplit_impute_MI[X_names][df_RWD_tosplit[\"two_status\"]==1], df_RWD_tosplit_impute_MI[S_names[1]][df_RWD_tosplit[\"two_status\"]==1])\n","    df_RWD_tosplit_impute_MI[S_names[1]][df_RWD_tosplit[\"two_status\"]==0] = rf_classifierS2.predict(df_RWD_tosplit_impute_MI[X_names][df_RWD_tosplit[\"two_status\"]==0])\n","    df_RWD_MI = df_RWD_tosplit_impute_MI.copy()\n","    df_RWD_MI = df_RWD_MI[df_RWD_tosplit[\"two_status\"]==0]\n","    df_test_MI = df_RWD_MI.reset_index(drop=True).copy()\n","\n","    A_expOracle_MI, _, model_expOracle_MI_0 = class_pred(df, df_test_MI, XS_names, y_exp, w_exp, s_type, is_tune, param_grid, df[\"TW\"])\n","    A_expOracle_MI_ones, _, model_expOracle_MI_ones_0 = class_pred(df, df_test_MI, XS_names, y_exp, w_exp, s_type, is_tune, param_grid, df[\"TW_ones\"])\n","    all_A_expOracle_MI_ones.append(A_expOracle_MI_ones)\n","    all_A_expOracle_MI.append(A_expOracle_MI)\n","    model_expOracle_MI.append(model_expOracle_MI_0)\n","    model_expOracle_MI_ones.append(model_expOracle_MI_ones_0)\n","\n","# Convert list of arrays into a 2D NumPy array\n","all_A_expOracle_MI = np.array(all_A_expOracle_MI)  # Shape: (num_loops, array_length)\n","all_A_expOracle_MI_ones = np.array(all_A_expOracle_MI_ones)\n","\n","# Now, find the majority vote for each position\n","majority_vote = []\n","majority_vote_ones = []\n","for col in range(all_A_expOracle_MI.shape[1]):  # Iterate over each column (position)\n","    votes = all_A_expOracle_MI[:, col]  # Get the votes for the current position\n","    most_common_vote = Counter(votes).most_common(1)[0][0]  # Get the majority vote\n","    majority_vote.append(most_common_vote)\n","\n","for col in range(all_A_expOracle_MI_ones.shape[1]):  # Iterate over each column (position)\n","    votes_ones = all_A_expOracle_MI_ones[:, col]  # Get the votes for the current position\n","    most_common_vote_ones = Counter(votes_ones).most_common(1)[0][0]  # Get the majority vote\n","    majority_vote_ones.append(most_common_vote_ones)\n","\n","# Convert the result to a NumPy array (if needed)\n","final_majority_vote = np.array(majority_vote)\n","A_MI = final_majority_vote\n","final_majority_vote_ones = np.array(majority_vote_ones)\n","A_MI_ones = final_majority_vote_ones\n","\n","\n","# QUA/INF E(Y|X,S,A)\n","set_random(seed)\n","df0[\"Yhat\"] = fit_Y_XS_A0.predict_proba(df0[XS_names])[:,1]\n","df1[\"Yhat\"] = fit_Y_XS_A1.predict_proba(df1[XS_names])[:,1]\n","df0[\"Yhat_ones\"] = fit_Y_XS_A0_ones.predict_proba(df0[XS_names])[:,1]\n","df1[\"Yhat_ones\"] = fit_Y_XS_A1_ones.predict_proba(df1[XS_names])[:,1]\n","\n","fit_qua0, fit_qua1 = None, None\n","\n","fit_qua0_test, fit_qua1_test = None, None\n","fit_qua0_ones, fit_qua1_ones = None, None\n","if (s_type == \"cont\"): # quantile: Yhat|X,A (no S)\n","    set_random(seed)\n","    fit_qua0, g_qua0 = fit_quantile(df0[X_names], df0[\"Yhat\"], df[X_names], qua_use, is_tune, param_grid, df0[\"TW\"])\n","    fit_qua1, g_qua1 = fit_quantile(df1[X_names], df1[\"Yhat\"], df[X_names], qua_use, is_tune, param_grid, df1[\"TW\"])\n","\n","    fit_qua0_ones, g_qua0_ones = fit_quantile(df0[X_names], df0[\"Yhat_ones\"], df[X_names], qua_use, is_tune, param_grid, df0[\"TW_ones\"])\n","    fit_qua1_ones, g_qua1_ones = fit_quantile(df1[X_names], df1[\"Yhat_ones\"], df[X_names], qua_use, is_tune, param_grid, df1[\"TW_ones\"])\n","\n","if (s_type == \"disc\"): # infinite: try all s and find s that helps achieve a minimal\n","    g_qua0 = fit_infinite(df, X_names, XS_names, fit_Y_XS_A0)\n","    g_qua1 = fit_infinite(df, X_names, XS_names, fit_Y_XS_A1)\n","\n","y_qua, w_qua = get_label_wt(g_qua1, g_qua0)\n","y_qua_ones, w_qua_ones = get_label_wt(g_qua1_ones, g_qua0_ones)\n","set_random(seed)\n","\n","\n","\n","A_te, A_tr, model_rise = class_pred(df, df_test, X_names, y_qua, w_qua, s_type, is_tune, param_grid, df[\"TW\"],S_names)\n","A_te_ones, A_tr_ones, model_rise_ones = class_pred(df, df_test, X_names, y_qua_ones, w_qua_ones, s_type, is_tune, param_grid, df[\"TW_ones\"],S_names)\n","\n","\n","\n","\n","# EXP E(Y|X,S,A) # XS train, XS predict\n","\n","obj_name = \"_base_\"\n","g_base0, _ = fit_expectation(obj_name, df0, X_names, df, is_class, is_tune, param_grid, df0[\"TW\"], df0_te)\n","g_base1, _ = fit_expectation(obj_name, df1, X_names, df, is_class, is_tune, param_grid, df1[\"TW\"], df1_te)\n","\n","y_base, w_base = get_label_wt(g_base1, g_base0)\n","A_te_base, A_tr_base, model_rise_base = class_pred(df, df_test, X_names, y_base, w_base, s_type, is_tune, param_grid, df[\"TW\"])\n","\n","g_base0_ones, _ = fit_expectation(obj_name, df0, X_names, df, is_class, is_tune, param_grid, df0[\"TW_ones\"], df0_te)\n","g_base1_ones, _ = fit_expectation(obj_name, df1, X_names, df, is_class, is_tune, param_grid, df1[\"TW_ones\"], df1_te)\n","\n","y_base_ones, w_base_ones = get_label_wt(g_base1_ones, g_base0_ones)\n","A_te_base_ones, A_tr_base_ones, model_rise_base_ones = class_pred(df, df_test, X_names, y_base_ones, w_base_ones, s_type, is_tune, param_grid, df[\"TW_ones\"])\n","\n","obj_name = \"_exp_\"\n","A_te_exp, A_tr_exp, model_rise_exp = class_pred(df, df_test, X_names, y_exp, w_exp, s_type, is_tune, param_grid, df[\"TW\"])\n","A_te_exp_ones, A_tr_exp_ones, model_rise_exp_ones = class_pred(df, df_test, X_names, y_exp_ones, w_exp_ones, s_type, is_tune, param_grid, df[\"TW_ones\"])\n","\n","\n","\n","A_te_obs = np.where(df_test[\"A\"]==0,\"yes\",\"no\")\n","\n","if is_plot:\n","\n","    out_mod = out_model(seed, df, df_test, X_names, y_qua, w_qua, s_type, is_tune, param_grid, df[\"TW\"])\n","    if (method == \"expOracle\"):\n","        out_mod = out_model(seed, df, df_test, XS_names, y_exp, w_exp, s_type, is_tune, param_grid, df[\"TW\"])\n","    if (method == \"base\"):\n","        out_mod = out_model(seed, df, df_test, X_names, y_base, w_base, s_type, is_tune, param_grid, df[\"TW\"])\n","    if (method == \"exp\"):\n","        out_mod = out_model(seed, df, df_test,X_names, y_exp, w_exp, s_type, is_tune, param_grid, df[\"TW\"])\n","\n","else:\n","    out_mod = None\n","out_mod_ones=None\n","\n"],"metadata":{"id":"cf3RQQv45EdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","############################################################################\n","############################# Evaluate Metrics #############################\n","############################################################################\n","\n","pqa=0.2\n","\n","fit_qua0_pqa, _ = fit_quantile(df0[X_names], df0[\"Yhat\"], df[X_names], pqa, is_tune, param_grid, df0[\"TW\"])\n","fit_qua1_pqa, _ = fit_quantile(df1[X_names], df1[\"Yhat\"], df[X_names], pqa, is_tune, param_grid, df1[\"TW\"])\n","\n","\n","A_name = 'A_rise'\n","M_name = 'M_opt'\n","df_test[A_name] = A_te\n","\n","A_te_MI=A_MI\n","df_test['A_obs'] = A_te_obs\n","df_test['A_base'] = A_te_base\n","df_test['A_exp'] = A_te_exp\n","df_test['A_MI'] = A_MI\n","df_test['A_expOracle'] = A_te_expOracle\n","\n","df_test['A_base_ones'] = A_te_base_ones\n","df_test['A_exp_ones'] = A_te_exp_ones\n","df_test['A_MI_ones'] = A_MI_ones\n","df_test['A_expOracle_ones'] = A_te_expOracle_ones\n","df_test['A_rise_ones'] = A_te_ones\n","\n","\n","\n","df_test['Yhat_XS_A0'] = fit_Y_XS_A0.predict_proba(df_test[XS_names])[:,1]\n","df_test['Yhat_XS_A1'] = fit_Y_XS_A1.predict_proba(df_test[XS_names])[:,1]\n","\n","\n","\n","def invlogit(x):\n","    return 1 / (1 + np.exp(-x))\n","def logit(p):\n","    return np.log(p / (1 - p))\n","df_test['q_A0'] = invlogit(fit_qua0_pqa.predict(df_test[X_names]))\n","\n","\n","df_test['q_A1'] = invlogit(fit_qua1_pqa.predict(df_test[X_names]))\n","\n","\n","if s_type in [\"cont\", \"disc\"]:\n","\n","\n","    df_test[M_name]=optimal_AS_inf(df_test, proj, is_sim, s_type, X_names, XS_names, y_fn,\n","                   is_class, pqa, fit_Y_XS_A0, fit_Y_XS_A1, fit_qua0, fit_qua1, fit_qua0_pqa, fit_qua1_pqa)\n","    df_predvul=df_test.copy()\n","    df_predvul[\"weights\"]=1\n","    print(\"20240220*************\")\n","\n","    print(np.mean(df_test[M_name]))\n","\n","else: # \"multi-disc\"\n","    df_test[M_name] = optimal_AS_inf_mulS(df_test, proj, is_sim, s_type, X_names,\n","                XS_names, y_fn, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","\n","\n","\n","_, _, fit_vul_XS = class_pred(df_predvul, df_predvul, XS_names, df_predvul[M_name], df_predvul[\"weights\"], s_type, is_tune, param_grid, df_predvul[\"weights\"])\n","\n","plot_shap(fit_vul_XS,XS_names,df_predvul, title=\"key factors of vulnerable samples\")\n","\n","\n","_, _, fit_vul_X = class_pred(df_predvul, df_predvul, X_names, df_predvul[M_name], df_predvul[\"weights\"], s_type, is_tune, param_grid, df_predvul[\"weights\"])\n","\n","plot_shap(fit_vul_X,X_names,df_predvul, title=\"key factors of vulnerable samples\")\n","\n","# metrics\n","if is_rct:\n","    Rval_all, Rval_vul, Rvalue_rest = expect_reward(df_test, A_name, M_name)\n","    Rval_all_obs, Rval_vul_obs, Rvalue_rest_obs = expect_reward(df_test, 'A_obs', M_name)\n","    Rval_all_base, Rval_vul_base, Rvalue_rest_base = expect_reward(df_test, 'A_base', M_name)\n","    Rval_all_exp, Rval_vul_exp, Rvalue_rest_exp = expect_reward(df_test, 'A_exp', M_name)\n","    Rval_all_expOracle, Rval_vul_expOracle, Rvalue_rest_expOracle = expect_reward(df_test, 'A_expOracle', M_name)\n","\n","    val_all, val_vul, value_rest, _ = get_value_fn(df_test, A_name, XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_obs, val_vul_obs, value_rest_obs, _ = get_value_fn(df_test, 'A_obs', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_base, val_vul_base, value_rest_base, _ = get_value_fn(df_test, 'A_base', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_exp, val_vul_exp, value_rest_exp, _ = get_value_fn(df_test, 'A_exp', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_MI, val_vul_MI, value_rest_MI, _ = get_value_fn(df_test, 'A_MI', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_expOracle, val_vul_expOracle, value_rest_expOracle, _ = get_value_fn(df_test, 'A_expOracle', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","    Rval_all_ones, Rval_vul_ones, Rvalue_rest_ones = expect_reward(df_test, 'A_rise_ones', M_name)\n","    Rval_all_base_ones, Rval_vul_base_ones, Rvalue_rest_base_ones = expect_reward(df_test, 'A_base_ones', M_name)\n","    Rval_all_exp_ones, Rval_vul_exp_ones, Rvalue_rest_exp_ones = expect_reward(df_test, 'A_exp_ones', M_name)\n","    Rval_all_expOracle_ones, Rval_vul_expOracle_ones, Rvalue_rest_expOracle_ones = expect_reward(df_test, 'A_expOracle_ones', M_name)\n","\n","    val_all_ones, val_vul_ones, value_rest_ones, _ = get_value_fn(df_test, 'A_rise_ones', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_base_ones, val_vul_base_ones, value_rest_base_ones, _ = get_value_fn(df_test, 'A_base_ones', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_exp_ones, val_vul_exp_ones, value_rest_exp_ones, _ = get_value_fn(df_test, 'A_exp_ones', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_MI_ones, val_vul_MI_ones, value_rest_MI_ones, _ = get_value_fn(df_test, 'A_MI_ones', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_expOracle_ones, val_vul_expOracle_ones, value_rest_expOracle_ones, _ = get_value_fn(df_test, 'A_expOracle_ones', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","else:\n","\n","    val_all, val_vul, value_rest, _ = get_value_fn(df_test, A_name, XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_obs, val_vul_obs, value_rest_obs, _ = get_value_fn(df_test, 'A_obs', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_base, val_vul_base, value_rest_base, _ = get_value_fn(df_test, 'A_base', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_exp, val_vul_exp, value_rest_exp, _ = get_value_fn(df_test, 'A_exp', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_MI, val_vul_MI, value_rest_MI, _ = get_value_fn(df_test, 'A_MI', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_expOracle, val_vul_expOracle, value_rest_expOracle, _ = get_value_fn(df_test, 'A_expOracle', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","    val_all_ones, val_vul_ones, value_rest_ones, _ = get_value_fn(df_test, 'A_rise_ones', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_base_ones, val_vul_base_ones, value_rest_base_ones, _ = get_value_fn(df_test, 'A_base_ones', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_exp_ones, val_vul_exp_ones, value_rest_exp_ones, _ = get_value_fn(df_test, 'A_exp_ones', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_MI_ones, val_vul_MI_ones, value_rest_MI_ones, _ = get_value_fn(df_test, 'A_MI_ones', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","    val_all_expOracle_ones, val_vul_expOracle_ones, value_rest_expOracle_ones, _ = get_value_fn(df_test, 'A_expOracle_ones', XS_names, M_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","if s_type == \"disc\" or y_fn is not None: # real with disc S | sim with disc/cont S\n","    obj_all, obj_vul, obj_rest = get_obj_inf(df_test, X_names, XS_names, A_name, M_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","    obj_all_obs, obj_vul_obs, obj_rest_obs = get_obj_inf(df_test, X_names, XS_names, 'A_obs', M_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","    obj_all_base, obj_vul_base, obj_rest_base = get_obj_inf(df_test, X_names, XS_names, 'A_base', M_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","    obj_all_exp, obj_vul_exp, obj_rest_exp = get_obj_inf(df_test, X_names, XS_names, 'A_exp', M_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","    obj_all_expOracle, obj_vul_expOracle, obj_rest_expOracle = get_obj_inf(df_test, X_names, XS_names, 'A_expOracle', M_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","    obj_all_ones, obj_vul_ones, obj_rest_ones = get_obj_inf(df_test, X_names, XS_names, 'A_rise_ones', M_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","    obj_all_base_ones, obj_vul_base_ones, obj_rest_base_ones = get_obj_inf(df_test, X_names, XS_names, 'A_base_ones', M_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","    obj_all_exp_ones, obj_vul_exp_ones, obj_rest_exp_ones = get_obj_inf(df_test, X_names, XS_names, 'A_exp_ones', M_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","    obj_all_expOracle_ones, obj_vul_expOracle_ones, obj_rest_expOracle_ones = get_obj_inf(df_test, X_names, XS_names, 'A_expOracle_ones', M_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","elif s_type == \"cont\": # real with cont S\n","    obj_all, obj_vul, obj_rest = get_obj_qua(df_test, X_names, s_type, A_name, M_name, fit_qua0, fit_qua1)\n","    obj_all_obs, obj_vul_obs, obj_rest_obs = get_obj_qua(df_test, X_names, s_type, 'A_obs', M_name, fit_qua0, fit_qua1)\n","    obj_all_base, obj_vul_base, obj_rest_base = get_obj_qua(df_test, X_names, s_type, 'A_base', M_name, fit_qua0, fit_qua1)\n","    obj_all_exp, obj_vul_exp, obj_rest_exp = get_obj_qua(df_test, X_names, s_type, 'A_exp', M_name, fit_qua0, fit_qua1)\n","    obj_all_MI, obj_vul_MI, obj_rest_MI = get_obj_qua(df_test, X_names, s_type, 'A_MI', M_name, fit_qua0, fit_qua1)\n","    obj_all_expOracle, obj_vul_expOracle, obj_rest_expOracle = get_obj_qua(df_test, X_names, s_type, 'A_expOracle', M_name, fit_qua0, fit_qua1)\n","\n","    obj_all_ones, obj_vul_ones, obj_rest_ones = get_obj_qua(df_test, X_names, s_type, 'A_rise_ones', M_name, fit_qua0, fit_qua1)\n","    obj_all_base_ones, obj_vul_base_ones, obj_rest_base_ones = get_obj_qua(df_test, X_names, s_type, 'A_base_ones', M_name, fit_qua0, fit_qua1)\n","    obj_all_exp_ones, obj_vul_exp_ones, obj_rest_exp_ones = get_obj_qua(df_test, X_names, s_type, 'A_exp_ones', M_name, fit_qua0, fit_qua1)\n","    obj_all_MI_ones, obj_vul_MI_ones, obj_rest_MI_ones = get_obj_qua(df_test, X_names, s_type, 'A_MI_ones', M_name, fit_qua0, fit_qua1)\n","    obj_all_expOracle_ones, obj_vul_expOracle_ones, obj_rest_expOracle_ones = get_obj_qua(df_test, X_names, s_type, 'A_expOracle_ones', M_name, fit_qua0, fit_qua1)\n","\n","# out\n","A_pred = df_test[A_name].values\n","A_pred_obs = df_test['A_obs'].values\n","A_pred_base = df_test['A_base'].values\n","A_pred_exp = df_test['A_exp'].values\n","A_pred_expOracle = df_test['A_expOracle'].values\n","\n","A_pred_ones = df_test['A_rise_ones'].values\n","A_pred_base_ones = df_test['A_base_ones'].values\n","A_pred_exp_ones = df_test['A_exp_ones'].values\n","A_pred_expOracle_ones = df_test['A_expOracle_ones'].values\n","\n","is_vul = df_test[M_name].values\n","out_res = output(A_pred, is_vul, obj_all, val_all, obj_vul, val_vul, model_rise)\n","out_res_obs = output(A_pred_obs, is_vul, obj_all_obs, val_all_obs, obj_vul_obs, val_vul_obs, model_rise)\n","out_res_base = output(A_pred_base, is_vul, obj_all_base, val_all_base, obj_vul_base, val_vul_base, model_rise_base)\n","out_res_exp = output(A_pred_exp, is_vul, obj_all_exp, val_all_exp, obj_vul_exp, val_vul_exp, model_rise_exp)\n","out_res_MI = output(A_pred_exp, is_vul, obj_all_MI, val_all_MI, obj_vul_MI, val_vul_MI, model_rise_exp)\n","out_res_expOracle = output(A_pred_expOracle, is_vul, obj_all_expOracle, val_all_expOracle, obj_vul_expOracle, val_vul_expOracle, model_rise_expOracle)\n","\n","out_res_ones = output(A_pred_ones, is_vul, obj_all_ones, val_all_ones, obj_vul_ones, val_vul_ones, model_rise_ones)\n","out_res_base_ones = output(A_pred_base_ones, is_vul, obj_all_base_ones, val_all_base_ones, obj_vul_base_ones, val_vul_base_ones, model_rise_base_ones)\n","out_res_exp_ones = output(A_pred_exp_ones, is_vul, obj_all_exp_ones, val_all_exp_ones, obj_vul_exp_ones, val_vul_exp_ones, model_rise_exp_ones)\n","out_res_MI_ones = output(A_pred_exp_ones, is_vul, obj_all_MI_ones, val_all_MI_ones, obj_vul_MI_ones, val_vul_MI_ones, model_rise_exp_ones)\n","out_res_expOracle_ones = output(A_pred_expOracle_ones, is_vul, obj_all_expOracle_ones, val_all_expOracle_ones, obj_vul_expOracle_ones, val_vul_expOracle_ones, model_rise_expOracle_ones)\n","\n","if is_rct:\n","    Rout_res = output(A_pred, is_vul, obj_all, Rval_all, obj_vul, Rval_vul, model_rise)\n","    Rout_res_obs = output(A_pred_obs, is_vul, obj_all_obs, Rval_all_obs, obj_vul_obs, Rval_vul_obs, model_rise)\n","    Rout_res_base = output(A_pred_base, is_vul, obj_all_base, Rval_all_base, obj_vul_base, Rval_vul_base, model_rise_base)\n","    Rout_res_exp = output(A_pred_exp, is_vul, obj_all_exp, Rval_all_exp, obj_vul_exp, Rval_vul_exp, model_rise_exp)\n","    Rout_res_MI = output(A_pred_exp, is_vul, obj_all_exp, Rval_all_MI, obj_vul_exp, Rval_vul_MI, model_rise_exp)\n","    Rout_res_expOracle = output(A_pred_expOracle, is_vul, obj_all_expOracle, Rval_all_expOracle, obj_vul_expOracle, Rval_vul_expOracle, model_rise_expOracle)\n","\n","    Rout_res_ones = output(A_pred_ones, is_vul, obj_all_ones, Rval_all_ones, obj_vul_ones, Rval_vul_ones, model_rise_ones)\n","    Rout_res_base_ones = output(A_pred_base_ones, is_vul, obj_all_base_ones, Rval_all_base_ones, obj_vul_base_ones, Rval_vul_base_ones, model_rise_base_ones)\n","    Rout_res_exp_ones = output(A_pred_exp_ones, is_vul, obj_all_exp_ones, Rval_all_exp_ones, obj_vul_exp_ones, Rval_vul_exp_ones, model_rise_exp_ones)\n","    Rout_res_expOracle_ones = output(A_pred_expOracle_ones, is_vul, obj_all_expOracle_ones, Rval_all_expOracle_ones, obj_vul_expOracle_ones, Rval_vul_expOracle_ones, model_rise_expOracle_ones)\n","\n","\n","\n","\n","\n","\n","\n","plot_shap(model_rise_exp_ones,X_names,df_test, title=\"mean\")\n","plot_shap(model_rise_exp,X_names,df_test, title=\"mean-tl\")\n","\n","plot_shap(model_rise_expOracle_ones,XS_names,df_test, title=\"full\")\n","plot_shap(model_rise_expOracle,XS_names,df_test, title=\"full-tl\")\n","\n","plot_shap(model_rise_ones,X_names,df_test, title=\"robust\")\n","plot_shap(model_rise,X_names,df_test, title=\"robust-tl\")\n","\n","if is_rct:\n","    out_res_rise, out_mod, out_res_obs, out_res_base, out_res_exp, out_res_expOracle, out_res_rise_ones, out_mod_ones, out_res_base_ones, out_res_exp_ones, out_res_expOracle_ones = out_res, out_mod, out_res_obs, out_res_base, out_res_exp, out_res_expOracle, Rout_res, Rout_res_obs, Rout_res_base, Rout_res_exp, Rout_res_expOracle, out_res_ones, out_mod_ones, out_res_base_ones, out_res_exp_ones, out_res_expOracle_ones, Rout_res_ones, Rout_res_base_ones, Rout_res_exp_ones, Rout_res_expOracle_ones\n","else:\n","    out_res_rise, out_mod, out_res_obs, out_res_base, out_res_exp, out_res_expOracle, out_res_rise_ones, out_mod_ones, out_res_base_ones, out_res_exp_ones, out_res_expOracle_ones = out_res, out_mod, out_res_obs, out_res_base, out_res_exp, out_res_expOracle, out_res_ones, out_mod_ones, out_res_base_ones, out_res_exp_ones, out_res_expOracle_ones\n","\n","print(\"obs\")\n","print(\"obj_all:\", np.round(out_res_obs.obj_all,2))\n","print(\"obj_vul:\", np.round(out_res_obs.obj_vul,2))\n","print(\"val_all:\", np.round(out_res_obs.val_all,2))\n","print(\"val_vul:\", np.round(out_res_obs.val_vul,2))\n","print(\"num_trt:\", out_res_rise.A_pred[np.where(out_res_obs.A_pred==\"yes\")].shape[0])\n","\n","\n","print(\"base_ones\")\n","print(\"obj_all:\", np.round(out_res_base_ones.obj_all,2))\n","print(\"obj_vul:\", np.round(out_res_base_ones.obj_vul,2))\n","print(\"val_all:\", np.round(out_res_base_ones.val_all,2))\n","print(\"val_vul:\", np.round(out_res_base_ones.val_vul,2))\n","print(\"num_vul:\", out_res_base.is_vul[np.where(out_res_base.is_vul==1)].shape[0])\n","print(\"num_trt:\", out_res_base.A_pred[np.where(out_res_base_ones.A_pred==\"yes\")].shape[0])\n","\n","\n","print(\"exp_ones\")\n","print(\"obj_all:\", np.round(out_res_exp_ones.obj_all,3))\n","print(\"obj_vul:\", np.round(out_res_exp_ones.obj_vul,3))\n","print(\"val_all:\", np.round(out_res_exp_ones.val_all,3))\n","print(\"val_vul:\", np.round(out_res_exp_ones.val_vul,3))\n","print(\"num_vul:\", out_res_exp.is_vul[np.where(out_res_exp.is_vul==1)].shape[0])\n","print(\"num_trt:\", out_res_exp.A_pred[np.where(out_res_exp_ones.A_pred==\"yes\")].shape[0])\n","\n","print(\"MI_ones\")\n","print(\"obj_all:\", np.round(out_res_MI_ones.obj_all,3))\n","print(\"obj_vul:\", np.round(out_res_MI_ones.obj_vul,3))\n","print(\"val_all:\", np.round(out_res_MI_ones.val_all,3))\n","print(\"val_vul:\", np.round(out_res_MI_ones.val_vul,3))\n","\n","print(\"expOracle_ones\")\n","print(\"obj_all:\", np.round(out_res_expOracle_ones.obj_all,3))\n","print(\"obj_vul:\", np.round(out_res_expOracle_ones.obj_vul,3))\n","print(\"val_all:\", np.round(out_res_expOracle_ones.val_all,3))\n","print(\"val_vul:\", np.round(out_res_expOracle_ones.val_vul,3))\n","print(\"num_vul:\", out_res_expOracle.is_vul[np.where(out_res_expOracle.is_vul==1)].shape[0])\n","print(\"num_trt:\", out_res_expOracle.A_pred[np.where(out_res_expOracle_ones.A_pred==\"yes\")].shape[0])\n","\n","\n","print(\"rise_ones\")\n","print(\"obj_all:\", np.round(out_res_rise_ones.obj_all,3))\n","print(\"obj_vul:\", np.round(out_res_rise_ones.obj_vul,3))\n","print(\"val_all:\", np.round(out_res_rise_ones.val_all,3))\n","print(\"val_vul:\", np.round(out_res_rise_ones.val_vul,3))\n","print(\"num_vul:\", out_res_rise.is_vul[np.where(out_res_rise.is_vul==1)].shape[0])\n","print(\"num_trt:\", out_res_rise.A_pred[np.where(out_res_rise_ones.A_pred==\"yes\")].shape[0])\n","\n","\n","print(\"base\")\n","print(\"obj_all:\", np.round(out_res_base.obj_all,3))\n","print(\"obj_vul:\", np.round(out_res_base.obj_vul,3))\n","print(\"val_all:\", np.round(out_res_base.val_all,3))\n","print(\"val_vul:\", np.round(out_res_base.val_vul,3))\n","print(\"num_vul:\", out_res_base.is_vul[np.where(out_res_base.is_vul==1)].shape[0])\n","print(\"num_trt:\", out_res_base.A_pred[np.where(out_res_base.A_pred==\"yes\")].shape[0])\n","\n","\n","print(\"exp\")\n","print(\"obj_all:\", np.round(out_res_exp.obj_all,3))\n","print(\"obj_vul:\", np.round(out_res_exp.obj_vul,3))\n","print(\"val_all:\", np.round(out_res_exp.val_all,3))\n","print(\"val_vul:\", np.round(out_res_exp.val_vul,3))\n","print(\"num_vul:\", out_res_exp.is_vul[np.where(out_res_exp.is_vul==1)].shape[0])\n","print(\"num_trt:\", out_res_exp.A_pred[np.where(out_res_exp.A_pred==\"yes\")].shape[0])\n","\n","print(\"MI\")\n","print(\"obj_all:\", np.round(out_res_MI.obj_all,3))\n","print(\"obj_vul:\", np.round(out_res_MI.obj_vul,3))\n","print(\"val_all:\", np.round(out_res_MI.val_all,3))\n","print(\"val_vul:\", np.round(out_res_MI.val_vul,3))\n","\n","print(\"expOracle\")\n","print(\"obj_all:\", np.round(out_res_expOracle.obj_all,3))\n","print(\"obj_vul:\", np.round(out_res_expOracle.obj_vul,3))\n","print(\"val_all:\", np.round(out_res_expOracle.val_all,3))\n","print(\"val_vul:\", np.round(out_res_expOracle.val_vul,3))\n","print(\"num_vul:\", out_res_expOracle.is_vul[np.where(out_res_expOracle.is_vul==1)].shape[0])\n","print(\"num_trt:\", out_res_expOracle.A_pred[np.where(out_res_expOracle.A_pred==\"yes\")].shape[0])\n","\n","\n","print(\"rise\")\n","print(\"obj_all:\", np.round(out_res_rise.obj_all,3))\n","print(\"obj_vul:\", np.round(out_res_rise.obj_vul,3))\n","print(\"val_all:\", np.round(out_res_rise.val_all,3))\n","print(\"val_vul:\", np.round(out_res_rise.val_vul,3))\n","print(\"num_vul:\", out_res_rise.is_vul[np.where(out_res_rise.is_vul==1)].shape[0])\n","print(\"num_trt:\", out_res_rise.A_pred[np.where(out_res_rise.A_pred==\"yes\")].shape[0])\n"],"metadata":{"id":"AbBjj2o56hTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def optimal_AS_inf(df, proj, is_sim, s_type, X_names, XS_names, y_fn,\n","                   is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1, fit_qua0, fit_qua1, fit_qua0_pqa, fit_qua1_pqa):\n","    \"\"\"\n","    Used for real data with disc S | simulation with disc/cont S\n","\n","    get df with minority index\n","        input:  df_train for E(Y|X,S,A) and df_test for Yhat -> optimal S\n","        output: df_test with vulnerable index\n","    \"\"\"\n","    df_use = df.copy()\n","\n","    S_names = list( set(XS_names) - set(X_names) )\n","    # S grid from empirical test data\n","\n","    X_df = df_use[X_names].copy()\n","\n","    # link S* to observed data\n","    XS_df = df_use[X_names+S_names].copy()\n","\n","\n","    def invlogit(x):\n","        return 1 / (1 + np.exp(-x))\n","    def logit(p):\n","        return np.log(p / (1 - p))\n","\n","\n","    XS_df[\"EY_0\"] = fit_Y_XS_A0.predict_proba(df_test[XS_names])[:,1]\n","    XS_df[\"EY_1\"] = fit_Y_XS_A1.predict_proba(df_test[XS_names])[:,1]\n","\n","    XS_df[\"QY_0\"] = invlogit(fit_qua0.predict(df_test[X_names]))\n","    XS_df[\"QY_1\"] = invlogit(fit_qua1.predict(df_test[X_names]))\n","\n","    XS_df[\"QY_0_pqa\"] = invlogit(fit_qua0_pqa.predict(df_test[X_names]))\n","    XS_df[\"QY_1_pqa\"] = invlogit(fit_qua1_pqa.predict(df_test[X_names]))\n","\n","\n","\n","    XS_df['M_opt']=0\n","    XS_df['M_opt'] = np.where((((XS_df['QY_0']>XS_df['QY_1'])&(XS_df['QY_1_pqa']>=XS_df['EY_1']))|((XS_df['QY_1']>XS_df['QY_0'])&(XS_df['QY_0_pqa']>=XS_df['EY_0']))),1,0)\n","\n","    merge_df=XS_df.copy()\n","\n","    # minor_index: 1-vulnerable, 0-rest\n","    print(\"M_opt 1:vulnerable 0:rest\\n\", merge_df['M_opt'].value_counts(dropna=False))\n","    M_opt = merge_df['M_opt'].copy()\n","    assert( M_opt.shape[0] == X_df.shape[0] )\n","    print(XS_df)\n","    return M_opt"],"metadata":{"id":"gMu5FhgoQlkE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvGDOTXYw6D2"},"source":["plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ab8N3ndTw75m"},"outputs":[],"source":["\n","!pip install shap\n","\n","import shap\n","\n","\n","def plot_shap(out, save_path):\n","    tf.compat.v1.disable_v2_behavior()\n","\n","    _, _, model = class_pred(out.df, out.df_test, out.X_names, out.y_qua, out.w_qua, out.s_type, out.is_tune,None, transfer_weights=out.transfer_weights, S_names=[\"S\"])\n","\n","    explainer = shap.DeepExplainer(model, out.df[out.X_names])\n","    shap_values = explainer.shap_values(out.df_test[out.X_names].values)\n","    shap_abs(shap_values[0], out.df_test[out.X_names], save_path+\"fig_rise\")\n","\n","def plot_shap(model,shap_names,df_test,title): #df_test[X_names]\n","    tf.compat.v1.disable_v2_behavior()\n","\n","    explainer = shap.TreeExplainer(model)\n","    shap_values = explainer.shap_values(df_test[shap_names])\n","\n","    shap_abs(shap_values, df_test[shap_names],title)\n","\n","\n","\n","def shap_abs(df_shap, df_in,title):\n","    \"\"\"\n","    Simplified version of SHAP representation for easier interpretation\n","    modified based on https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d\n","    \"\"\"\n","\n","    shap_v = pd.DataFrame(df_shap)\n","    feature_list = df_in.columns\n","    shap_v.columns = feature_list\n","    df_v = df_in.copy().reset_index().drop('index',axis=1)\n","\n","    # Determine the correlation in order to plot with different colors\n","    corr_list = list()\n","    for i in feature_list:\n","        b = np.corrcoef(shap_v[i],df_v[i])[1][0]\n","        corr_list.append(b)\n","    corr_df = pd.concat([pd.Series(feature_list),pd.Series(corr_list)],axis=1).fillna(0)\n","    corr_df.columns  = ['Variable','Corr']\n","\n","    corr_df['Sign'] = np.where(corr_df['Corr']>0,'dimgrey','lightgrey')\n","\n","    shap_abs = np.abs(shap_v)\n","    k=pd.DataFrame(shap_abs.mean()).reset_index()\n","    k.columns = ['Variable','SHAP_abs']\n","    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n","    k2 = k2.sort_values(by='SHAP_abs',ascending = True)\n","    colorlist = k2['Sign']\n","    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, figsize=(5,6),legend=False, title=title)\n","    ax.set_xlabel(\"SHAP Value (Darkgrey = Positive Impact)\")\n","    fig = ax.get_figure()\n","    fig.tight_layout()\n","\n","\n","\n"]},{"cell_type":"code","source":["import cvxpy as cp\n","import numpy as np\n","\n","def entr(w):\n","    # Define the entropy function\n","    return cp.sum(cp.entr(w))\n","\n","def entr_balance(tol, x_in_rct, x_in_rwe):\n","    # the optimization problem for the nonparametric weighting method\n","    n = x_in_rct.shape[0]\n","    w = cp.Variable(n)\n","    p = x_in_rct.shape[1]-1\n","    objective = cp.Maximize(cp.sum(cp.entr(w)))\n","    constraints = [w >= 0, cp.sum(w) == 1]\n","    for k in range(p):\n","        constraints += [cp.abs(cp.sum(w*x_in_rct[:,k+1]) - np.mean(x_in_rwe[:,k+1])) <= tol*np.std(x_in_rwe[:,k+1])*np.sqrt(n/(n-1))]\n","        constraints += [cp.abs(cp.sum(w*(x_in_rct[:,k+1]**2)) - np.mean(x_in_rwe[:,k+1]**2)) <= tol*np.std(x_in_rwe[:,k+1]**2)*np.sqrt(n/(n-1))]\n","\n","        print(\"check std:\",np.std(x_in_rwe[:,k+1]))\n","\n","    prob = cp.Problem(objective, constraints)\n","    w_tilde = np.zeros(n)\n","    try:\n","        result = prob.solve(solver=cp.ECOS)\n","        w_tilde = w.value\n","\n","        print(k,w_tilde)\n","\n","        if w_tilde is None:\n","            w_tilde = np.zeros(n)\n","        else:\n","            if np.isnan(w_tilde[0]):\n","                w_tilde = np.zeros(n)\n","\n","    except cp.error.SolverError as e:\n","\n","        pass\n","    return w_tilde\n","\n","\n","def weight_tuned_entr(tols, x_in_rct, x_in_rwe, prop=0.5, smps=10):\n","    n = x_in_rct.shape[0]\n","    sds = np.apply_along_axis(np.std, 0, x_in_rwe)\n","    means = np.apply_along_axis(np.mean, 0, x_in_rwe)\n","    sds2 = np.apply_along_axis(np.std, 0, x_in_rwe**2)\n","    moment2 = np.apply_along_axis(np.mean, 0, x_in_rwe**2)\n","    cov_diff_bars = []\n","    w_tildes = np.full((n, len(tols)), np.nan)\n","    for i in range(len(tols)):\n","        tol = tols[i]\n","        w_tilde = entr_balance(tol, x_in_rct, x_in_rwe)\n","        w_tilde[w_tilde < 0] = 0  # the optimization results sometimes return negative weights, so clip them to 0\n","        if np.sum(w_tilde) == 0:\n","            cov_diff_bar = 1e+10\n","        else:\n","            w_tildes[:, i] = w_tilde\n","            cov_diffs = []\n","            for s in range(smps):\n","                boot_ind = np.random.choice(n, size=int(round(prop * n)), replace=True)\n","                x_in_rct_boot = x_in_rct[boot_ind, :]\n","                w_tilde_boot = w_tilde[boot_ind]\n","                cov_diff = (np.sum(x_in_rct_boot * w_tilde_boot.reshape((-1, 1)), axis=0) / np.sum(w_tilde_boot) - means)[1:] / sds[1:]\n","                cov_diff_moment2 = (np.sum(x_in_rct_boot**2 * w_tilde_boot.reshape((-1, 1)), axis=0) / np.sum(w_tilde_boot) - moment2)[1:] / sds2[1:]\n","\n","                cov_diffs.append(np.sum(np.concatenate((cov_diff, cov_diff_moment2))**2))  # L2 measure\n","            cov_diff_bar = np.mean(cov_diffs)\n","        cov_diff_bars.append(cov_diff_bar)\n","    tuned_tol = tols[np.argmin(cov_diff_bars)]\n","    if min(cov_diff_bars) < 1e+10:\n","        return w_tildes[:, np.argmin(cov_diff_bars)]\n","    else:\n","        raise ValueError('************ optimization not feasible *****************')\n","\n","\n","def learn_weights(y_in_rct, x_in_rct, a_in_rct, x_in_rwe, w_method, misspecify, N):\n","    # w_method: 1, 2, 3, 4\n","    n = x_in_rct.shape[0]\n","    p = x_in_rct.shape[1] - 1\n","    if w_method == 4:\n","\n","        w_tilde = weight_tuned_entr([ 1.000], x_in_rct, x_in_rwe)\n","    else:\n","        raise ValueError(\"Wrong input for weighting method\")\n","    return w_tilde\n","\n"],"metadata":{"id":"bCBi8PGUsEjH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5c0Ifh17QhrX"},"outputs":[],"source":["\n","def set_random(seed_value):\n","    os.environ['PYTHONHASHSEED']=str(seed_value)\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    tf.random.set_seed(seed_value)\n","\n","    return\n","\n","\n","def normalize(df_tr, df_te, names_to_norm):\n","    df = df_tr.copy()\n","    df_test = df_te.copy()\n","\n","    # normalize X & S\n","    scaler = StandardScaler()\n","    df[names_to_norm] = scaler.fit_transform(df[names_to_norm])\n","    df_test[names_to_norm] = scaler.transform(df_test[names_to_norm])\n","\n","    return(df.copy(), df_test.copy())\n","\n","\n","def get_train_test(df_all, s_type, seed_value):\n","    # train-test split by A\n","    df0_ori = df_all[df_all[\"A\"]==0].reset_index(drop=True).copy()\n","    df1_ori = df_all[df_all[\"A\"]==1].reset_index(drop=True).copy()\n","    df0_train, df0_test = train_test_split(df0_ori, test_size=0.2, random_state=seed_value)\n","    df1_train, df1_test = train_test_split(df1_ori, test_size=0.2, random_state=seed_value)\n","\n","    df_ori = pd.concat([df0_train, df1_train]).reset_index(drop=True).copy()\n","    df_test_ori = pd.concat([df0_test, df1_test]).reset_index(drop=True).copy()\n","\n","\n","    print(\"df_ori.shape\", df_ori.shape)\n","    if (s_type == \"disc\"):\n","        print(\"train Pr(S=0)/Pr(S=1):\", round(sum(df_ori[\"S\"]==0) / sum(df_ori[\"S\"]==1),3))\n","    print(\"train Pr(A=0)/Pr(A=1):\", round(sum(df_ori[\"A\"]==0) / sum(df_ori[\"A\"]==1),3))\n","\n","    print(\"df_test_ori.shape\", df_test_ori.shape)\n","    if (s_type == \"disc\"):\n","        print(\"test Pr(S=0)/Pr(S=1):\", round(sum(df_test_ori[\"S\"]==0) / sum(df_test_ori[\"S\"]==1),3))\n","    print(\"test Pr(A=0)/Pr(A=1):\", round(sum(df_test_ori[\"A\"]==0) / sum(df_test_ori[\"A\"]==1),3))\n","\n","    return(df_ori.copy(), df_test_ori.copy())\n","\n","\n","\n","\n","\n","\n","def get_ps_fit(df_train, use_covars, transfer_weights):\n","    df_tr = df_train.copy()\n","\n","    regr = RandomForestClassifier(random_state=1234).fit(df_tr[use_covars], df_tr[\"A\"],sample_weight=transfer_weights)\n","\n","    return(regr)\n","\n","def keras_wrap(x_train, train_labels, train_wts, x_test, loss_fn, act_out,\n","               layer=2, node=512, dropout=0.2, n_epoch=100, bsize=64, act=\"relu\",\n","               opt=\"Adam\", val_split=0.2, is_early_stop=True, verb=0, obj=None):\n","\n","\n","    val_split=None\n","\n","    x_train=x_train.reset_index(drop=True)\n","    if is_early_stop:\n","        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n","        callback = [early_stop]\n","    else:\n","        callback = None\n","\n","    # set input_dim for the number of features\n","    if len(x_train.shape) == 1:\n","        input_dim = 1\n","    else:\n","        input_dim = x_train.shape[1]\n","\n","    # model\n","    if obj==\"quan\":\n","\n","        param_grid = {'n_estimators': [100, 200, 300],\n","                      'learning_rate': [0.1, 0.2, 0.3],\n","                      'max_depth': [3, 4, 5]\n","                      }\n","        param_grid = {\n","            'n_estimators': [200],\n","            'max_depth': [3, 4],\n","            'learning_rate': [0.2, 0.3],\n","            'subsample': [0.5, 0.75, 1],\n","            'colsample_bytree': [0.6, 0.8],\n","            'gamma': [0]\n","            }\n","\n","# Create an instance of the XGBRegressor model\n","        model = XGBRegressor(objective=loss_fn, seed=5292023)\n","\n","# Perform grid search with cross-validation\n","        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n","        grid_search.fit(x_train, train_labels, sample_weight=train_wts)\n","\n","# Get the best model and its hyperparameters\n","        best_model = grid_search.best_estimator_\n","        best_params = grid_search.best_params_\n","\n","# Use the best model for prediction\n","        model = best_model\n","        pred_test = model.predict(x_test).flatten()\n","        pred_train = model.predict(x_train).flatten()\n","\n","\n","    else:\n","        param_grid = {\n","            'n_estimators': [150],\n","            'max_depth': [1, 2, 3],\n","            'learning_rate': [0.3, 0.4],\n","            'subsample': [0.5, 0.75, 1],\n","            'colsample_bytree': [0.6, 0.8],\n","            'gamma': [0]\n","            }\n","\n","    # Create an XGBoost classifier\n","        model = XGBClassifier(seed=5292023)\n","\n","    # Perform grid search with cross-validation\n","        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='roc_auc')\n","        grid_search.fit(x_train, train_labels, sample_weight=train_wts)\n","\n","    # Get the best model and its hyperparameters\n","        best_model = grid_search.best_estimator_\n","        best_params = grid_search.best_params_\n","\n","        model_caret = best_model\n","\n","\n","\n","        pred_test = model_caret.predict_proba(x_test)[:, 1].flatten()\n","        pred_train = model_caret.predict_proba(x_train)[:, 1].flatten()\n","        model=model_caret\n","\n","        print(pred_test.shape)\n","        print(\"check here (pred_test in keras wrap)\")\n","        print(pred_test)\n","\n","\n","    return pred_test, pred_train, model\n","\n","\n","def hyper_tuning(x_train, train_labels, train_wts, loss_fn, act_out,\n","                 param_grid, n_cv=5, n_jobs=1):\n","\n","    # set input_dim for the number of features\n","    if len(x_train.shape) == 1:\n","        input_dim = 1\n","    else:\n","        input_dim = x_train.shape[1]\n","\n","    def create_model(layers,nodes,activation,optimizer,dropout):\n","        model = Sequential()\n","        for i in range(layers):\n","            if i==0:\n","                model.add(Dense(nodes, input_dim=input_dim))\n","                model.add(Activation(activation))\n","                model.add(Dropout(dropout))\n","            else:\n","                model.add(Dense(nodes, activation=activation))\n","                model.add(Activation(activation))\n","                model.add(Dropout(dropout))\n","\n","        model.add(Dense(units=1, activation=act_out))\n","\n","        model.compile(optimizer=optimizer, loss=loss_fn, metrics=['acc'],weighted_metrics=['acc'])\n","        return model\n","\n","    model = KerasRegressor(build_fn=create_model, verbose=0)\n","\n","    assert param_grid is not None\n","    layers = param_grid['layers']\n","    nodes = param_grid['nodes']\n","    dropout = param_grid['dropouts']\n","    activation = param_grid['acts']\n","    optimizer = param_grid['opts']\n","    bsizes = param_grid['bsizes']\n","    n_epochs = param_grid['n_epochs']\n","\n","    param_grid = dict(layers=layers, nodes=nodes, activation=activation, optimizer=optimizer,\n","                      dropout=dropout, batch_size=bsizes, epochs=n_epochs)\n","    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=n_cv, n_jobs=n_jobs)\n","\n","    grid_result = grid.fit(x_train, train_labels, sample_weight=train_wts)\n","\n","    print(\"hyperparams tuning:\", grid_result.best_score_,grid_result.best_params_)\n","\n","    bst_params = grid_result.best_params_\n","    layer = bst_params['layers']\n","    node = bst_params['nodes']\n","    dropout = bst_params['dropout']\n","    n_epoch = bst_params['epochs']\n","    bsize = bst_params['batch_size']\n","    act = bst_params['activation']\n","    opt = bst_params['optimizer']\n","\n","    return layer, node, dropout, n_epoch, bsize, act, opt\n","\n","\n","\n","\n","\n","def fit_expectation(obj_name, df, use_covars, df_pred, is_class, is_tune, param_grid, transfer_weights, df_val=None):\n","    \"\"\"E(Y|X=x,A=a) or E(Y|X=x,S=s,A=a)\n","    (model separated by A)\n","    \"\"\"\n","\n","    df_tr = df.copy()\n","    df_te = df_pred.copy()\n","    if df_val is not None:\n","        df_va = df_val.copy()\n","\n","    # fit on df0/df1\n","    if is_class:\n","        loss_fn = 'binary_crossentropy'\n","        act_out = 'sigmoid'\n","    else:\n","        loss_fn = \"mean_squared_error\"\n","        act_out = None\n","\n","\n","    Yhat, _, regr = keras_wrap(df_tr[use_covars], df_tr[\"Y\"], transfer_weights,\n","                        df_te[use_covars], loss_fn, act_out,\n","                        layer=2, node=1024, dropout=0.2, n_epoch=100, bsize=64, act=\"relu\", opt=\"Adam\",\n","                        val_split=0.2, is_early_stop=True, verb=0)\n","\n","    if df_val is not None:\n","        if not is_class:\n","            y_tr = regr.predict(df_tr[use_covars])\n","            rms = mean_squared_error(df_tr['Y'], y_tr, squared=False)\n","            met = rms / (np.max(df_tr['Y']) - np.min(df_tr['Y']))\n","            print(obj_name, \"evaluate on train: NRMSE\", met)\n","\n","            y_va = regr.predict(df_va[use_covars])\n","            rms = mean_squared_error(df_va['Y'], y_va, squared=False)\n","            met = rms / (np.max(df_va['Y']) - np.min(df_va['Y']))\n","            print(obj_name, \"evaluate on test : NRMSE\", met)\n","        elif is_class:\n","            y_tr = regr.predict(df_tr[use_covars])\n","            met = roc_auc_score(df_tr['Y'], y_tr)\n","            print(obj_name, \"evaluate on train: AUC\", met)\n","\n","            y_va = regr.predict(df_va[use_covars])\n","            met = roc_auc_score(df_va['Y'], y_va)\n","            print(obj_name, \"evaluate on test : AUC\", met)\n","\n","    return(Yhat, regr)\n","\n","\n","def fit_quantile(x_train, train_labels, x_test, q, is_tune, param_grid, transfer_weights):\n","    \"\"\"\n","    quantile regression: yhat | X,A\n","    \"\"\"\n","\n","    def invlogit(x):\n","        return 1 / (1 + np.exp(-x))\n","    def logit(p):\n","        return np.log(p / (1 - p))\n","    train_labels=logit(train_labels)\n","\n","    def tilted_loss(q, y, f):\n","        \"\"\"quantile loss for Keras\"\"\"\n","\n","        e = (y - f)\n","        return keras.backend.mean(keras.backend.maximum(q * e, (q - 1) * e), axis=-1)\n","\n","    act_out = None\n","\n","    act_out = 'sigmoid'\n","    def loss_fn(y, f):\n","\n","        alpha = q\n","        x = y - f\n","\n","        grad = np.where(x >= 0, -alpha,  1 - alpha)\n","        hess = np.ones_like(x)\n","        return grad, hess\n","\n","    if is_tune:\n","        layer, node, dropout, n_epoch, bsize, act, opt = \\\n","                    hyper_tuning(x_train, train_labels, transfer_weights, loss_fn, act_out,\n","                        param_grid, n_cv=5, n_jobs=1)\n","\n","        pred_test, _, model = keras_wrap(x_train, train_labels, transfer_weights,\n","                        x_test, loss_fn, act_out,\n","                        layer, node, dropout, n_epoch, bsize, act, opt,\n","                        val_split=None, is_early_stop=False, verb=0, obj=\"quan\")\n","    else:\n","\n","\n","        pred_test, _, model = keras_wrap(x_train, train_labels, transfer_weights, x_test, loss_fn, act_out,\n","                layer=2, node=512, dropout=0.2, n_epoch=100, bsize=32, act=\"relu\",\n","                opt=\"Adam\", val_split=0.2, is_early_stop=True, verb=0, obj=\"quan\")\n","\n","    train_labels=invlogit(train_labels)\n","    pred_test=invlogit(pred_test)\n","    return model, pred_test\n","\n","\n","\n","\n","def fit_infinite(df, X_names, XS_names, fit):\n","    df_use = df.copy()\n","\n","    S_names = list( set(XS_names) - set(X_names) )\n","\n","    if len(S_names) == 1:\n","        s_min = int(np.min(df_use[S_names[0]]))\n","        s_max = int(np.max(df_use[S_names[0]]))\n","        s_grid = range(s_min, s_max+1)\n","    else:\n","        s_lst = []\n","        for s_var in S_names:\n","            s_min = int(np.min(df_use[s_var]))\n","            s_max = int(np.max(df_use[s_var]))\n","            s_grid = range(s_min, s_max+1)\n","            s_lst.append(np.array(s_grid))\n","        s_grid = s_lst.copy()\n","        print(\"s_grid:\", s_grid)\n","\n","    X_df = df_use[X_names].copy()\n","    idx = range(X_df.shape[0])\n","    X_df[\"idx\"] = idx\n","\n","    if len(S_names) == 1:\n","        idx_s = np.array(np.meshgrid(idx, s_grid)).reshape(2, len(idx)*len(s_grid)).T\n","        idx_s = pd.DataFrame(idx_s, columns = ['idx',S_names[0]])\n","    else:\n","        idx_s_lst = [np.array(idx)] + s_grid\n","        idx_s = np.array(np.meshgrid(*np.array(idx_s_lst, dtype=object))).reshape(1+len(S_names), -1).T\n","        idx_s = pd.DataFrame(idx_s, columns = ['idx']+S_names)\n","\n","    aug_df = X_df.merge(idx_s, on='idx', how='left').copy()\n","\n","    # predict Y\n","    aug_df[\"pred\"] = fit.predict(aug_df[XS_names])\n","\n","    smry = aug_df.groupby(by=\"idx\", as_index=False).agg({\"pred\":[\"min\"]})\n","\n","    return(smry.loc[:,(\"pred\",\"min\")].values.copy())\n","\n","\n","\n","def get_label_wt(g1, g0):\n","    \"\"\"get label & weight for classification\"\"\"\n","    c1 = g1\n","    c2 = g0\n","    label = np.sign(c1 - c2)\n","    label = np.where(label==1, 1, 0)\n","\n","    weight = np.abs(c1 - c2)\n","\n","\n","    return(label, weight)\n","\n","\n","\n","\n","def class_pred(df_train, df_test, use_covars, label, weight, s_type, is_tune, param_grid,transfer_weights,S_names=None):\n","    \"\"\"binary classification with sample weights for decision rule by Keras\n","    \"\"\"\n","\n","    df_tr = df_train.reset_index(drop=True).copy()\n","    df_te = df_test.reset_index(drop=True).copy()\n","    print(df_train)\n","    print(df_test)\n","    print(use_covars)\n","    print(label)\n","\n","    loss_fn = 'binary_crossentropy'\n","    act_out = 'sigmoid'\n","\n","\n","\n","    prob_test, prob_train, model = keras_wrap(df_tr[use_covars], label, weight*transfer_weights,\n","                    df_te[use_covars], loss_fn, act_out,\n","                    layer=2, node=1024, dropout=0.2, n_epoch=100, bsize=64, act=\"relu\",\n","                    opt=\"Adam\", val_split=None, is_early_stop=True, verb=0)\n","\n","\n","    pred_test = np.where(prob_test>0.5, \"yes\", \"no\")\n","    pred_train = np.where(prob_train>0.5, \"yes\", \"no\")\n","\n","    # print table\n","    print(\"pred_train table:\", collections.Counter(pred_train.ravel()))\n","    print(\"pred_test table:\", collections.Counter(pred_test.ravel()))\n","    if s_type == \"disc\": #disc S\n","        df_tr[\"pred_train\"] = pred_train\n","        df_te[\"pred_test\"] = pred_test\n","        print(pd.crosstab(df_tr[S_names[0]], df_tr[\"pred_train\"], normalize='index'))\n","        print(pd.crosstab(df_te[S_names[0]], df_te[\"pred_test\"], normalize='index'))\n","    print(\"=\"*30)\n","\n","    return(pred_test, pred_train, model)\n","\n","\n","def get_A_names(is_base, is_exp, is_expOracle, is_qua, is_inf):\n","    A_names = list()\n","    A_names.append(\"A_obs\")\n","    if is_base:\n","        A_names.append(\"A_base\")\n","    if is_exp:\n","        A_names.append(\"A_exp\")\n","    if is_expOracle:\n","        A_names.append(\"A_expOracle\")\n","    if is_qua or is_inf:\n","        A_names.append(\"A_qua\")\n","    print(\"A_names:\",A_names)\n","\n","    return A_names\n","\n","\n","def add_Apred_Midx(df, A_names, A_base, A_exp, A_expOracle, A_qua):\n","    df_use = df.copy()\n","\n","    # add A_pred to df\n","    df_use[\"A_obs\"] = np.where(df_use[\"A\"]==1, \"yes\",\"no\")\n","    if \"A_base\" in A_names:\n","        df_use[\"A_base\"] = A_base\n","    if \"A_exp\" in A_names:\n","        df_use[\"A_exp\"] = A_exp\n","    if \"A_expOracle\" in A_names:\n","        df_use[\"A_expOracle\"] = A_expOracle\n","    if \"A_qua\" in A_names:\n","        df_use[\"A_qua\"] = A_qua\n","\n","    return df_use.copy()\n","\n","\n","\n","def minor_rule(df, use_covars, s_type, S_name):\n","    \"\"\"use decision tree to print out rule of vulnerable group (X -> S)\n","        only apply when S is discrete\n","    \"\"\"\n","    df_use = df.copy()\n","\n","    df_use[\"S_minor\"] = np.where(np.isnan(df_use[S_name]), -999,df_use[S_name])\n","    print('df_use[\"S_minor\"]', df_use[\"S_minor\"].value_counts(dropna=False))\n","\n","    if s_type == \"disc\":\n","        clf = DecisionTreeClassifier(max_depth=3, random_state=4211)\n","    else:\n","        clf = DecisionTreeRegressor(max_depth=3, random_state=4211)\n","\n","    clf.fit(df_use[use_covars], df_use[\"S_minor\"])\n","\n","\n","    tree_rules = export_text(clf, feature_names = list(use_covars))\n","    print(S_name)\n","    print(tree_rules)\n","\n","    clf.predict(df_use[use_covars])\n","\n","    return\n","\n","\n","\n","\n","\n","def expect_reward(df, A_name, minor_name):\n","    df_use = df.copy()\n","\n","    df_use[\"A_obs\"]=np.where(df_use[\"A\"]==1, \"yes\",\"no\")\n","\n","    minor_df = df_use[df_use[minor_name]==1].reset_index(drop=True).copy()\n","    rest_df = df_use[df_use[minor_name]==0].reset_index(drop=True).copy()\n","\n","    numer = df_use[\"Y\"] * (df_use[\"A_obs\"] == df_use[A_name]) / df_use[\"ps\"]\n","    denom = (df_use[\"A_obs\"] == df_use[A_name]) / df_use[\"ps\"]\n","    reward_all = np.sum(numer) / np.sum(denom)\n","\n","    if not minor_df.empty:\n","        numer = minor_df[\"Y\"] * (minor_df[\"A_obs\"] == minor_df[A_name]) / minor_df[\"ps\"]\n","        denom = (minor_df[\"A_obs\"] == minor_df[A_name]) / minor_df[\"ps\"]\n","        reward_minor = np.sum(numer) / np.sum(denom)\n","    else:\n","        reward_minor = np.nan\n","\n","    if not rest_df.empty:\n","        numer = rest_df[\"Y\"] * (rest_df[\"A_obs\"] == rest_df[A_name]) / rest_df[\"ps\"]\n","        denom = (rest_df[\"A_obs\"] == rest_df[A_name]) / rest_df[\"ps\"]\n","        reward_rest = np.sum(numer) / np.sum(denom)\n","    else:\n","        reward_rest = np.nan\n","\n","    print(A_name, \"reward  \",\n","          \"all:\",round(reward_all,3),\n","          \"minor:\",round(reward_minor,3),\n","          \"rest:\",round(reward_rest,3))\n","\n","    return reward_all, reward_minor, reward_rest\n","\n","\n","def get_pred_Y(df, A_name, XS_names, y_fn, model_sxa0, model_sxa1):\n","    \"\"\"generate Y based on predicted A (treatment assignment)\n","    model: Y|X,S,A=a\n","    should use unnormalized X & S for simulation & normalized for real data!\n","    \"\"\"\n","    df_use = df[XS_names].copy()\n","\n","    df_use[\"A\"] = np.where(df[A_name]==\"yes\", 1,0)\n","    if y_fn is not None:\n","        Y_pred = df_use.eval(y_fn)\n","    else:\n","\n","        Y_pred0 = model_sxa0.predict_proba(df_use[XS_names])[:,1].flatten()\n","        Y_pred1 = model_sxa1.predict_proba(df_use[XS_names])[:,1].flatten()\n","        Y_pred = np.where(df_use[\"A\"]==0, Y_pred0, Y_pred1)\n","    assert len(Y_pred.shape)==1\n","\n","    return Y_pred\n","\n","\n","def get_value_fn(df, A_name, XS_names, minor_name, y_fn, model_sxa0, model_sxa1):\n","    \"\"\"get value function for subgroup of interest\"\"\"\n","    df_use = df.copy()\n","\n","    df_use[\"Y_pred\"] = get_pred_Y(df_use, A_name, XS_names, y_fn, model_sxa0, model_sxa1)\n","\n","    minor_df = df_use[df_use[minor_name]==1].reset_index(drop=True).copy()\n","    rest_df = df_use[df_use[minor_name]==0].reset_index(drop=True).copy()\n","\n","    value_all = np.mean(df_use[\"Y_pred\"].values)\n","\n","    if not minor_df.empty:\n","        value_minor = np.mean(minor_df[\"Y_pred\"].values)\n","        minorPct=minor_df.shape[0]/df_use.shape[0]\n","    else:\n","        value_minor = np.nan\n","        minorPct=np.nan\n","\n","    if not rest_df.empty:\n","        value_rest = np.mean(rest_df[\"Y_pred\"].values)\n","    else:\n","        value_rest = np.nan\n","\n","    print(A_name, \"value   \",\n","          \"all:\",round(value_all,3),\n","          \"minor:\",round(value_minor,3),\n","          \"rest:\",round(value_rest,3),\n","          \"minorPct:\",round(minorPct,3))\n","\n","    return value_all, value_minor, value_rest, minorPct\n","\n","\n","\n","def get_obj_qua(df, X_names, s_type, A_name, minor_name, fit_qua0, fit_qua1):\n","    \"\"\"\n","    Used for real data with cont S (no need S grid)\n","\n","    df includes X,A_test (A_test is from prediction)\n","    objective: E[Gs{E[Y|X,S,A=d(x)]}]\n","    -> Quantile_regress Y|X,A -> E[.]\n","    -> take avg over all subjects\n","    \"\"\"\n","    df_use = df.copy()\n","    del df_use['A']\n","    del df_use['Y']\n","\n","    if s_type != \"cont\":\n","        assert(1 == 0)\n","\n","\n","    Y_pred0 = 1 / (1 + np.exp(-fit_qua0.predict(df_use[X_names]))).flatten()\n","    Y_pred1 = 1 / (1 + np.exp(-fit_qua1.predict(df_use[X_names]))).flatten()\n","\n","    df_use[\"A\"] = np.where(df_use[A_name]==\"yes\", 1,0) #contains X & A\n","    df_use[\"Y\"] = np.where(df_use[\"A\"]==0, Y_pred0, Y_pred1)\n","\n","    minor_df = df_use[df_use[minor_name]==1].reset_index(drop=True).copy()\n","    rest_df = df_use[df_use[minor_name]==0].reset_index(drop=True).copy()\n","\n","    obj_all = np.mean(df_use[\"Y\"].values)\n","\n","    if not minor_df.empty:\n","        obj_minor = np.mean(minor_df[\"Y\"].values)\n","    else:\n","        obj_minor = np.nan\n","\n","    if not rest_df.empty:\n","        obj_rest = np.mean(rest_df[\"Y\"].values)\n","    else:\n","        obj_rest = np.nan\n","\n","    print(A_name, \"obj     \",\n","          \"all:\",round(obj_all,3),\n","          \"minor:\",round(obj_minor,3),\n","          \"rest:\",round(obj_rest,3))\n","\n","    return obj_all, obj_minor, obj_rest\n","\n","\n","def get_obj_inf(df, X_names, XS_names, A_name, minor_name, y_fn, s_type, is_class, qua_use, model_sxa0, model_sxa1):\n","    \"\"\"\n","    Used for real data with disc S | simulation with disc/cont S\n","    df includes X,A_test (A_test is from prediction)\n","    objective: E[Gs{E[Y|X,S,A=d(x)]}]\n","    -> get E[Y|X,S,A=d(x)] from true distn (simulation) or fitted model (real data)\n","    -> given X & A, generate S from empirical test data\n","    -> find quan/inf wrt S\n","    -> take avg over all subjects\n","    \"\"\"\n","    df_use = df.copy()\n","\n","    # S grid from empirical test data\n","    if s_type == \"disc\": # real/simulation with disc S\n","        S_names = list( set(XS_names) - set(X_names) )\n","        if len(S_names) == 1:\n","            s_min = int(np.min(df_use[\"S\"]))\n","            s_max = int(np.max(df_use[\"S\"]))\n","            s_grid = range(s_min, s_max+1)\n","            if y_fn is None:\n","                s_grid = [s_grid]\n","        else:\n","            s_lst = []\n","            for s_var in S_names:\n","                s_min = int(np.min(df_use[s_var]))\n","                s_max = int(np.max(df_use[s_var]))\n","                s_grid = range(s_min, s_max+1)\n","                s_lst.append(np.array(s_grid))\n","            s_grid = s_lst.copy()\n","    elif s_type == \"cont\" and y_fn is not None: # simulation with cont S\n","        s_grid = df[\"S\"].values.copy()\n","        assert len(s_grid) > 2\n","    else:\n","        assert(1 == 0)\n","\n","    # augment X&A with S grid\n","    df_XA = df_use[[minor_name]+X_names].copy() #contains X\n","    df_XA[\"A\"] = np.where(df_use[A_name]==\"yes\", 1,0) #contains X & A\n","    idx = range(df_XA.shape[0])\n","    df_XA[\"idx\"] = idx\n","\n","    if y_fn is None:\n","        idx_s_lst = [np.array(idx)] + s_grid\n","        idx_s = np.array(np.meshgrid(*np.array(idx_s_lst, dtype=object))).reshape(1+len(S_names), -1).T\n","        idx_s = pd.DataFrame(idx_s, columns = ['idx']+S_names)\n","    else: # if remove, may occur error in .eval(y_fn) # this works as long as there is no multi-S in simulation\n","        idx_s = np.array(np.meshgrid(idx, s_grid)).reshape(2, len(idx)*len(s_grid)).T\n","        idx_s = pd.DataFrame(idx_s, columns = ['idx','S'])\n","\n","    aug_df = df_XA.merge(idx_s, on='idx', how='left').reset_index(drop=True).copy()\n","\n","    # get E[Y|X,S,A=d(x)]\n","    if y_fn is not None: # from true distn (simulation)\n","        aug_df[\"Y\"] = aug_df.eval(y_fn)\n","    else: # from fitted model (real data)\n","\n","        Y_pred0 = model_sxa0(aug_df[XS_names]).flatten()\n","        Y_pred1 = model_sxa1(aug_df[XS_names]).flatten()\n","        aug_df[\"Y\"] = np.where(aug_df[\"A\"]==0, Y_pred0, Y_pred1)\n","\n","    # find quantile/inf wrt S\n","    def quan(x):\n","        return x.quantile(qua_use)\n","\n","    minor_df = aug_df[aug_df[minor_name]==1].reset_index(drop=True).copy()\n","    rest_df = aug_df[aug_df[minor_name]==0].reset_index(drop=True).copy()\n","\n","    # Gs{E[Y|X,S,A=d(x)]}\n","    if (s_type == \"cont\"):\n","        agg_fn = {\"Y\": quan}\n","    elif (s_type == \"disc\"):\n","        agg_fn = {\"Y\": \"min\"}\n","\n","    smry_all = aug_df.groupby(by=\"idx\", as_index=False).agg(agg_fn)\n","    smry_minor = minor_df.groupby(by=\"idx\", as_index=False).agg(agg_fn)\n","    smry_rest = rest_df.groupby(by=\"idx\", as_index=False).agg(agg_fn)\n","\n","    # obj: E{G(.)}\n","    obj_all = np.mean(smry_all.loc[:,\"Y\"].values)\n","\n","    if not minor_df.empty:\n","        obj_minor = np.mean(smry_minor.loc[:,\"Y\"].values)\n","    else:\n","        obj_minor = np.nan\n","\n","    if not rest_df.empty:\n","        obj_rest = np.mean(smry_rest.loc[:,\"Y\"].values)\n","    else:\n","        obj_rest = np.nan\n","\n","    print(A_name, \"obj     \",\n","          \"all:\",round(obj_all,3),\n","          \"minor:\",round(obj_minor,3),\n","          \"rest:\",round(obj_rest,3))\n","\n","    return obj_all, obj_minor, obj_rest\n","\n","\n","\n","def dict_mean(dict_list):\n","    mean_list = list()\n","    mean_dict = dict()\n","    for A_name in dict_list[0].keys():\n","        mean_dict[A_name] = dict()\n","        for metric in dict_list[0][A_name].keys():\n","            mean_dict[A_name][metric] = np.nansum([d[A_name][metric] for d in dict_list]) / len(dict_list)\n","            mean_list.append(mean_dict[A_name][metric])\n","\n","    mean_list = np.array(mean_list)\n","    mean_list = np.reshape(mean_list, (len(dict_list[0].keys()), -1)).T\n","    mean_df = pd.DataFrame(data=mean_list, columns=dict_list[0].keys())\n","    mean_df.index = list(dict_list[0][list(dict_list[0].keys())[0]].keys())\n","\n","    return mean_dict, mean_df\n","\n","\n","def dict_std(dict_list):\n","    \"\"\"\n","    Replicate this sampling test data procedure 100 times,\n","    estimate values of each ITR for each replicate,\n","    and get the averaged estimated values (standard error)\n","    \"\"\"\n","    mean_list = list()\n","    mean_dict = dict()\n","    for A_name in dict_list[0].keys():\n","        mean_dict[A_name] = dict()\n","        for metric in dict_list[0][A_name].keys():\n","            mean = np.nansum([d[A_name][metric] for d in dict_list]) / len(dict_list)\n","            variance = np.nansum([(d[A_name][metric] - mean)**2 for d in dict_list]) / len(dict_list)\n","            mean_dict[A_name][metric] = variance**0.5 / (len(dict_list))**0.5  #SE\n","            mean_list.append(mean_dict[A_name][metric])\n","\n","    mean_list = np.array(mean_list)\n","    mean_list = np.reshape(mean_list, (len(dict_list[0].keys()), -1)).T\n","    mean_df = pd.DataFrame(data=mean_list, columns=dict_list[0].keys())\n","    mean_df.index = list(dict_list[0][list(dict_list[0].keys())[0]].keys())\n","\n","    return mean_dict, mean_df\n","\n","\n","def prop_treat(df, A_name, minor_name):\n","    df_use = df.reset_index(drop=True).copy()\n","\n","    minor_df = df_use[df_use[minor_name]==1].reset_index(drop=True).copy()\n","    rest_df = df_use[df_use[minor_name]==0].reset_index(drop=True).copy()\n","\n","    prop_all = df_use[df_use[A_name]==\"yes\"].shape[0] / df_use.shape[0]\n","\n","    if not minor_df.empty:\n","        prop_minor = minor_df[minor_df[A_name]==\"yes\"].shape[0] / minor_df.shape[0]\n","    else:\n","        prop_minor = np.nan\n","\n","    if not rest_df.empty:\n","        prop_rest = rest_df[rest_df[A_name]==\"yes\"].shape[0] / rest_df.shape[0]\n","    else:\n","        prop_rest = np.nan\n","\n","    print(A_name, \"pr_treat\",\n","          \"all:\",round(prop_all,3),\n","          \"minor:\",round(prop_minor,3),\n","          \"rest:\",round(prop_rest,3))\n","\n","    return prop_all, prop_minor, prop_rest\n","\n","\n","\n","\n","def eval_metrics(minor_name, res_df, A_names, X_names, XS_names, y_fn, s_type, is_class,\n","                    qua_use, fit_Y_XS_A0, fit_Y_XS_A1, fit_qua0, fit_qua1, res_df2):\n","    print(minor_name)\n","    print(\"=\"*30)\n","    res = dict()\n","    for A_name in A_names:\n","        res[A_name] = dict()\n","        # metrics\n","        prop_all, prop_minor, prop_rest = prop_treat(res_df, A_name, minor_name)\n","        reward_all, reward_minor, reward_rest = expect_reward(res_df, A_name, minor_name)\n","        value_all, value_minor, value_rest, minorPct = get_value_fn(res_df, A_name, XS_names, minor_name, y_fn, fit_Y_XS_A0, fit_Y_XS_A1)\n","\n","        if s_type == \"disc\" or y_fn is not None: # real with disc S | sim with disc/cont S\n","            obj_all, obj_minor, obj_rest = get_obj_inf(res_df, X_names, XS_names, A_name, minor_name, y_fn, s_type, is_class, qua_use, fit_Y_XS_A0, fit_Y_XS_A1)\n","        elif s_type == \"cont\": # real with cont S\n","            obj_all, obj_minor, obj_rest = get_obj_qua(res_df, X_names, s_type, A_name, minor_name, fit_qua0, fit_qua1)\n","        else:\n","            assert(1 == 0)\n","\n","        minor_df_train = res_df2[res_df2[minor_name]==1].reset_index(drop=True).copy()\n","        if not minor_df_train.empty:\n","            minorPct_train=minor_df_train.shape[0]/res_df2.shape[0]\n","        else:\n","            minorPct_train=np.nan\n","\n","        # save\n","        res[A_name][\"prop_all\"] = prop_all;     res[A_name][\"prop_minor\"] = prop_minor;     res[A_name][\"prop_rest\"] = prop_rest\n","        res[A_name][\"reward_all\"] = reward_all; res[A_name][\"reward_minor\"] = reward_minor; res[A_name][\"reward_rest\"] = reward_rest\n","        res[A_name][\"value_all\"] = value_all;   res[A_name][\"value_minor\"] = value_minor;   res[A_name][\"value_rest\"] = value_rest\n","        res[A_name][\"minorPct_test\"] = minorPct; res[A_name][\"minorPct_train\"] = minorPct_train\n","        res[A_name][\"obj_all\"] = obj_all;       res[A_name][\"obj_minor\"] = obj_minor;       res[A_name][\"obj_rest\"] = obj_rest\n","        print(\"=\"*30)\n","    print(\"=\"*50)\n","    return res\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}